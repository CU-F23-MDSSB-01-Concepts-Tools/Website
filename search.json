[
  {
    "objectID": "W05.html#data-sets-1a-and-1b-widsom-of-crowd",
    "href": "W05.html#data-sets-1a-and-1b-widsom-of-crowd",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Data Sets 1a and 1b: Widsom of Crowd",
    "text": "Data Sets 1a and 1b: Widsom of Crowd\n1a: Ox weigh guessing competition 1907 (collected by Galton)\n\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5)\n\n\n\n\n1b: Viertelfest “guess the number of sold lots”-competition 2009\n\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500)"
  },
  {
    "objectID": "W05.html#data-set-2-palmer-penguins",
    "href": "W05.html#data-set-2-palmer-penguins",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Data Set 2: Palmer Penguins",
    "text": "Data Set 2: Palmer Penguins\nPalmer Penguins\nChinstrap, Gentoo, and Adélie Penguins\n   \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "W05.html#summary-from-base-r",
    "href": "W05.html#summary-from-base-r",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "summary from base R",
    "text": "summary from base R\n\n\nShows summary statistics for the values in a vector\n\nsummary(galton$Estimate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    896    1162    1208    1197    1236    1516 \n\n\n\nsummary(viertel$Schätzung)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n     120     5000     9843    53164    20000 29530000 \n\n\nOr for all columns in a data frame\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\nQuestion\nWhat does\n1st Qu. and\n3rd Qu. mean?"
  },
  {
    "objectID": "W05.html#quantiles",
    "href": "W05.html#quantiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Quantiles",
    "text": "Quantiles\nCut points specifying intervals which contain equal amounts of values of the distribution.\n\\(q\\)-quantiles divide numbers into \\(q\\) intervals covering all values.\nThe quantiles are the cut points: For \\(q\\) intervals there are \\(q-1\\) cut points of interest.\n\nThe one 2-quantile is the median\nThe three 4-quantiles are called quartiles\n\n1st Qu. is the first quartile\nThe second quartile is the median\n3rd Qu. is the third quartile\n\n100-quantiles are called percentiles\n\n\n\nWe omit problems of estimating quantiles from a sample where the number of estimates does not fit to a desired partition of equal size here."
  },
  {
    "objectID": "W05.html#a-galton-quartiles",
    "href": "W05.html#a-galton-quartiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "1a Galton: Quartiles",
    "text": "1a Galton: Quartiles\n\n# Min, 3 Quartiles, Max\nquantile(galton$Estimate, prob = seq(0, 1, by = 0.25))\n\n    0%    25%    50%    75%   100% \n 896.0 1162.5 1208.0 1236.0 1516.0 \n\n\nInterpretation: What does the value at 25% mean?\n\nThe 25% of all values are lower than the value. 75% are larger.\n\n\n\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + \n geom_vline(xintercept = quantile(galton$Estimate, prob = seq(0, 1, by = 0.25)), color = \"red\")"
  },
  {
    "objectID": "W05.html#a-galton-20-quantiles",
    "href": "W05.html#a-galton-20-quantiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "1a Galton: 20-quantiles",
    "text": "1a Galton: 20-quantiles\n\n# Min, 3 Quartiles, Max\nquantile(galton$Estimate, prob = seq(0, 1, by = 0.05))\n\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n 896.0 1078.3 1109.0 1126.9 1150.0 1162.5 1174.0 1181.0 1189.0 1199.0 1208.0 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%   100% \n1214.0 1219.0 1225.0 1231.0 1236.0 1243.8 1255.1 1270.0 1293.0 1516.0 \n\n\n\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + \n geom_vline(xintercept = quantile(galton$Estimate, prob = seq(0, 1, by = 0.05)), color = \"red\")"
  },
  {
    "objectID": "W05.html#b-viertelfest-quartiles",
    "href": "W05.html#b-viertelfest-quartiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "1b Viertelfest: Quartiles",
    "text": "1b Viertelfest: Quartiles\n\nquantile(viertel$Schätzung, prob = seq(0, 1, by = 0.25))\n\n      0%      25%      50%      75%     100% \n     120     5000     9843    20000 29530000 \n\n\n\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + \n geom_vline(xintercept = quantile(viertel$`Schätzung`, prob = seq(0, 1, by = 0.25))[1:4], color = \"red\")"
  },
  {
    "objectID": "W05.html#b-viertelfest-20-quantiles",
    "href": "W05.html#b-viertelfest-20-quantiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "1b Viertelfest: 20-quantiles",
    "text": "1b Viertelfest: 20-quantiles\n\nquantile(viertel$Schätzung, prob = seq(0, 1, by = 0.05))\n\n         0%          5%         10%         15%         20%         25% \n     120.00     1213.25     2000.00     3115.00     4012.00     5000.00 \n        30%         35%         40%         45%         50%         55% \n    5853.50     7000.00     7821.00     8705.25     9843.00    10967.50 \n        60%         65%         70%         75%         80%         85% \n   12374.00    14444.00    16186.00    20000.00    27500.00    38000.00 \n        90%         95%        100% \n   63649.50    99773.50 29530000.00 \n\n\n\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + \n geom_vline(xintercept = quantile(viertel$`Schätzung`, prob = seq(0, 1, by = 0.05))[1:19], color = \"red\")"
  },
  {
    "objectID": "W05.html#palmer-penguins-flipper-length-quartiles",
    "href": "W05.html#palmer-penguins-flipper-length-quartiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "2 Palmer Penguins Flipper Length: Quartiles",
    "text": "2 Palmer Penguins Flipper Length: Quartiles\n\nquantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.25), na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n 172  190  197  213  231 \n\n\n\npenguins |> ggplot(aes(flipper_length_mm)) + geom_histogram(binwidth = 1) + \n geom_vline(xintercept = quantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.25), na.rm = TRUE), color = \"red\")"
  },
  {
    "objectID": "W05.html#palmer-penguins-flipper-length-20-quantiles",
    "href": "W05.html#palmer-penguins-flipper-length-20-quantiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "2 Palmer Penguins Flipper Length: 20-quantiles",
    "text": "2 Palmer Penguins Flipper Length: 20-quantiles\n\nquantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.05), na.rm = TRUE)\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n172.0 181.0 185.0 187.0 188.0 190.0 191.0 193.0 194.0 195.0 197.0 199.0 203.0 \n  65%   70%   75%   80%   85%   90%   95%  100% \n208.0 210.0 213.0 215.0 218.0 220.9 225.0 231.0 \n\n\n\npenguins |> ggplot(aes(flipper_length_mm)) + geom_histogram(binwidth = 1) + \n geom_vline(xintercept = quantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.05), na.rm = TRUE), color = \"red\")"
  },
  {
    "objectID": "W05.html#interquartile-range-iqr",
    "href": "W05.html#interquartile-range-iqr",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\nThe difference between the 1st and the 3rd quartile. Alternative dispersion measure.\nThe range in which the middle 50% of the values are located.\nExamples:\n\n\n\n# Min, 3 Quartiles, Max\nIQR(galton$Estimate)\n\n[1] 73.5\n\nsd(galton$Estimate) # for comparison\n\n[1] 73.58677\n\nIQR(viertel$Schätzung)\n\n[1] 15000\n\nsd(viertel$Schätzung) # for comparison\n\n[1] 848395.5\n\nIQR(penguins$flipper_length_mm, na.rm = TRUE)\n\n[1] 23\n\nsd(penguins$flipper_length_mm, na.rm = TRUE) # for comparison\n\n[1] 14.06171"
  },
  {
    "objectID": "W05.html#boxplots",
    "href": "W05.html#boxplots",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Boxplots",
    "text": "Boxplots\nA condensed visualization of a distribution showing location, spread, skewness and outliers.\n\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n\n\n\nThe box shows the median in the middle and the other two quartiles as their borders.\nWhiskers: From above the upper quartile, a distance of 1.5 times the IQR is measured out and a whisker is drawn up to the largest observed data point from the dataset that falls within this distance. Similarly, for the lower quartile.\nWhiskers must end at an observed data point! (So lengths can differ.)\nAll other values outside of box and whiskers are shown as points and often called outliers. (There may be none.)"
  },
  {
    "objectID": "W05.html#boxplots-vs.-histograms",
    "href": "W05.html#boxplots-vs.-histograms",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\n\nHistograms can show the shape of the distribution well, but not the summary statistics like the median.\n\n\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n\n\n\n\n\ngalton |> ggplot(aes(x = Estimate)) + geom_histogram(binwidth = 5)"
  },
  {
    "objectID": "W05.html#boxplots-vs.-histograms-1",
    "href": "W05.html#boxplots-vs.-histograms-1",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\n\nBoxplots can not show the patterns of bimodal or multimodal distributions.\n\n\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_boxplot()\n\n\n\n\n\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_histogram(binwidth = 1)"
  },
  {
    "objectID": "W05.html#minimizing-proporties-of-mean-and-median",
    "href": "W05.html#minimizing-proporties-of-mean-and-median",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Minimizing proporties of Mean and Median",
    "text": "Minimizing proporties of Mean and Median\nMean minimizes the mean of squared deviations from it. No other value \\(a\\) has a lower mean of square distances from the data points. \\(\\frac{1}{n}\\sum_{i=1}^n(x_i - a)^2\\).\n\nMedian minimizes the sum of the absolute deviation. No other value \\(a\\) has a lower mean of absolute distances from the data points. \\(\\frac{1}{n}\\sum_{i=1}^n|x_i - a|\\)."
  },
  {
    "objectID": "W05.html#two-families-of-summary-statistics",
    "href": "W05.html#two-families-of-summary-statistics",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Two families of summary statistics",
    "text": "Two families of summary statistics\n\nMeasures based on sums (related to mathematical moments)\n\nMean\nStandard deviation\n\nMeasures based on the ordered sequence of these observations (order statistics)\n\nMedian (and all quantiles)\nInterquartile range"
  },
  {
    "objectID": "W05.html#covariance",
    "href": "W05.html#covariance",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Covariance",
    "text": "Covariance\nGoal: We want to measure the joint variation in numerical observations of two variables \\(x_1,\\dots,x_n\\) and \\(y_1, \\dots, y_n\\).\nCovariance\n\\(\\text{cov}(x,y) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)\\)\nwhere \\(\\mu_x\\) and \\(\\mu_y\\) are the arithmetic means of \\(x\\) and \\(y\\).\n\nNote: \\(\\text{cov}(x,x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)(x_i - \\mu_x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)^2 = \\text{Var}(x)\\)"
  },
  {
    "objectID": "W05.html#correlation",
    "href": "W05.html#correlation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Correlation",
    "text": "Correlation\nGoal: We want to measure the linear correlation in numerical observations of two variables \\(x_1,\\dots,x_n\\) and \\(y_1, \\dots, y_n\\).\nPearson correlation coefficient \\(r_{xy} = \\frac{\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^n(x_i - \\mu_x)^2}\\sqrt{\\sum_{i=1}^n(y_i - \\mu_y)^2}}\\)\n\nRelation to covariance: \\(r_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}\\)\nwhere \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of \\(x\\) and \\(y\\).\nRelation to standard scores:\nWhen \\(x\\) and \\(y\\) are standard scores (each with mean zero and standard deviation one), then \\(\\text{cov}(x,y) = r_{xy}\\).\n\n\n\nThere are other correlation coefficients which we omit here."
  },
  {
    "objectID": "W05.html#interpretation-of-correlation",
    "href": "W05.html#interpretation-of-correlation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretation of correlation",
    "text": "Interpretation of correlation\nCorrelation between two vectors \\(x\\) and \\(y\\) is “normalized”.\n\nThe maximal possible values is \\(r_{xy} = 1\\)\n\n\\(x\\) and \\(y\\) are fully correlated\n\nThe minimal values is \\(r_{xy} = -1\\)\n\n\\(x\\) and \\(y\\) are anticorrelated\n\n\\(r_{xy} \\approx 0\\) mean\n\nthe variables are uncorrelated\n\n\\(r_{xy} = r_{yx}\\)"
  },
  {
    "objectID": "W05.html#correlation-matrix",
    "href": "W05.html#correlation-matrix",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nUsing corrr from the packagestidymodels\n\nlibrary(corrr)\npenguins |> select(-species, -island, -sex) |> \n correlate()\n\n# A tibble: 5 × 6\n  term        bill_length_mm bill_depth_mm flipper_length_mm body_mass_g    year\n  <chr>                <dbl>         <dbl>             <dbl>       <dbl>   <dbl>\n1 bill_lengt…        NA            -0.235              0.656      0.595   0.0545\n2 bill_depth…        -0.235        NA                 -0.584     -0.472  -0.0604\n3 flipper_le…         0.656        -0.584             NA          0.871   0.170 \n4 body_mass_g         0.595        -0.472              0.871     NA       0.0422\n5 year                0.0545       -0.0604             0.170      0.0422 NA"
  },
  {
    "objectID": "W05.html#correlation-table",
    "href": "W05.html#correlation-table",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Correlation table",
    "text": "Correlation table\nUsing correlation from the packagescorrelation\n\nlibrary(correlation)\nresults <- palmerpenguins::penguins |> \n select(-species, -island, -sex) |> \n correlation()\nresults\n\n# Correlation Matrix (pearson-method)\n\nParameter1        |        Parameter2 |     r |         95% CI | t(340) |         p\n-----------------------------------------------------------------------------------\nbill_length_mm    |     bill_depth_mm | -0.24 | [-0.33, -0.13] |  -4.46 | < .001***\nbill_length_mm    | flipper_length_mm |  0.66 | [ 0.59,  0.71] |  16.03 | < .001***\nbill_length_mm    |       body_mass_g |  0.60 | [ 0.52,  0.66] |  13.65 | < .001***\nbill_length_mm    |              year |  0.05 | [-0.05,  0.16] |   1.01 | 0.797    \nbill_depth_mm     | flipper_length_mm | -0.58 | [-0.65, -0.51] | -13.26 | < .001***\nbill_depth_mm     |       body_mass_g | -0.47 | [-0.55, -0.39] |  -9.87 | < .001***\nbill_depth_mm     |              year | -0.06 | [-0.17,  0.05] |  -1.11 | 0.797    \nflipper_length_mm |       body_mass_g |  0.87 | [ 0.84,  0.89] |  32.72 | < .001***\nflipper_length_mm |              year |  0.17 | [ 0.06,  0.27] |   3.17 | 0.007**  \nbody_mass_g       |              year |  0.04 | [-0.06,  0.15] |   0.78 | 0.797    \n\np-value adjustment method: Holm (1979)\nObservations: 342\n\n\n\n\nWhat do the stars mean? Statistical significance automatically added by the . We treat that later."
  },
  {
    "objectID": "W05.html#correlation-visualization",
    "href": "W05.html#correlation-visualization",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Correlation visualization",
    "text": "Correlation visualization\n\nresults %>%\n  summary(redundant = TRUE) %>%\n  plot()"
  },
  {
    "objectID": "W05.html#exploratory-data-analysis-1",
    "href": "W05.html#exploratory-data-analysis-1",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nEDA is the systematic exploration of data using\n\nvisualization\ntransformation\ncomputation of characteristic values\nmodeling\n\n\n\nComputation of characteristic values: Functions like mean, median, mode, standard deviation, or interquartile range\nModeling: Operations like linear regression or dimensionality reduction. We haven’t talked about it, but will do soon."
  },
  {
    "objectID": "W05.html#systematic-but-no-standard-routine",
    "href": "W05.html#systematic-but-no-standard-routine",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Systematic but no standard routine",
    "text": "Systematic but no standard routine\n\n“There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox\n\n\n“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey"
  },
  {
    "objectID": "W05.html#systematic-but-no-standard-routine-1",
    "href": "W05.html#systematic-but-no-standard-routine-1",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Systematic but no standard routine",
    "text": "Systematic but no standard routine\n\nGoal of EDA: Develop understanding of your data.\nEDA’s iterative cycle\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\n\nEDA is fundamentally a creative process."
  },
  {
    "objectID": "W05.html#questions",
    "href": "W05.html#questions",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Questions",
    "text": "Questions\n\nThe way to ask quality questions:\n\nGenerate many questions!\nYou cannot come up with most interesting questions when you start.\n\nThere is no rule which questions to ask. These are useful\n\nWhat type of variation occurs within my variables?\n(Barplots, Histograms,…)\nWhat type of covariation occurs between my variables?\n(Scatterplots, Timelines,…)"
  },
  {
    "objectID": "W05.html#eda-embedded-in-a-statistical-data-science-project",
    "href": "W05.html#eda-embedded-in-a-statistical-data-science-project",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "EDA embedded in a statistical data science project",
    "text": "EDA embedded in a statistical data science project\n\nStating and refining the question\nExploring the data\nBuilding formal statistical models\nInterpreting the results\nCommunicating the results\n\n\n\nRoger D. Peng and Elizabeth Matsui. “The Art of Data Science.” A Guide for Anyone Who Works with Data. Skybrude Consulting, LLC (2015)."
  },
  {
    "objectID": "W05.html#example-eda-strange-airports",
    "href": "W05.html#example-eda-strange-airports",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Example EDA: Strange Airports",
    "text": "Example EDA: Strange Airports\nFrom Homework 02:\n\nlibrary(nycflights13)\nggplot(data = airports, mapping = aes(x = lon, y = lat)) + geom_point(aes(color = tzone)) \n\n\n\nairports %>% filter(lon >= 0) \n\n# A tibble: 4 × 8\n  faa   name                            lat   lon   alt    tz dst   tzone       \n  <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr> <chr>       \n1 DVT   Deer Valley Municipal Airport  33.4 112.   1478     8 A     Asia/Chongq…\n2 EEN   Dillant Hopkins Airport        72.3  42.9   149    -5 A     <NA>        \n3 MYF   Montgomery Field               32.5 118.     17     8 A     Asia/Chongq…\n4 SYA   Eareckson As                   52.7 174.     98    -9 A     America/Anc…"
  },
  {
    "objectID": "W05.html#airport-errors",
    "href": "W05.html#airport-errors",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Airport errors",
    "text": "Airport errors\n\n\n# A tibble: 4 × 8\n  faa   name                            lat   lon   alt    tz dst   tzone       \n  <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr> <chr>       \n1 DVT   Deer Valley Municipal Airport  33.4 112.   1478     8 A     Asia/Chongq…\n2 EEN   Dillant Hopkins Airport        72.3  42.9   149    -5 A     <NA>        \n3 MYF   Montgomery Field               32.5 118.     17     8 A     Asia/Chongq…\n4 SYA   Eareckson As                   52.7 174.     98    -9 A     America/Anc…\n\n\n\nCorrect locations (internet research and location of maps):\n\n\n\nDeer Valley Municipal Airport: Phoenix\n33°41′N 112°05′W Missing minus for lon (W)\nDillant Hopkins Airport: New Hampshire\n42°54′N 72°16′W lon-lat switched, minus (W)\nMontgomery Field: San Diego\n32°44′N 117°11″W Missing minus for lon (W)\nEareckson As: Alaska\n52°42′N 174°06′E No error: Too west,it’s east!"
  },
  {
    "objectID": "W05.html#conclusions-on-data-errors",
    "href": "W05.html#conclusions-on-data-errors",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Conclusions on data errors",
    "text": "Conclusions on data errors\n\nIn real-world datasets errors like the 3 airport are quite common.\nErrors of this type are often hard to detect and remain unnoticed.\n\nThis can (but need not) change results drastically!\n\n\n\nConclusions\n\nAlways remain alert for inconsistencies and be ready to check the plausibility of results.\nSkills in exploratory data analysis (EDA) are essential to find errors and explore their nature and implication\nErrors are unpredictable, of diverse types, and are often deeply related to the reality the data presents.\n\nThis is one reason why EDA can not be a fully formalized and automatized process."
  },
  {
    "objectID": "W05.html#six-types-of-questions",
    "href": "W05.html#six-types-of-questions",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Six types of questions",
    "text": "Six types of questions\n\nDescriptive: summarize a characteristic of a set of data\nExploratory: analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\nInferential: analyze patterns, trends, or relationships in representative data from a population\nPredictive: make predictions for individuals or groups of individuals\nCausal: whether changing one factor will change another factor, on average, in a population\nMechanistic: explore “how” one factor (probably/most likely/potentially) changes another\n\n\nWe only did 1 and 2, so far.\n\n\n\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146."
  },
  {
    "objectID": "W05.html#descriptive-projects",
    "href": "W05.html#descriptive-projects",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Descriptive Projects",
    "text": "Descriptive Projects\n\n\n\nDubin (1969). Theory Building - A Practical Guide to the Construction and Testing of Theoretical Models"
  },
  {
    "objectID": "W05.html#data-analysis-flowchart",
    "href": "W05.html#data-analysis-flowchart",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Data Analysis Flowchart",
    "text": "Data Analysis Flowchart"
  },
  {
    "objectID": "W05.html#example-covid-19-and-vitamin-d",
    "href": "W05.html#example-covid-19-and-vitamin-d",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Example: COVID-19 and Vitamin D",
    "text": "Example: COVID-19 and Vitamin D\n\n\nDescriptive: frequency of hospitalisations due to COVID-19 in a set of data collected from a group of individuals\nExploratory: examine relationships between a range of dietary factors and COVID-19 hospitalisations\nInferential: examine whether any relationship between taking Vitamin D supplements and COVID-19 hospitalisations found in the sample hold for the population at large\nPredictive: what types of people will take Vitamin D supplements during the next year\nCausal: whether people with COVID-19 who were randomly assigned to take Vitamin D supplements or those who were not are hospitalised\nMechanistic: how increased vitamin D intake leads to a reduction in the number of viral illnesses"
  },
  {
    "objectID": "W05.html#questions-to-questions",
    "href": "W05.html#questions-to-questions",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Questions to questions",
    "text": "Questions to questions\n\nDo you have appropriate data to answer your question?\nDo you have information on confounding variables?\nWas the data you’re working with collected in a way that introduces bias?\n\n\n\nExample\nI want to estimate the average number of children in households in Bremen. I conduct a survey at an elementary school and ask pupils how many children, including themselves, live in their house. Then, I take the average of the responses.\n\nIs this a biased or an unbiased estimate of the number of children in households in Bremen?\nIf biased, will the value be an overestimate or underestimate?"
  },
  {
    "objectID": "W05.html#context-information-is-important",
    "href": "W05.html#context-information-is-important",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Context Information is important!",
    "text": "Context Information is important!\n\nNot all information is in the data!\nPotential confounding variables you infer from general knowledge\nInformation about data collection you may receive from an accompanying report\nInformation about computed variables you may need to look up in accompanying documentation\nInformation about certain variables you may find in an accompanying codebook. For example the exact wording of questions in survey data."
  },
  {
    "objectID": "W05.html#data-science-projects-in-the-course",
    "href": "W05.html#data-science-projects-in-the-course",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Data Science Projects in the Course",
    "text": "Data Science Projects in the Course\n\n\nA project report is the main assessment for the Data Science Tools module.\nThis week more homework repositories will be released.\nThese homework repositories may be updated with new tasks once new concepts have been treated.\nTopics will for example be\n\nCOVID-19\nGermany’s income distribution and income tax\nFuel prices in Western Australia\nPolitical Attitudes in Germany\n…\n\nThe repositories mimick data science projects. Later you will choose a data science project on your own. It can build on these topics or on different datasets which you find interesting."
  },
  {
    "objectID": "W05.html#pca-description",
    "href": "W05.html#pca-description",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "PCA Description",
    "text": "PCA Description\nPrinciple component analysis\n\nis a dimensionality-reduction technique, that means it can be used to reduce the number of variables\ncomputes new variables which represent the data in a different way\ntransforms the data linearly to a new coordinate system where most of the variation in the data can be described with fewer variables than the original data\n\nToday: Quick walk through how to use and interpret it."
  },
  {
    "objectID": "W05.html#example-numerical-variables-of-penguins",
    "href": "W05.html#example-numerical-variables-of-penguins",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Example: Numerical variables of penguins",
    "text": "Example: Numerical variables of penguins\n\npeng <- \n penguins |> \n select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |> \n na.omit()\npeng |> count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      151\n2 Chinstrap    68\n3 Gentoo      123\n\n\nWe have 342 penguins and 4 numeric variables."
  },
  {
    "objectID": "W05.html#two-variables",
    "href": "W05.html#two-variables",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Two Variables",
    "text": "Two Variables\nExample for the new axes.\n\n\n\n\n\n\n\nThe two arrows show the two eigenvectors of the covariance matrix of the two variables scaled by the square root of the corresponding eigenvalues, and shifted so their origins are at the means of both variables."
  },
  {
    "objectID": "W05.html#computation-in-r",
    "href": "W05.html#computation-in-r",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Computation in R",
    "text": "Computation in R\nThe basic function is base-R’s prcomp (there is an older princomp which is not advisable to use).\n\n# prcomp can take a data frame with all numerical vectors as 1st argument\nP <- peng |> select(flipper_length_mm, bill_length_mm) |> prcomp()\n\n\n\nThe base output\n\nP\n\nStandard deviations (1, .., p=2):\n[1] 14.549388  3.981729\n\nRotation (n x k) = (2 x 2):\n                        PC1        PC2\nflipper_length_mm 0.9637169 -0.2669266\nbill_length_mm    0.2669266  0.9637169\n\n\n\nThe summary output\n\nsummary(P)\n\nImportance of components:\n                           PC1     PC2\nStandard deviation     14.5494 3.98173\nProportion of Variance  0.9303 0.06968\nCumulative Proportion   0.9303 1.00000"
  },
  {
    "objectID": "W05.html#the-prcomp-object",
    "href": "W05.html#the-prcomp-object",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "The prcomp object",
    "text": "The prcomp object\nIncludes 4 different related entities.\n\n\nThe standard deviations related to each principal component.\n\nP$sdev\n\n[1] 14.549388  3.981729\n\n\nThe matrix of variable loadings. (It is also the matrix which rotates the original data vectors.)\n\nP$rotation\n\n                        PC1        PC2\nflipper_length_mm 0.9637169 -0.2669266\nbill_length_mm    0.2669266  0.9637169\n\n\n\nThe means for each original variable.\n\nP$center\n\nflipper_length_mm    bill_length_mm \n        200.91520          43.92193 \n\n\nNote, there are also standard deviations of original variables in $scale when this is set to be used.\nThe centered (scaled, if set) and rotated data.\n\nP$x\n\n               PC1         PC2\n  [1,] -20.4797199  0.66892267\n  [2,] -15.5543649 -0.28022355\n  [3,]  -6.6673719 -1.91158941\n  [4,]  -9.5557414 -4.84711693\n  [5,] -11.7528828 -1.54067330\n  [6,] -20.5331052  0.47617930\n  [7,]  -6.9609911 -2.97167796\n  [8,] -10.2497505 -7.35278078\n  [9,] -11.0321810  1.06136223\n [10,] -16.0081402 -1.91854222\n [11,] -21.7904413 -0.31698266\n [12,] -18.9821498  2.32942980\n [13,] -10.9760146 -2.48220170\n [14,]  -5.2977029 -8.20555532\n [15,] -17.2921689 -2.80807586\n [16,]  -7.0944544 -3.45353639\n [17,]  -4.1526997 -0.32526550\n [18,] -18.8431243 -4.66132637\n [19,]  -6.1096072  3.84852331\n [20,] -27.5727425  1.28457691\n [21,] -21.8171340 -0.41335434\n [22,] -13.6241501 -4.55038405\n [23,] -16.8650864 -1.26612888\n [24,] -21.5235147  0.64673421\n [25,] -15.7117398 -4.59476098\n [26,] -18.1518963  1.58064478\n [27,] -14.3237215  0.41656672\n [28,] -29.4734836  1.91480178\n [29,] -21.0697395  2.28505287\n [30,] -23.2640999  1.85518920\n [31,] -23.8780310 -0.36135959\n [32,] -13.6269312 -0.81407674\n [33,] -17.1081014  1.60283324\n [34,]  -7.7083856 -5.67008518\n [35,]  -5.9972743 -3.23860456\n [36,] -11.8863461 -2.02253174\n [37,] -20.6159643  3.92337154\n [38,] -20.8801098 -0.77665262\n [39,] -17.4017207  0.54274469\n [40,] -20.2100122 -2.10366777\n [41,]  -6.5339086 -1.42973098\n [42,] -16.4886080 -3.65323258\n [43,]  -4.6893340  1.48360808\n [44,] -17.1853983 -2.42258912\n [45,] -11.6728048 -1.25155824\n [46,] -18.9821498  2.32942980\n [47,] -22.8342362 -0.33917112\n [48,] -12.6337406 -4.72093895\n [49,]  -9.9883862  1.08355069\n [50,] -15.5276723 -0.18385187\n [51,] -13.4667753 -0.23584662\n [52,] -12.9006672 -5.68465582\n [53,]  -1.3950124 -1.60790371\n [54,] -15.9252810 -5.36573447\n [55,] -10.2286201  0.21620552\n [56,] -15.6878282 -0.76208199\n [57,]  -8.5147276 -1.08862116\n [58,] -21.1737290 -1.83674117\n [59,]  -8.3517906 -4.24669835\n [60,] -17.5324029 -3.67542104\n [61,]  -6.4004453 -0.94787255\n [62,] -17.0252423 -1.84435900\n [63,]  -9.3449812 -0.33983614\n [64,] -18.3092711 -2.73389264\n [65,]  -9.2115179  0.14202229\n [66,]  -7.9486195 -6.53743036\n [67,] -13.1998487  0.72787024\n [68,] -12.6604332 -4.81731064\n [69,]  -3.3758314 -1.26679390\n [70,] -13.3010571 -7.13023111\n [71,] -11.6461122 -1.15518656\n [72,]  -5.8905036 -2.85311781\n [73,]  -3.2718419  2.85500015\n [74,] -12.7672039 -5.20279739\n [75,]  -6.0000554  0.49770275\n [76,] -10.3620834 -0.26565292\n [77,] -18.0957298 -1.96291915\n [78,] -15.4715058 -3.72741580\n [79,]  -6.1869040 -0.17689906\n [80,] -13.9711547 -5.80321597\n [81,]  -5.0096459  0.32714784\n [82,] -15.3380425 -3.24555737\n [83,]  -9.9828239 -6.38906391\n [84,] -11.3230191 -3.73503363\n [85,]  -7.3641622 -0.68094595\n [86,] -12.5536626 -4.43182390\n [87,] -13.3572235 -3.58666718\n [88,] -12.9835263 -2.23746357\n [89,] -11.8596534 -1.92616005\n [90,]  -1.1492162 -8.21317314\n [91,]   3.1833380 -3.80988186\n [92,] -17.9861781 -5.31373971\n [93,] -15.5276723 -0.18385187\n [94,] -15.4715058 -3.72741580\n [95,]   5.9944106 -4.89977671\n [96,] -12.0731947 -2.69713354\n [97,]  -5.7036550 -2.17851601\n [98,] -24.9724301 -4.31259873\n [99,]  -8.7844354  1.68396928\n[100,] -10.9732334 -6.21850901\n[101,]   1.2292116 -3.37240036\n[102,] -18.9259834 -1.21413413\n[103,] -12.1532727 -2.98624860\n[104,]  -9.2354294 -3.69065670\n[105,] -17.4284133  0.44637301\n[106,]  -3.2662796 -4.61761446\n[107,] -12.0465021 -2.60076185\n[108,] -20.7466465 -0.29479419\n[109,]  -3.9658510  0.34933630\n[110,]  -4.3634598 -4.83254629\n[111,]  -9.1075284  4.26381634\n[112,]  -8.7549616 -1.95596634\n[113,]  -4.2327776 -0.61438056\n[114,] -10.7090880 -1.51848484\n[115,]  -5.0630312  0.13440447\n[116,] -13.8671651 -1.68142192\n[117,]  -3.6132842 -5.87044638\n[118,] -13.6775354 -4.74312742\n[119,] -12.2361318  0.46094364\n[120,] -15.4715058 -3.72741580\n[121,]  -4.4702304 -5.21803304\n[122,] -25.0046850  3.06364419\n[123,]   0.3722654 -2.71998702\n[124,] -16.7021493 -4.42420607\n[125,]  -2.7324265 -2.69018073\n[126,] -10.9226292 -2.28945833\n[127,]  -6.3470600 -0.75512918\n[128,] -10.8692439 -2.09671496\n[129,]   8.8027021 -2.25336424\n[130,] -11.9664241 -2.31164679\n[131,]  -3.9925437  0.25296462\n[132,]  -9.5290487 -4.75074525\n[133,]  -3.5598989 -5.67770301\n[134,] -14.9643453 -1.89635376\n[135,] -11.2724149  0.19401705\n[136,] -11.7767943 -5.37335229\n[137,]  -1.8754802 -3.34259407\n[138,] -17.1853983 -2.42258912\n[139,]  -8.7549616 -1.95596634\n[140,]  -8.6214983 -1.47410791\n[141,] -14.2970288  0.51293840\n[142,] -15.6021880 -7.94558153\n[143,] -11.3791856 -0.19146969\n[144,] -10.3593023 -4.00196022\n[145,] -16.6515451 -0.49515539\n[146,] -11.7795755 -1.63704499\n[147,] -18.2558858 -2.54114927\n[148,]  -7.8151562 -6.05557193\n[149,]  -9.2621221 -3.78702838\n[150,] -15.5248912 -3.92015917\n[151,]  -0.5647588 -2.35668874\n[152,]  10.3002722 -0.59285711\n[153,]  29.6519063 -1.90596663\n[154,]  10.0305645  2.17973333\n[155,]  18.0873039  1.29715250\n[156,]  14.5555295 -0.21498819\n[157,]   9.4433259  0.05955623\n[158,]  10.1134236 -1.26745892\n[159,]  18.1701630 -2.15003975\n[160,]   7.6254440 -2.75741114\n[161,]  14.3419882 -0.98596168\n[162,]  11.8034045 -6.40496458\n[163,]  15.8929436  0.86728882\n[164,]  13.0312668 -1.97186701\n[165,]  12.8416371  1.08983849\n[166,]   9.2564773 -0.61504558\n[167,]  16.9367385  0.88947729\n[168,]   8.2421563 -4.27716966\n[169,]  20.7649133 -0.27460078\n[170,]   8.3995311  0.03736776\n[171,]  21.5951668 -1.02338580\n[172,]  18.1406893  1.48989587\n[173,]  13.8882130 -2.62428035\n[174,]  12.3344765 -0.74122355\n[175,]  14.2085249 -1.46782012\n[176,]  13.3009745 -4.74445745\n[177,]  14.1551396 -1.66056349\n[178,]  14.6917739 -3.46943706\n[179,]  14.6089148 -0.02224482\n[180,]   9.8971012  1.69787490\n[181,]  20.0147377  0.76329931\n[182,]  21.2214696 -2.37258941\n[183,]   7.4919807 -3.23926957\n[184,]   6.1784781 -0.48886760\n[185,]  32.2144016  7.34571526\n[186,]  19.7745037 -0.10404587\n[187,]  19.5876551 -0.77864767\n[188,]  11.2934628 -4.49971932\n[189,]  17.5562319 -4.36658853\n[190,]   6.8485757 -1.81588274\n[191,]   8.1031307  2.71358652\n[192,]   6.5015712 -3.06871466\n[193,]  24.7265513 -0.95682041\n[194,]   9.1230140 -1.09690401\n[195,]  16.0530996  1.44551894\n[196,]  22.0756347  0.71130455\n[197,]  15.4152569 -4.60370884\n[198,]   9.1763994 -0.90416063\n[199,]  24.9667853 -0.08947523\n[200,]  11.9073940 -2.28317054\n[201,]  13.9149057 -2.52790867\n[202,]   9.4700186  0.15592792\n[203,]  19.6143478 -0.68227599\n[204,]   9.0696287 -1.28964738\n[205,]  24.8600146 -0.47496198\n[206,]  16.1893440 -1.80892993\n[207,]  18.6801047 -4.05528501\n[208,]   6.7951904 -2.00862611\n[209,]  18.8135680 -3.57342658\n[210,]   6.6350345 -2.58685623\n[211,]  23.9763758  0.08107968\n[212,]   7.1955803 -0.56305082\n[213,]  19.9641335 -3.16575137\n[214,]  13.0846521 -1.77912364\n[215,]  31.7634075  1.97108929\n[216,]  17.9299291 -3.01738492\n[217,]  29.5985210 -2.09871001\n[218,]  13.2181154 -1.29726521\n[219,]  28.5547261 -2.12089847\n[220,]  18.2797148 -5.50086030\n[221,]  23.0927369  0.63712133\n[222,]  15.5459390 -0.38554310\n[223,]  20.0175188 -2.97300799\n[224,]  20.4979867 -1.23831764\n[225,]  16.1893440 -1.80892993\n[226,]  15.1989345 -1.63837502\n[227,]  29.2782091 -3.25517024\n[228,]   8.7465357  1.29019969\n[229,]  20.3083569  1.82338786\n[230,]  13.9149057 -2.52790867\n[231,]  21.6246406 -4.66332142\n[232,]  12.0647688  2.03136689\n[233,]  21.6457710  2.90566487\n[234,]  11.6109936  0.39304822\n[235,]  23.8696051 -0.30440707\n[236,]  10.9436771 -2.01624394\n[237,]  27.9380138 -0.60113995\n[238,]  16.3255884 -5.06337880\n[239,]  18.4343085  2.54998442\n[240,]  11.6376863  0.48941990\n[241,]  30.2124521  0.11783878\n[242,]  17.4199874 -1.11213966\n[243,]  28.3117111  0.74806366\n[244,]  11.1038331 -1.43801382\n[245,]  23.7361418 -0.78626550\n[246,]  12.7643402 -2.93558388\n[247,]  26.0105801 -0.06728677\n[248,]  15.9997143  1.25277557\n[249,]  21.1146989 -2.75807616\n[250,]   3.2044684  3.75910443\n[251,]  25.1269412  0.48875489\n[252,]  18.6506309 -0.41534939\n[253,]  29.2993395  4.31381605\n[254,]  14.4487589 -0.60047494\n[255,]  27.4842386 -2.23945862\n[256,]  15.4391684 -0.77102985\n[257,]  14.3419882 -0.98596168\n[258,]   8.1620783 -4.56628472\n[259,]  19.9585712  4.30686324\n[260,]   6.6617271 -2.49048455\n[261,]   8.9066916  1.86842981\n[262,]  16.2933335  2.31286412\n[263,]  28.6348041 -1.83178341\n[264,]  11.5336968 -3.63237414\n[265,]  30.0522962 -0.46039134\n[266,]  16.1092660 -2.09804499\n[267,]  31.0132319  3.00898937\n[268,]  15.6554908 -3.73636366\n[269,]  21.6218595 -0.92701412\n[270,]  13.4850420 -0.33354834\n[271,]  14.3419882 -0.98596168\n[272,]  22.0489420  0.61493287\n[273,]  11.0237551 -1.72712888\n[274,]  13.2420270  2.53541378\n[275,]  -7.9035776  4.86423493\n[276,]  -3.1144671  7.16953757\n[277,]  -5.6586131  9.22314928\n[278,] -12.0520643  4.87185275\n[279,]  -1.4300484  9.50464651\n[280,]  -2.4682810  2.00984344\n[281,] -21.5023843  8.21572050\n[282,]  -1.8037456  8.15544290\n[283,]  -5.1458903  3.58159671\n[284,]  -0.8400288  7.88851631\n[285,]  -6.9131681  4.69368002\n[286,]  -4.5881256  9.34170943\n[287,] -14.5161323  7.21457952\n[288,]   2.2379704  7.76233833\n[289,]  -9.9911673  4.81985800\n[290,]   1.8375806  6.31676303\n[291,]  -2.0706722  7.19172604\n[292,] -15.4348073 18.88317139\n[293,]  -9.8577040  5.30171643\n[294,]  -4.2917252  6.66549067\n[295,] -19.5988621  3.84918832\n[296,]  -8.3334413  7.05859525\n[297,] -13.6030197  3.01860225\n[298,]  -5.8454617  8.54854747\n[299,]  -4.9590417  4.25619852\n[300,]  -1.6168970  8.83004470\n[301,]   0.8738637  6.58368963\n[302,]   0.6069371  5.61997276\n[303,]  -8.8939871  5.03478983\n[304,]   6.3063792  7.46560544\n[305,] -14.2169508  0.80205346\n[306,]   2.8252089  9.88251543\n[307,] -13.7898683  2.34400044\n[308,]   3.8984776  6.26476828\n[309,]  -4.1582619  7.14734911\n[310,]  -0.8906330  3.95946563\n[311,]  -4.7188078  5.12354369\n[312,]  10.9114222  5.35999898\n[313,]  -7.7968070  5.24972167\n[314,]   6.4932278  8.14020725\n[315,]  10.1106424  2.46884839\n[316,] -12.8022399  5.90975284\n[317,]  -2.8742331  8.03688275\n[318,]  -4.3156367  2.83281168\n[319,]  -2.8742331  8.03688275\n[320,]   1.9176585  6.60587809\n[321,]  -8.8700756  8.86746882\n[322,]  12.0380762  1.93499520\n[323,] -11.3875289 11.01745222\n[324,]  -1.2404187  6.44294101\n[325,]  -0.7304770  4.53769575\n[326,]   2.0778145  7.18410821\n[327,]  -7.1534020  3.82633484\n[328,]   3.8183996  5.97565322\n[329,] -13.7898683  2.34400044\n[330,]  -1.5635117  9.02278808\n[331,]  -9.2142990  3.87832960\n[332,]   3.4447024  4.62644961\n[333,]   2.7212194  5.76072138\n[334,]  -6.2163778  3.46303656\n[335,]   7.0298621  6.33133367\n[336,] -10.7146502  5.95412977\n[337,]  -5.2259683  3.29248165\n[338,]   9.0345927  9.82290284\n[339,]   0.9328113 -0.69618161\n[340,]  -6.1123883  7.58483061\n[341,]  10.5911103  4.20353874\n[342,]  -1.1336480  6.82842776"
  },
  {
    "objectID": "W05.html#pca-as-exploratory-data-analysis",
    "href": "W05.html#pca-as-exploratory-data-analysis",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "PCA as Exploratory Data Analysis",
    "text": "PCA as Exploratory Data Analysis\nSuppose we do a PCA with all 342 penguins (rows) and all 4 numeric variables.\n\nHow long will the vector of standard deviations be? 4\n\nWhat dimensions will the rotation matrix have? 4 x 4\n\nWhat dimensions will the rotated data frame have? 342 x 4\n\n\nWhen we do a PCA for exploration there are 3 things to look at:\n\nThe data in PC coordinates - the centered (scaled, if set) and rotated data.\n\nThe rotation matrix - the variable loadings.\nThe variance explained by each PC - based on the standard deviations."
  },
  {
    "objectID": "W05.html#all-variables",
    "href": "W05.html#all-variables",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "All variables",
    "text": "All variables\nNow, with scale = TRUE (recommended). Data will be centered and scaled (a.k.a. standardized) first.\n\npeng_PCA <- peng |> select(-species) |> \n prcomp(scale = TRUE)\npeng_PCA\n\nStandard deviations (1, .., p=4):\n[1] 1.6594442 0.8789293 0.6043475 0.3293816\n\nRotation (n x k) = (4 x 4):\n                         PC1          PC2        PC3        PC4\nbill_length_mm     0.4552503 -0.597031143 -0.6443012  0.1455231\nbill_depth_mm     -0.4003347 -0.797766572  0.4184272 -0.1679860\nflipper_length_mm  0.5760133 -0.002282201  0.2320840 -0.7837987\nbody_mass_g        0.5483502 -0.084362920  0.5966001  0.5798821"
  },
  {
    "objectID": "W05.html#explore-data-in-pc-coordinates",
    "href": "W05.html#explore-data-in-pc-coordinates",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Explore data in PC coordinates",
    "text": "Explore data in PC coordinates\n\n\n\nStart plotting PC1 against PC2. By default these are the most important ones. Drill deeper later.\nAppend the original data. Here used to color by species.\n\n\n\nplotdata <- peng_PCA$x |> as_tibble() |> bind_cols(peng)\nplotdata |> ggplot(aes(PC1, PC2, color = species)) +\n geom_point() +\n coord_fixed() + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W05.html#variable-loadings",
    "href": "W05.html#variable-loadings",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Variable loadings",
    "text": "Variable loadings\n\nThe columns of the rotation matrix shows how the original variables load on the principle components.\nWe can try to interpret these loadings and give descriptive names to principal components.\ntidy extracts the rotation matrix in long format with a PC, a column (for the original variable name), and a value variable.\n\n\npeng_PCA$rotation |> as_tibble(rownames = \"variable\") |> \n pivot_longer(starts_with(\"PC\"), names_to = \"PC\", values_to = \"value\") |> \n ggplot(aes(value, variable)) + geom_col() + geom_vline(xintercept = 0, color = \"blue\") +\n facet_wrap(~PC, nrow = 1) + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W05.html#variance-explained",
    "href": "W05.html#variance-explained",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Variance explained",
    "text": "Variance explained\n\nPrinciple components are by default sorted by importance.\nThe squares of the standard deviation for each component gives its variances and variances have to sum up to the sum of the variances of the original variables.\n\nWhen original variables were standardized their original variances are all each one. Consequently, the variances of the principal components sum up to the number of original variables.\n\nA typical plot to visualize the importance of the components is to plot the percentage of the variance explained by each component.\n\n\ntibble(PC = 1:4, sdev = peng_PCA$sdev) |> \n mutate(percent = sdev^2/sum(sdev^2) * 100) |>\n ggplot(aes(PC, percent)) + geom_col() + theme_grey(base_size = 20)"
  },
  {
    "objectID": "W05.html#interpretations-1",
    "href": "W05.html#interpretations-1",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretations (1)",
    "text": "Interpretations (1)\n\n\nThe first component explains almost 70% of the variance. So most emphasize should be on this.\nThe first two explain about 88% of the total variance."
  },
  {
    "objectID": "W05.html#interpretations-2",
    "href": "W05.html#interpretations-2",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretations (2)",
    "text": "Interpretations (2)\n\n\nTo score high on PC1 a penguin needs to be generally large but with low bill depth.\nPenguins scoring high on PC2 are penguins with generally small bills."
  },
  {
    "objectID": "W05.html#interpretations-3",
    "href": "W05.html#interpretations-3",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretations (3)",
    "text": "Interpretations (3)"
  },
  {
    "objectID": "W05.html#apply-pca",
    "href": "W05.html#apply-pca",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Apply PCA",
    "text": "Apply PCA\n\nBesides standardization, PCA may benefit by preprocessing steps of data transformation with variables with skew distributions (log, square-root, or Box-Cox transformation). This may result in less outliers.\nPCA is a often a useful step of exploratory data analysis when you have a large number of numerical variables to show the empirical dimensionality of the data and its structure\nLimitation: PCA is only sensitive for linear relation ships (no U-shaped) or the like\nThe principal components can be used as predictors in a model instead of the raw variables."
  },
  {
    "objectID": "W05.html#properties-of-pca",
    "href": "W05.html#properties-of-pca",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Properties of PCA",
    "text": "Properties of PCA\n\n\nThe principal components (the columns of the rotation matrix) are maximally uncorrelated (actually they are even orthogonal).\nThis also holds for the columns of the rotated data.\nThe total variances of all prinicipal components sum up to the number of variables (when variables are standardized)\nThe PCA is unique. All principle components together are a complete representation of the data. (Unlike other technique of dimensionality reduction which may rely on starting values, random factors, or tuning parameters)"
  },
  {
    "objectID": "W05.html#relations-of-pca",
    "href": "W05.html#relations-of-pca",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Relations of PCA",
    "text": "Relations of PCA\n\n\nA technique similar in spirit is factor analysis (e.g. factanal). It is more theory based as it requires to specify to the theoriezed number of factors up front.\nPCA is an example of the importance of linear algebra (“matrixology”) in data science techniques.\n\nPCA is based on the eigenvalue decomposition of the covariance matrix (or correlation matrix in the standardized case) of the data.\n\n\n\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Developing Schedule\n\n\n\n\n\nThe schedule is tentative it will be continuously updated also with readings/viewings, homework and learnings.\nImportant Links:\nGitHub organization of the course: https://github.com/CU-F23-MDSSB-01-Concepts-Tools\nOrganization repository with non-public information and FAQ in Discussions: https://github.com/CU-F23-MDSSB-01-Concepts-Tools/Organization\n\n\n\n\nTabular Schedule\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopic\nTools Course\n\n\n\n\n1\n09-04, 09-07\nWhat is Data Science? Course Organization, Our software toolkit  Slides Week 1, Achievements Week 1\nR\n\n\n2\n09-11, 09-14\nData Visualization, Data Formats  Slides Week 2 Intro, Slides Week 2 Main Part, Repo with Example Visualization of our data science profiles Achievements Week 2\nR\n\n\n3\n09-18, 09-21\nData Import, Data Wrangling, Relational Data  Slides Week 3\npython\n\n\n4\n09-25, 09-28\nMath refresh, Function Programming, Descriptive Statistics  Slides Week 4\npython\n\n\n5\n10-02, 10-05\nSummary Statistics, Exploratory Data Analysis, Principle Component Analysis  Slides Week 5\nR\n\n\n6\n10-09, 10-12\nEpidemic Modeling, Calculus, Linear Model, Fitting a Linear Model  Slides Week 6\npython\n\n\n7\n10-16, 10-19\nFitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables  Slides Week 7\nR\n\n\n8\n10-23, 10-26\nLogistic Regression, Classification Problems, Prediction  Slides Week 8\n\n\n\n9\n10-30, 11-02\nIntroduction to Project Reports (in Organization repo) Hypothesis Testing  Slides Week 9\nR\n\n\n10\n11-06, 11-09\nProbability, Random Variables, Statistics, Cluster Analysis\npython\n\n\n11\n11-13, 11-16\nModeling Mindsets, Domain Knowledge, Communication\nR\n\n\n12\n11-20, 11-23\nTo be decided\npython\n\n\n13\n11-27, 11-30\nTo be decided\nR / python\n\n\n14\n12-04, 12-07\nCourse Review, Exam Preparation\nPresentations\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nAll topics in the future are subject to change and adaptation to student needs. Topics in italics are tentative.\n\n\n\n\n\n1 Achievements Week 1\nYou have\n\nread the Syllabus and understood the course organization\na running R with RStudio installation an you computer\ndone the git-GitHub dance and know what the tools are in principle\nrendered quarto documents and understood its idea\nmade your first steps with R\nfinished Homework 01 (double check your if your html-file looks nice and that all files including all necessary figure are also in the repository on GitHub)\n\nAdditional material:\n\nQuarto: Watch https://www.youtube.com/watch?v=_f3latmOhew from Mine Çetinkaya-Rundel (co-developer of quarto, R for Data Science, datasciencebox).\nR: Read the following sections in R for Data Science\n\nCh 1:Introduction\nWhole Game (the outline of the part)\n\n\n\n\n2 Achievements Week 2\nYou have\nTo be completed\nAdditional material:\n\nggplot: Read the following sections in R for Data Science\n\nCh 2: Data Visualization\nIf you want to go deeper already now, read the chapter Ch 10: Layers\n\n\nTo be completed\n\n\n3 Achievements Week 3\nYou have To be completed\nAdditional material:\nRead the following sections in R for Data Science\n\nCh 6: Tidy Data\nCh 8: Data Import\nCh 20: Joins\n\n\n\n4 Achievements Week 4\nYou have To be completed\nAdditional material:\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146.\nRead the following sections in R for Data Science\n\nCh 26: Functions\nCh 27: Iteration\nCh 11: Exploratory Data Analysis"
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "Create a homework report",
    "section": "",
    "text": "In the homework repositories following Homework 02 you are to create reports in quarto yourself.\nThese are suggestions how you create such a report."
  },
  {
    "objectID": "reports.html#yaml",
    "href": "reports.html#yaml",
    "title": "Create a homework report",
    "section": "1 YAML",
    "text": "1 YAML\nA useful YAML header for a homework report is\n---\ntitle: \"Homework 02\"\nformat:   \n  html:                     \n    standalone: true        # quarto will render to a html-file with a header \n                            # and all css and javascript included (no extra directory with files for these needed)\n    embed-resources: true   # The html will also embed all figures you produce \n                            # (no extra directory with files for these needed)\n    code-fold: true         # Code does not appear visually in the report, but can be \"folded out\"\n    number-sections: true   # Usually you want to number the sections in a report\n    toc : true              # Table of contents make navigation in longer files easier\n---\n\n\n\n\n\n\nWriting in a jupyter notebook (.ipynb)\n\n\n\n\n\nInstead of a .qmd file you can also write in a jupyter notebook (.ipynb) and render this file with quarto. To that end you start your notebook with the YAML header above in a raw cell."
  },
  {
    "objectID": "reports.html#after-the-yaml",
    "href": "reports.html#after-the-yaml",
    "title": "Create a homework report",
    "section": "2 After the YAML",
    "text": "2 After the YAML\nTypically, you have a chunk loading your packages directly after the YAML header.\nThis is for an R report:\n```{r}\n#| label: load-packages  \nlibrary(tidyverse) # You always need the tidyverse\n\n# More helpful packages \nlibrary(patchwork) # for combining plots\n```"
  },
  {
    "objectID": "reports.html#structuring-your-report",
    "href": "reports.html#structuring-your-report",
    "title": "Create a homework report",
    "section": "3 Structuring your report",
    "text": "3 Structuring your report\nMake headers with # and maybe ## for second level headers."
  },
  {
    "objectID": "W09.html#organ-donors",
    "href": "W09.html#organ-donors",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Organ donors",
    "text": "Organ donors\nPeople providing an organ for donation sometimes seek the help of a special “medical consultant”. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients.\nOne consultant tried to attract patients by noting that the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!)."
  },
  {
    "objectID": "W09.html#data",
    "href": "W09.html#data",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Data",
    "text": "Data\n\norgan_donor <- tibble(\n  outcome = c(rep(\"complication\", 3), rep(\"no complication\", 59))\n)\n\n\norgan_donor |>\n  count(outcome)\n\n# A tibble: 2 × 2\n  outcome             n\n  <chr>           <int>\n1 complication        3\n2 no complication    59"
  },
  {
    "objectID": "W09.html#parameter-vs.-statistic",
    "href": "W09.html#parameter-vs.-statistic",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Parameter vs. statistic",
    "text": "Parameter vs. statistic\nA parameter for a hypothesis test is the “true” value of interest. We typically estimate the parameter using a sample statistic as a point estimate.\n\\(p\\): true rate of complication, here 0.1 (10% complication rate in US)\n\\(\\hat{p}\\): rate of complication in the sample = \\(\\frac{3}{62}\\) = 0.048"
  },
  {
    "objectID": "W09.html#correlation-vs.-causation",
    "href": "W09.html#correlation-vs.-causation",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Correlation vs. causation",
    "text": "Correlation vs. causation\nIs it possible to assess the consultant’s claim using the data?\nNo. The claim is: There is a causal connection, but the data are observational. For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate (for example).\n\nWhile it is not possible to assess the causal claim, it is still possible to test for an association using these data. For this question we ask, could the low complication rate of \\(\\hat{p}\\) = 0.048 be due to chance?"
  },
  {
    "objectID": "W09.html#two-claims",
    "href": "W09.html#two-claims",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Two claims",
    "text": "Two claims\n\nNull hypothesis: “There is nothing going on”\n\nComplication rate for this consultant is no different than the US average of 10%\n\nAlternative hypothesis: “There is something going on”\n\nComplication rate for this consultant is lower than the US average of 10%"
  },
  {
    "objectID": "W09.html#hypothesis-testing-as-a-court-trial",
    "href": "W09.html#hypothesis-testing-as-a-court-trial",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?”\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "W09.html#hypothesis-testing-framework",
    "href": "W09.html#hypothesis-testing-framework",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\), that represents the status quo\nSet an alternative hypothesis, \\(H_A\\), that represents the research question, i.e. what we are testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value.\nDefinition: Probability of observed or more extreme outcome given that the null hypothesis is true.\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "W09.html#setting-the-hypotheses",
    "href": "W09.html#setting-the-hypotheses",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Setting the hypotheses",
    "text": "Setting the hypotheses\nWhich of the following is the correct set of hypotheses for the claim that the consultant has lower complication rates?\n\n\\(H_0: p = 0.10\\); \\(H_A: p \\ne 0.10\\)\n\\(H_0: p = 0.10\\); \\(H_A: p > 0.10\\)\n\\(H_0: p = 0.10\\); \\(H_A: p < 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} \\ne 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} > 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} < 0.10\\)\n\n\nCorrect is c. Hypotheses are about the true rate of complication \\(p\\) not the observed ones \\(\\hat{p}\\)"
  },
  {
    "objectID": "W09.html#simulating-the-null-distribution",
    "href": "W09.html#simulating-the-null-distribution",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Simulating the null distribution",
    "text": "Simulating the null distribution\nSince \\(H_0: p = 0.10\\), we need to simulate a null distribution where the probability of success (complication) for each trial (patient) is 0.10.\nHow should we simulate the null distribution for this study using a bag of chips?\n\nHow many chips? For example 10 which makes 10% choices possible\nHow many colors? 2\nWhat should colors represent? “complication”, “no complication”\nHow many draws? 62 as the data\nWith replacement or without replacement? With replacement\n\nWhen sampling from the null distribution, what would be the expected proportion of “complications”? 0.1"
  },
  {
    "objectID": "W09.html#simulation",
    "href": "W09.html#simulation",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Simulation!",
    "text": "Simulation!\n\nset.seed(1234)\noutcomes <- c(\"complication\", \"no complication\")\nsim1 <- sample(outcomes, size = 62, prob = c(0.1, 0.9), replace = TRUE)\nsim1\n\n [1] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n [5] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n [9] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[13] \"no complication\" \"complication\"    \"no complication\" \"no complication\"\n[17] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[21] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[25] \"no complication\" \"no complication\" \"no complication\" \"complication\"   \n[29] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[33] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[37] \"no complication\" \"no complication\" \"complication\"    \"no complication\"\n[41] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[45] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[49] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[53] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[57] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[61] \"no complication\" \"no complication\"\n\nsum(sim1 == \"complication\")/62\n\n[1] 0.0483871\n\n\nOh OK, this was is pretty close to the consultant’s rate. But maybe it was a rare event?"
  },
  {
    "objectID": "W09.html#more-simulation",
    "href": "W09.html#more-simulation",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "More simulation!",
    "text": "More simulation!\n\none_sim <- function() sample(outcomes, size = 62, prob = c(0.1, 0.9), replace = TRUE)\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1290323\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1290323\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.09677419\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.09677419\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1774194\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1129032\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1129032"
  },
  {
    "objectID": "W09.html#automating-with-tidymodels",
    "href": "W09.html#automating-with-tidymodels",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Automating with tidymodels1",
    "text": "Automating with tidymodels1\n\n\n\norgan_donor\n\n# A tibble: 62 × 1\n   outcome        \n   <chr>          \n 1 complication   \n 2 complication   \n 3 complication   \n 4 no complication\n 5 no complication\n 6 no complication\n 7 no complication\n 8 no complication\n 9 no complication\n10 no complication\n# ℹ 52 more rows\n\n\n\n\nset.seed(10)\nnull_dist <- organ_donor |>\n  specify(response = outcome, success = \"complication\") |>\n  hypothesize(null = \"point\", \n              p = c(\"complication\" = 0.10, \"no complication\" = 0.90)) |> \n  generate(reps = 100, type = \"draw\") |> \n  calculate(stat = \"prop\")\nnull_dist\n\nResponse: outcome (factor)\nNull Hypothesis: point\n# A tibble: 100 × 2\n   replicate   stat\n       <int>  <dbl>\n 1         1 0.0323\n 2         2 0.0645\n 3         3 0.0968\n 4         4 0.0161\n 5         5 0.161 \n 6         6 0.0968\n 7         7 0.0645\n 8         8 0.129 \n 9         9 0.161 \n10        10 0.0968\n# ℹ 90 more rows\n\n\n\n\nOf course, you can also do it in your own way without packages."
  },
  {
    "objectID": "W09.html#visualizing-the-null-distribution",
    "href": "W09.html#visualizing-the-null-distribution",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Visualizing the null distribution",
    "text": "Visualizing the null distribution\n\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Null distribution\")"
  },
  {
    "objectID": "W09.html#calculating-the-p-value-visually",
    "href": "W09.html#calculating-the-p-value-visually",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Calculating the p-value, visually",
    "text": "Calculating the p-value, visually\nWhat is the p-value:1 How often was the simulated sample proportion at least as extreme as the observed sample proportion?\n\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Null distribution\")  +\n geom_vline(xintercept = 3/62, color = \"red\")\n\n\n\n\nThe name p-value has nothing to do with the value \\(p\\) we are currently trying to use as population parameter."
  },
  {
    "objectID": "W09.html#calculating-the-p-value-directly",
    "href": "W09.html#calculating-the-p-value-directly",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Calculating the p-value, directly",
    "text": "Calculating the p-value, directly\n\nnull_dist |>\n summarise(p_value = sum(stat <= 3/62)/n())\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1    0.13\n\n\nThis is the fraction of simulations where complications was equal or below 0.0483871."
  },
  {
    "objectID": "W09.html#significance-level",
    "href": "W09.html#significance-level",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Significance level",
    "text": "Significance level\n\nA significance level \\(\\alpha\\) is a threshold we make up to make our judgment about the plausibility of the null hypothesis being true given the observed data.\nWe often use \\(\\alpha = 0.05 = 5\\%\\) as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model.\nIf p-value < \\(\\alpha\\), reject \\(H_0\\) in favor of \\(H_A\\): The data provide convincing evidence for the alternative hypothesis.\nIf p-value > \\(\\alpha\\), fail to reject \\(H_0\\) in favor of \\(H_A\\): The data do not provide convincing evidence for the alternative hypothesis.\n\nWhat is the conclusion of the hypothesis test?\nSince the p-value is greater than the significance level, we fail to reject the null hypothesis. These data do not provide convincing evidence that this consultant incurs a lower complication rate than the 10% overall US complication rate."
  },
  {
    "objectID": "W09.html#simulations-is-not-sufficient",
    "href": "W09.html#simulations-is-not-sufficient",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "100 simulations is not sufficient",
    "text": "100 simulations is not sufficient\n\nWe simulate 15,000 times to get an accurate distribution.\n\n\nnull_dist <- organ_donor |>\n  specify(response = outcome, success = \"complication\") |>\n  hypothesize(null = \"point\", \n              p = c(\"complication\" = 0.10, \"no complication\" = 0.90)) |> \n  generate(reps = 15000, type = \"simulate\") |> \n  calculate(stat = \"prop\")\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  geom_vline(xintercept = 3/62, color = \"red\")"
  },
  {
    "objectID": "W09.html#our-more-robust-p-value",
    "href": "W09.html#our-more-robust-p-value",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Our more robust p-value",
    "text": "Our more robust p-value\nFor the null distribution with 15,000 simulations\n\nnull_dist |>\n  filter(stat <= 3/62) |>\n  summarise(p_value = n()/nrow(null_dist))\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.125\n\n\nOh OK, our fist p-value was much more borderline in favor of the alternative hypothesis."
  },
  {
    "objectID": "W09.html#xkcd-on-p-values",
    "href": "W09.html#xkcd-on-p-values",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "xkcd on p-values",
    "text": "xkcd on p-values\n\n\n \n\n\n\nSignificance levels are fairly arbitrary. Sometimes they are used (wrongly) as definitive judgments\nThey can even be used to do p-hacking: Searching for “significant” effects in observational data\nIn parts of science it has become a “gamed” performance metric.\nThe p-value says nothing about effect size!"
  },
  {
    "objectID": "W09.html#p-value-misinterpretation",
    "href": "W09.html#p-value-misinterpretation",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "p-value misinterpretation",
    "text": "p-value misinterpretation\np-values do not measure1\n\nthe probability that the studied hypothesis is true\nthe probability that the data were produced by random chance alone\nthe size of an effect\nthe importance of a result” or “evidence regarding a model or hypothesis”.\n\n\nCorrect:\nThe p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\np-values and significance tests, when properly applied and interpreted, increase the rigor of the conclusions drawn from data.2\n\nFrom the American Statistical Association (ASA) 2016From the American Statistical Association (ASA) 2019"
  },
  {
    "objectID": "W09.html#p-value-in-model-outputs",
    "href": "W09.html#p-value-in-model-outputs",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "p-value in model outputs",
    "text": "p-value in model outputs\nModel output for a linear model with palmer penguins.\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(bill_length_mm ~ bill_depth_mm, data = penguins) |> \n tidy()\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     55.1       2.52      21.9  6.91e-67\n2 bill_depth_mm   -0.650     0.146     -4.46 1.12e- 5\n\n\nModel output for a logistic regression model with email from openintro\n\nlogistic_reg() |>  set_engine(\"glm\") |>\n  fit(spam ~ from + cc, data = email, family = \"binomial\") |>\n  tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  13.6      309.        0.0439   0.965\n2 from1       -15.8      309.       -0.0513   0.959\n3 cc            0.00423    0.0193    0.220    0.826\n\n\nWhat do the p-values mean? What is the null hypothesis?\n\nNull-Hypothesis: There is no relationship between the predictor variable and the response variable, that means that the coefficient is equal to zero."
  },
  {
    "objectID": "W09.html#probability-topics-for-data-science",
    "href": "W09.html#probability-topics-for-data-science",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Probability Topics for Data Science",
    "text": "Probability Topics for Data Science\n\n\nThe concept of probability from the mathematical perspective.\n\nWhat are probabilistic events, probability functions and random variables.\n\nRandom variables: A probabilistic view on variables in a data frame\nProbabilistic simulations. For example resampling and cross-validation.\nThe confusion matrix from a probabilistic view: Conditional probabilities\nModeling distributions of variables: The concepts of discrete and continuous probability distributionsa and some examples.\nThe central limit theorem or why the normal distribution is so important.\nProbability theory and statistics"
  },
  {
    "objectID": "W09.html#what-is-probability",
    "href": "W09.html#what-is-probability",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "What is probability",
    "text": "What is probability\n\n\nThe systematic and rigorous treatment of uncertainty.\nWe have a certain intuition of probability visible in sentences like:\n\n“That’s not very probable.”\n“That is likely.”\n“I don’t have a prior for that.”\n\nWe can call it a model for uncertainty: A simplified but formalized way to think about uncertain events.\nThe model of probability is one of the most successful mathematical models. It is used in many domains."
  },
  {
    "objectID": "W09.html#two-different-flavors",
    "href": "W09.html#two-different-flavors",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Two different flavors",
    "text": "Two different flavors\n\nModel for uncertainty as subjective or objective probability of an uncertain event.\nThey are also called Bayesian vs. Frequentist interpretation of probability. * They differ in the way of reasoning, the interpretation what is random, and in terminology."
  },
  {
    "objectID": "W09.html#objective-interpretation",
    "href": "W09.html#objective-interpretation",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Objective interpretation",
    "text": "Objective interpretation\nProbability is relative frequency in the limit of indefinite sampling. It is the long run behavior of non-deterministic outcomes.\n\nIf we flip the same coin several times the relative frequency of the number of HEADS converges to the probability of HEADS.\nIf we draw people at random the relative frequency of people with heights between 1.70m and 1.75m converges to the probability that a person is in this range.\nIn this frequentist philosophy the parameters of the population we sample from is fixed and the data is a random selection."
  },
  {
    "objectID": "W09.html#subjective-interpretation",
    "href": "W09.html#subjective-interpretation",
    "title": "W#09: Hypothesis Testing, Probability for Data Science",
    "section": "Subjective interpretation",
    "text": "Subjective interpretation\nProbability is a belief a person has about the likelihood that an event occurs. This can be formalized by the condition under which a person would make a bet.\n\nIf say we flip a coin, we can offer a person a bet for HEADS, e.g. you gain 1€ when the outcome is heads and you lose 2€ when it is TAILS. The person would be indifferent between accepting and rejecting the bet if their subjective belief is that HEADS will come two times more likely than TAILS (odds 2:1 or probability 2/3). The subjective probability is the probability where the person is indifferent.\nWe can make up similar bets for the heights of randomly drawn people.\nIn Bayesian philosophy the data we know is fixed but the parameters of the population are random and associated with probabilities.\n\nThe objective and subjective views are not mutually exclusive and it is not important to take a side.\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W08.html#logistic-regression-1",
    "href": "W08.html#logistic-regression-1",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLogistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors.\nThe distribution is the Bernoulli distribution.\nAs link function connecting \\(\\eta_i\\) to \\(p_i\\) we use the logit function.\n\n\n\nLogit function: \\(\\text{logit}: [0,1] \\to \\mathbb{R}\\)\n\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\n\\(\\frac{p}{1-p}\\) is called the odds of a success which happens with probability \\(p\\).\nExample: Roll a six with a die has \\(p=1/6\\). Thus, the odds are \\(\\frac{1/6}{5/6} = 1/5\\). Sometimes written as 1:5. “The odds of success are one to five.”"
  },
  {
    "objectID": "W08.html#properties-of-the-logit",
    "href": "W08.html#properties-of-the-logit",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Properties of the logit",
    "text": "Properties of the logit\n\nLogit takes values between 0 and 1 and returns values between \\(-\\infty\\) and \\(\\infty\\)\nThe inverse of the logit function if the logistic function (mapping values from \\(-\\infty\\) and \\(\\infty\\) to values between 0 and 1): \\[\\text{logit}^{-1}(x) = \\text{logistic}(x) = \\frac{e^x}{1+e^x} = \\frac{1}{1+e^{-x}}\\]\nLogit can be interpreted as the log odds of a success – more on this later.\n\n\n\n\nThe logistic function is the solution of the differential equation \\(\\frac{d}{dx}f(x) = f(x)(1-f(x))\\) which also appears in the SI-model of epidemics (and other models of exponential growth with saturation).\nGood exercises to check your math skills:\n\nShow that \\(\\text{logit}^{-1}(x) = \\text{logistic}(x)\\) or the other way round.\nTransform \\(\\frac{e^x}{1+e^x}\\) into \\(\\frac{1}{1+e^{-x}}\\).\nCheck that \\(\\text{logistic}(x)\\) is a solution to \\(\\frac{d}{dx}f(x) = f(x)(1-f(x))\\).\n\nYou can request hints from me when you get stuck."
  },
  {
    "objectID": "W08.html#logit-and-logistic-function",
    "href": "W08.html#logit-and-logistic-function",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Logit and logistic function",
    "text": "Logit and logistic function\n\nggplot() + \n geom_function(fun = function(x) log(x/(1-x)), xlim=c(0.001,0.999), n = 500) + \n geom_function(fun = function(x) 1/(1 + exp(-x)), color = \"red\") +\n scale_x_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n scale_y_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n coord_fixed() + theme_minimal(base_size = 24) + labs(x = \"x\")"
  },
  {
    "objectID": "W08.html#the-logistic-regression-model",
    "href": "W08.html#the-logistic-regression-model",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nBased on the three GLM criteria we have\n\n\\(y_i \\sim \\text{Bernoulli}(p_i)\\)\n\\(\\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\n\\(\\text{logit}(p_i) = \\eta_i\\)\n\n\n\n\nFrom which we get\n\n\\[p_i = \\frac{e^{\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}}}{1 + e^{\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}}}\\]"
  },
  {
    "objectID": "W08.html#modeling-spam",
    "href": "W08.html#modeling-spam",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Modeling spam",
    "text": "Modeling spam\nWith tidymodels we fit a GLM in the same way as a linear model except we\n\nspecify the model with logistic_reg()\nuse \"glm\" instead of \"lm\" as the engine\ndefine family = \"binomial\" for the link function to be used in the model\n\n\nspam_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ num_char, data = email, family = \"binomial\")\ntidy(spam_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n\n\n\n\n\nThe family is binomial because the Bernoulli distribution is a special case of the binomial distribution \\(\\text{Binomial}(n,p)\\) with \\(n=1\\).\nThe binomial distribution specifies the probability to have \\(k\\) successes in \\(n\\) Bernoulli trials with the same success probability \\(p\\)."
  },
  {
    "objectID": "W08.html#spam-model",
    "href": "W08.html#spam-model",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Spam model",
    "text": "Spam model\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n\n\nModel: \\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot \\text{num_char}\\]"
  },
  {
    "objectID": "W08.html#predicted-probability-examples",
    "href": "W08.html#predicted-probability-examples",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Predicted probability: Examples",
    "text": "Predicted probability: Examples\nWe can compute the predicted probability that an email with 2000 character is spam as follows:\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot 2 = -1.9242\\]\n(Note: num_char is in thousands.)\n\\[\\frac{p}{1-p} = e^{-1.9242} = 0.15 \\Rightarrow p = 0.15 \\cdot (1 - p)\\]\n\\[p = 0.15 - 0.15\\cdot p \\Rightarrow 1.15\\cdot p = 0.15\\]\n\\[p = 0.15 / 1.15 = 0.13\\]"
  },
  {
    "objectID": "W08.html#predicted-probability",
    "href": "W08.html#predicted-probability",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Predicted probability",
    "text": "Predicted probability\n\nlogistic <- function(t) 1/(1+exp(-t))\npreds <- tibble(x=c(2,15,40), y = logistic(-1.80-0.0621*x))\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + \n geom_point(alpha = 0.2) + \n geom_function(fun = function(x) logistic(-1.80-0.0621*x),color=\"red\") +\n geom_point(data = preds, mapping = aes(x,y), color = \"blue\", size = 3)\n\n\nSpam probability 2,000 characters: 0.1273939\nSpam probability 15,000 characters: 0.06114\nSpam probability 40,000 characters: 0.0135999"
  },
  {
    "objectID": "W08.html#interpretation-of-coefficients",
    "href": "W08.html#interpretation-of-coefficients",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot \\text{num_char}\\]\nWhat does an increase by thousand characters (num_char + 1) imply?\n\nLet us assume the predicted probability of an email is \\(p_0\\). Then an increase of num_char by one implied that the log-odds become\n\\[\\log\\left(\\frac{p_0}{1-p_0}\\right) - 0.0621 = \\log\\left(\\frac{p_0}{1-p_0}\\right) - \\log(e^{0.0621})\\]\n\\[ = \\log\\left(\\frac{p_0}{1-p_0}\\right) + \\log(\\frac{1}{e^{0.0621}}) = \\log\\left(\\frac{p_0}{1-p_0} \\frac{1}{e^{0.0621}}\\right) = \\log\\left(\\frac{p_0}{1-p_0} 0.94\\right)\\]\nThat means the odds of being spam decrease by 6%."
  },
  {
    "objectID": "W08.html#penguins-example",
    "href": "W08.html#penguins-example",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Penguins example",
    "text": "Penguins example\n\nlibrary(palmerpenguins)\nsex_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(sex ~ body_mass_g, data = na.omit(penguins), family = \"binomial\")\ntidy(sex_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) -5.16     0.724        -7.13 1.03e-12\n2 body_mass_g  0.00124  0.000173      7.18 7.10e-13\n\n\n\nna.omit(penguins) |> ggplot(aes(x = body_mass_g, y = sex)) + \n geom_point(alpha = 0.2) + \n geom_function(fun = function(x) logistic(-5.16+0.00124*x) + 1,color=\"red\") +\n xlim(c(2000,7000))"
  },
  {
    "objectID": "W08.html#summarizing-the-idea-of-logistic-regression",
    "href": "W08.html#summarizing-the-idea-of-logistic-regression",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Summarizing the Idea of Logistic Regression",
    "text": "Summarizing the Idea of Logistic Regression\n\nWe want to explain/predict a binary response variable.\nWe want to use the concept of linear models.\nTo that end we first interprete each outcome as the outcome of a probabilisitic event happening with probability \\(p_i\\). That way transform the problem to from modeling a binary outcome to a numerical outcome.\nHowever, probabilities are bounded between 0 and 1.\nWe use the logit function to map probabilities to the real line. That means we predict the log-odds of the probability.\nWe model the log-odds with a linear model."
  },
  {
    "objectID": "W08.html#false-positive-and-negative",
    "href": "W08.html#false-positive-and-negative",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail labelled spam\nEmail labelled not spam\n\n\n\n\nEmail is spam\nTrue positive\nFalse negative (Type 2 error)\n\n\nEmail is not spam\nFalse positive (Type 1 error)\nTrue negative"
  },
  {
    "objectID": "W08.html#confusion-matrix",
    "href": "W08.html#confusion-matrix",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nMore general: Confusion matrix of statistical classification:"
  },
  {
    "objectID": "W08.html#sensitivity-and-specificity-1",
    "href": "W08.html#sensitivity-and-specificity-1",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\nSensitivity is the true positive rate: TP / (TP + FN)\nSpecificity is the true negative rate: TN / (TN + FP)\n\n\nFor spam:\nSensitivity: Fraction of emails labelled as spam among all emails which are spam.\nLow sensitivity \\(\\to\\) More false negatives \\(\\to\\) More spam in you inbox!\nSpecificity: Fraction of emails labelled as not spam among all emails which are not spam.\nLow specificity \\(\\to\\) More false positives \\(\\to\\) More relevant emails in spam folder!\n\nIf you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "W08.html#covid-19-tests",
    "href": "W08.html#covid-19-tests",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "COVID-19 tests",
    "text": "COVID-19 tests\nWhat is the sensitivity of a test?\n\nProbability to have COVID-19 when the test is positive.\n\n\nWhat is the specificity of a test?\n\n\nProbability to not have COVID-19 when the test is negative.\nOften the sensitivity is around 90% and the specificity is around 99%. What does that mean?\n\n\n\nWhen you test negative you can be more sure that you don’t have it, than you can be sure that you have it when your test is positive.\nHowever, in a larger population of testing individuals with high prevalence also 99% specificity implies a large fraction of false negatives!"
  },
  {
    "objectID": "W08.html#another-view",
    "href": "W08.html#another-view",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Another view",
    "text": "Another view"
  },
  {
    "objectID": "W08.html#prediction-vs.-explanation",
    "href": "W08.html#prediction-vs.-explanation",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Prediction vs. Explanation",
    "text": "Prediction vs. Explanation\nExplanation and Prediction are different Modelling Mindsets1.\n\nWith an Explanatory Mindset we want to understand the relationship between the predictors and the outcome.\n\nWe interpret coefficients\nWe ask what variance can be explained by a linear model or a PCA\n\nWith a Predictive Mindset we want to predict the outcome for new observations.\n\nWe use the model to predict the outcome for new observations\nWe ask how well we can predict the outcome for new observations\nWe may ignore why the model works\n\n\nImportant: The mindsets are very different but they may use the same models!\nFrom Molnar, C. (2022). Modeling mindsets: The many cultures of learning from data. Christoph Molnar c/o Mucbook Clubhouse."
  },
  {
    "objectID": "W08.html#prediction-example-building-a-spam-filter",
    "href": "W08.html#prediction-example-building-a-spam-filter",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Prediction Example: Building a spam filter",
    "text": "Prediction Example: Building a spam filter\n\nData: Set of emails and we know\n\nif each email is spam or not\nseveral other features\n\nUse logistic regression to predict the probability that an incoming email is spam\nUse model selection to pick the model with the best predictive performance\n\n\n\nBuilding a model to predict the probability that an email is spam is only half of the battle! We also need a decision rule about which emails get flagged as spam (e.g. what probability should we use as out cutoff?)\n\n\n\n\nA simple approach: choose a single threshold probability and any email that exceeds that probability is flagged as spam"
  },
  {
    "objectID": "W08.html#emails-use-all-predictors",
    "href": "W08.html#emails-use-all-predictors",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Emails: Use all predictors",
    "text": "Emails: Use all predictors\n\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ ., data = email, family = \"binomial\") |>\n  tidy() |> print(n = 22)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n# A tibble: 22 × 5\n   term         estimate std.error statistic  p.value\n   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)  -9.09e+1   9.80e+3  -0.00928 9.93e- 1\n 2 to_multiple1 -2.68e+0   3.27e-1  -8.21    2.25e-16\n 3 from1        -2.19e+1   9.80e+3  -0.00224 9.98e- 1\n 4 cc            1.88e-2   2.20e-2   0.855   3.93e- 1\n 5 sent_email1  -2.07e+1   3.87e+2  -0.0536  9.57e- 1\n 6 time          8.48e-8   2.85e-8   2.98    2.92e- 3\n 7 image        -1.78e+0   5.95e-1  -3.00    2.73e- 3\n 8 attach        7.35e-1   1.44e-1   5.09    3.61e- 7\n 9 dollar       -6.85e-2   2.64e-2  -2.59    9.64e- 3\n10 winneryes     2.07e+0   3.65e-1   5.67    1.41e- 8\n11 inherit       3.15e-1   1.56e-1   2.02    4.32e- 2\n12 viagra        2.84e+0   2.22e+3   0.00128 9.99e- 1\n13 password     -8.54e-1   2.97e-1  -2.88    4.03e- 3\n14 num_char      5.06e-2   2.38e-2   2.13    3.35e- 2\n15 line_breaks  -5.49e-3   1.35e-3  -4.06    4.91e- 5\n16 format1      -6.14e-1   1.49e-1  -4.14    3.53e- 5\n17 re_subj1     -1.64e+0   3.86e-1  -4.25    2.16e- 5\n18 exclaim_subj  1.42e-1   2.43e-1   0.585   5.58e- 1\n19 urgent_subj1  3.88e+0   1.32e+0   2.95    3.18e- 3\n20 exclaim_mess  1.08e-2   1.81e-3   5.98    2.23e- 9\n21 numbersmall  -1.19e+0   1.54e-1  -7.74    9.62e-15\n22 numberbig    -2.95e-1   2.20e-1  -1.34    1.79e- 1\n\n\n\n\nWe treat the warning later."
  },
  {
    "objectID": "W08.html#the-prediction-task",
    "href": "W08.html#the-prediction-task",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "The prediction task",
    "text": "The prediction task\n\nThe mechanics of prediction is easy:\n\nPlug in values of predictors to the model equation\nCalculate the predicted value of the response variable, \\(\\hat{y}\\)\n\n\n\n\nGetting it right is harder\n\nThere is no guarantee the model estimates you have are correct\nOr that your model will perform as well with new data as it did with your sample data"
  },
  {
    "objectID": "W08.html#balance-over--and-underfitting",
    "href": "W08.html#balance-over--and-underfitting",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Balance Over- and Underfitting",
    "text": "Balance Over- and Underfitting\nIn a one predictor model we can show both visually:\n\n\n\n\n\n\n\nThis is simulated data."
  },
  {
    "objectID": "W08.html#spending-our-data",
    "href": "W08.html#spending-our-data",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Spending our data",
    "text": "Spending our data\nProblem:\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available can lead to overfitting.\n\nSolution: We subsets our data for different tasks, as opposed to allocating all data to parameter estimation (as we have done so far)."
  },
  {
    "objectID": "W08.html#splitting-data-1",
    "href": "W08.html#splitting-data-1",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Splitting data",
    "text": "Splitting data\n\nTraining set:\n\nSandbox for model building\nSpend most of your time using the training set to develop the model\nMajority of the data (usually 80%)\n\nTesting set:\n\nHeld in reserve to determine efficacy of one or two chosen models\nCritical to look at it once, otherwise it becomes part of the modeling process\nRemainder of the data (usually 20%)"
  },
  {
    "objectID": "W08.html#performing-the-split",
    "href": "W08.html#performing-the-split",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Performing the split",
    "text": "Performing the split\n\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1116)\n\n# Put 80% of the data into the training set \nemail_split <- initial_split(email, prop = 0.80)\n\n# Create data frames for the two sets:\ntrain_data <- training(email_split)\ntest_data  <- testing(email_split)\n\nA note on the seed: The seed is an arbitrary number that is used to initialize a pseudorandom number generator. The same seed will always result in the same sequence of pseudorandom numbers. This is useful for reproducibility."
  },
  {
    "objectID": "W08.html#peek-at-the-split",
    "href": "W08.html#peek-at-the-split",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Peek at the split",
    "text": "Peek at the split\n\n\n\nglimpse(train_data)\n\nRows: 3,136\nColumns: 21\n$ spam         <fct> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0,…\n$ sent_email   <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ time         <dttm> 2012-01-25 23:46:55, 2012-01-03 06:28:28, 2012-02-04 17:…\n$ image        <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 10, 0, 0, 0, 0, 0, 13, 0, 0, 0, 2, 0, 0, 0, 14, 0, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, yes, no, no, no, no, no, no, …\n$ inherit      <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, …\n$ num_char     <dbl> 23.308, 1.162, 4.732, 42.238, 1.228, 25.599, 16.764, 10.7…\n$ line_breaks  <int> 477, 2, 127, 712, 30, 674, 367, 226, 98, 671, 46, 192, 67…\n$ format       <fct> 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, …\n$ re_subj      <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 12, 0, 2, 2, 2, 31, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 11, 1, …\n$ number       <fct> small, none, big, big, small, small, small, small, small,…\n\n\n\n\nglimpse(test_data)\n\nRows: 785\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, …\n$ sent_email   <fct> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ time         <dttm> 2012-01-01 18:55:06, 2012-01-01 20:38:32, 2012-01-02 06:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, …\n$ dollar       <dbl> 0, 0, 5, 0, 0, 0, 0, 5, 4, 0, 0, 0, 21, 0, 0, 2, 9, 0, 0,…\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, …\n$ num_char     <dbl> 4.837, 15.075, 18.037, 45.842, 11.438, 1.482, 14.431, 0.9…\n$ line_breaks  <int> 193, 354, 345, 881, 125, 24, 296, 13, 192, 14, 32, 30, 55…\n$ format       <fct> 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, …\n$ re_subj      <fct> 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, …\n$ exclaim_subj <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 1, 10, 20, 5, 2, 0, 0, 0, 6, 0, 0, 1, 3, 0, 4, 0, 1, 0, 1…\n$ number       <fct> big, small, small, big, small, none, small, small, small,…"
  },
  {
    "objectID": "W08.html#fit-a-model-to-the-training-dataset",
    "href": "W08.html#fit-a-model-to-the-training-dataset",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Fit a model to the training dataset",
    "text": "Fit a model to the training dataset\n\nemail_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ ., data = train_data, family = \"binomial\")\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWe get a warning and should explore the reasons for 0 or 1 probability.\n\n\n\nA deeper looking into the predicted probabilities (not shown here) shows that 4 cases are predicted to be spam with 100% probability, as well as 864 cases are predicted to be not spam with 100% probability.\nNote: The dplyr function near was used to assess if predicted probabilities were one.\nThis is usually undesirable. Hence the warning."
  },
  {
    "objectID": "W08.html#look-at-categorical-predictors",
    "href": "W08.html#look-at-categorical-predictors",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Look at categorical predictors",
    "text": "Look at categorical predictors\n\nCloser look at from and sent_email."
  },
  {
    "objectID": "W08.html#counting-cases",
    "href": "W08.html#counting-cases",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Counting cases",
    "text": "Counting cases\n\n\nfrom: Whether the message was listed as from anyone (this is usually set by default for regular outgoing email).\n\ntrain_data |> count(spam, from)\n\n# A tibble: 3 × 3\n  spam  from      n\n  <fct> <fct> <int>\n1 0     1      2837\n2 1     0         3\n3 1     1       296\n\n\nNo non-spam mails without from.\n\nsent_mail: Indicator for whether the sender had been sent an email from the receiver in the last 30 days.\n\ntrain_data |> count(spam, sent_email)\n\n# A tibble: 3 × 3\n  spam  sent_email     n\n  <fct> <fct>      <int>\n1 0     0           1972\n2 0     1            865\n3 1     0            299\n\n\nNo spam mails with sent_email.\n\n\n\nThere is incomplete separation in the data for those variables.\nThat mean we have a sure prediction probabilities (0 or 1). (That is the warning. Also, these variables have the highest coefficients.)\nThis is not what we assume about reality. Maybe our sample is too small to see it.\nTherefore we exclude these variables."
  },
  {
    "objectID": "W08.html#look-at-numerical-variables",
    "href": "W08.html#look-at-numerical-variables",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Look at numerical variables",
    "text": "Look at numerical variables\n\n\ntrain_data |>\n group_by(spam) |>\n select(where(is.numeric)) |> \n pivot_longer(-spam) |> \n group_by(name, spam) |> \n summarize(mean = mean(value), sd = sd(value)) |> \n print(n = 22)\n\n\n# A tibble: 22 × 4\n# Groups:   name [11]\n   name         spam       mean       sd\n   <chr>        <fct>     <dbl>    <dbl>\n 1 attach       0       0.124     0.775 \n 2 attach       1       0.227     0.620 \n 3 cc           0       0.393     2.62  \n 4 cc           1       0.388     3.25  \n 5 dollar       0       1.56      5.33  \n 6 dollar       1       0.779     3.01  \n 7 exclaim_mess 0       6.68     50.2   \n 8 exclaim_mess 1       8.75     88.4   \n 9 exclaim_subj 0       0.0783    0.269 \n10 exclaim_subj 1       0.0769    0.267 \n11 image        0       0.0536    0.503 \n12 image        1       0.00334   0.0578\n13 inherit      0       0.0352    0.216 \n14 inherit      1       0.0702    0.554 \n15 line_breaks  0     247.      326.    \n16 line_breaks  1     108.      321.    \n17 num_char     0      11.4      14.9   \n18 num_char     1       5.63     15.7   \n19 password     0       0.112     0.938 \n20 password     1       0.0201    0.182 \n21 viagra       0       0         0     \n22 viagra       1       0.0268    0.463 \n\n\n\nviagra has no mentions in non-spam emails.\n\nWe should exclude this variable for the same reason."
  },
  {
    "objectID": "W08.html#fit-a-model-to-the-training-dataset-1",
    "href": "W08.html#fit-a-model-to-the-training-dataset-1",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Fit a model to the training dataset",
    "text": "Fit a model to the training dataset\n\nemail_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ . - from - sent_email - viagra, data = train_data, family = \"binomial\") \n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nemail_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = spam ~ . - from - sent_email - viagra, family = stats::binomial, \n    data = data)\n\nCoefficients:\n (Intercept)  to_multiple1            cc          time         image  \n  -9.867e+01    -2.505e+00     1.944e-02     7.396e-08    -2.854e+00  \n      attach        dollar     winneryes       inherit      password  \n   5.070e-01    -6.440e-02     2.170e+00     4.499e-01    -7.065e-01  \n    num_char   line_breaks       format1      re_subj1  exclaim_subj  \n   5.870e-02    -5.420e-03    -9.017e-01    -2.995e+00     1.002e-01  \nurgent_subj1  exclaim_mess   numbersmall     numberbig  \n   3.572e+00     1.009e-02    -8.518e-01    -1.329e-01  \n\nDegrees of Freedom: 3135 Total (i.e. Null);  3117 Residual\nNull Deviance:      1974 \nResidual Deviance: 1447     AIC: 1485\n\n\nWe still get a warning, but without very high coefficients.\n\n\nA deeper analysis shows that now only two cases are predicted not spam with 100% probability."
  },
  {
    "objectID": "W08.html#predict-with-the-testing-dataset",
    "href": "W08.html#predict-with-the-testing-dataset",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Predict with the testing dataset",
    "text": "Predict with the testing dataset\nPredicting the raw values (log-odds)\n\npredict(email_fit, test_data, type = \"raw\") |> head() # head prints the first values\n\n        1         2         3         4         5         6 \n-4.942500 -6.312226 -3.938487 -6.688992 -4.399541 -1.587700 \n\n\n\n\nPredicting probabilities\n\npredict(email_fit, test_data, type = \"prob\") |> head()\n\n# A tibble: 6 × 2\n  .pred_0 .pred_1\n    <dbl>   <dbl>\n1   0.993 0.00709\n2   0.998 0.00181\n3   0.981 0.0191 \n4   0.999 0.00124\n5   0.988 0.0121 \n6   0.830 0.170  \n\n\n\nPredicting spam (default)\n\npredict(email_fit, test_data) # Would be type = \"class\"\n\n# A tibble: 785 × 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ℹ 775 more rows"
  },
  {
    "objectID": "W08.html#relate-back-to-the-model-concept",
    "href": "W08.html#relate-back-to-the-model-concept",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Relate back to the model concept",
    "text": "Relate back to the model concept\n\n\nemail_pred <- \n predict(email_fit, test_data, type = \"prob\") |>\n select(spam_prob = .pred_1) |> \n mutate(spam_logodds = \n         predict(email_fit, test_data, type = \"raw\"), \n        spam_odds = exp(spam_logodds)) |> \n bind_cols(predict(email_fit, test_data)) |> \n # Append real data\n bind_cols(test_data |> select(spam)) \nemail_pred\n\n\n# A tibble: 785 × 5\n   spam_prob spam_logodds spam_odds .pred_class spam \n       <dbl>        <dbl>     <dbl> <fct>       <fct>\n 1   0.00709        -4.94   0.00714 0           0    \n 2   0.00181        -6.31   0.00181 0           0    \n 3   0.0191         -3.94   0.0195  0           0    \n 4   0.00124        -6.69   0.00124 0           0    \n 5   0.0121         -4.40   0.0123  0           0    \n 6   0.170          -1.59   0.204   0           0    \n 7   0.0410         -3.15   0.0427  0           0    \n 8   0.139          -1.83   0.161   0           0    \n 9   0.0617         -2.72   0.0657  0           0    \n10   0.0983         -2.22   0.109   0           0    \n# ℹ 775 more rows\n\n\n\n\nThe raw predictions are the log-odds.\nFrom which we can compute the odds.\nFrom which the probability is computed. Here it is done by predict.\nThe .pred_class prediction is when the probability > 0.5.\n\nWhat does it mean for the odds and the log-odds?\n\n\n\nAnswers: odds > 1, log-odds > 0"
  },
  {
    "objectID": "W08.html#another-look",
    "href": "W08.html#another-look",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Another look",
    "text": "Another look\n\nemail_pred |> arrange(desc(spam_prob)) |> print(n = 20)\n\n# A tibble: 785 × 5\n   spam_prob spam_logodds spam_odds .pred_class spam \n       <dbl>        <dbl>     <dbl> <fct>       <fct>\n 1     0.903       2.23       9.29  1           1    \n 2     0.833       1.60       4.98  1           0    \n 3     0.825       1.55       4.71  1           1    \n 4     0.733       1.01       2.75  1           1    \n 5     0.683       0.766      2.15  1           1    \n 6     0.626       0.517      1.68  1           1    \n 7     0.614       0.464      1.59  1           0    \n 8     0.597       0.392      1.48  1           1    \n 9     0.538       0.153      1.17  1           1    \n10     0.537       0.148      1.16  1           0    \n11     0.510       0.0404     1.04  1           0    \n12     0.491      -0.0345     0.966 0           0    \n13     0.490      -0.0407     0.960 0           0    \n14     0.489      -0.0453     0.956 0           1    \n15     0.483      -0.0698     0.933 0           1    \n16     0.473      -0.107      0.899 0           0    \n17     0.463      -0.150      0.861 0           0    \n18     0.457      -0.174      0.840 0           0    \n19     0.447      -0.212      0.809 0           0    \n20     0.447      -0.214      0.808 0           1    \n# ℹ 765 more rows\n\n\nWe see false positives and false negatives."
  },
  {
    "objectID": "W08.html#evaluate-the-performance",
    "href": "W08.html#evaluate-the-performance",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nReceiver operating characteristic (ROC) curve1 which plots true positive rate (sensitivity) vs. false positive rate (1 - specificity)\n\nemail_pred |> roc_curve(\n    truth = spam, spam_prob,\n    event_level = \"second\" # this adjusts the location above the diagonal\n  ) |> autoplot()\n\n\n\n\nOriginally developed for operators of military radar receivers, hence the odd name."
  },
  {
    "objectID": "W08.html#evaluate-the-performance-1",
    "href": "W08.html#evaluate-the-performance-1",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nFind the area under the curve.\nIn calculus language:\n\\(\\int_0^1 \\text{TPR}(\\text{FPR}) d\\text{FPR}\\)\nRemember:\n\nTPR = True Positive Rate = sensitivity\nFPR = False Positive Rate = 1 - specificity\nspecificity is the true negative rate.\n\n\nemail_pred |>\n  roc_auc(\n    truth = spam,\n    spam_prob,\n    event_level = \"second\" \n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.857"
  },
  {
    "objectID": "W08.html#feature-engineering-1",
    "href": "W08.html#feature-engineering-1",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are critical to the success of the model\nFeature engineering is getting creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "W08.html#modeling-workflow-revisited",
    "href": "W08.html#modeling-workflow-revisited",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Modeling workflow, revisited",
    "text": "Modeling workflow, revisited\n\nCreate a recipe for feature engineering steps to be applied to the training data\n\nThe tidymodels way (similar to a pipeline in python’s scikit-learn).\n\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "W08.html#initiate-a-recipe",
    "href": "W08.html#initiate-a-recipe",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nemail_rec <- recipe(\n spam ~ .,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n)\nsummary(email_rec) |> print(n = 21)\n\n# A tibble: 21 × 4\n   variable     type      role      source  \n   <chr>        <list>    <chr>     <chr>   \n 1 to_multiple  <chr [3]> predictor original\n 2 from         <chr [3]> predictor original\n 3 cc           <chr [2]> predictor original\n 4 sent_email   <chr [3]> predictor original\n 5 time         <chr [1]> predictor original\n 6 image        <chr [2]> predictor original\n 7 attach       <chr [2]> predictor original\n 8 dollar       <chr [2]> predictor original\n 9 winner       <chr [3]> predictor original\n10 inherit      <chr [2]> predictor original\n11 viagra       <chr [2]> predictor original\n12 password     <chr [2]> predictor original\n13 num_char     <chr [2]> predictor original\n14 line_breaks  <chr [2]> predictor original\n15 format       <chr [3]> predictor original\n16 re_subj      <chr [3]> predictor original\n17 exclaim_subj <chr [2]> predictor original\n18 urgent_subj  <chr [3]> predictor original\n19 exclaim_mess <chr [2]> predictor original\n20 number       <chr [3]> predictor original\n21 spam         <chr [3]> outcome   original\n\n\nThe object email_rec only includes meta-data (columns names and types)!"
  },
  {
    "objectID": "W08.html#remove-certain-variables",
    "href": "W08.html#remove-certain-variables",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Remove certain variables",
    "text": "Remove certain variables\n\nemail_rec |>\n  step_rm(from, sent_email, viagra)"
  },
  {
    "objectID": "W08.html#feature-engineer-date",
    "href": "W08.html#feature-engineer-date",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Feature engineer date",
    "text": "Feature engineer date\n\nThe date-time may not be such an interesting predictor.\n\nIt could only bring in a general trend over time\n\nOften decomposing the date to the month or the day of the week (dow) is more interesting.\n\nstep_date can easily extract these\n\n\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time)"
  },
  {
    "objectID": "W08.html#create-dummy-variables",
    "href": "W08.html#create-dummy-variables",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Create dummy variables",
    "text": "Create dummy variables\n\nUse helper functions like all_nominal or all_outcomes from tidymodels for column selection.\n\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes())"
  },
  {
    "objectID": "W08.html#remove-zero-variance-variables",
    "href": "W08.html#remove-zero-variance-variables",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Remove zero variance variables",
    "text": "Remove zero variance variables\nVariables that contain only a single value.\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())"
  },
  {
    "objectID": "W08.html#full-recipe",
    "href": "W08.html#full-recipe",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Full recipe",
    "text": "Full recipe\n\nemail_rec <- recipe(\n spam ~ .,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n) |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\nprint(email_rec)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 20\n\n\n\n\n\n── Operations \n\n\n• Variables removed: from, sent_email, viagra\n\n\n• Date features from: time\n\n\n• Variables removed: time\n\n\n• Dummy variables from: all_nominal(), -all_outcomes()\n\n\n• Zero variance filter on: all_predictors()\n\n\nThe object email_rec only includes meta-data of the data frame it shall work on (a formula, columns names and types)!"
  },
  {
    "objectID": "W08.html#define-model",
    "href": "W08.html#define-model",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Define model",
    "text": "Define model\n\nemail_mod <- logistic_reg() |> \n  set_engine(\"glm\")\n\nemail_mod\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "W08.html#define-workflow",
    "href": "W08.html#define-workflow",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Define workflow",
    "text": "Define workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nemail_wflow <- workflow() |> \n  add_model(email_mod) |> \n  add_recipe(email_rec)\nemail_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_date()\n• step_rm()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "W08.html#fit-model-to-training-data",
    "href": "W08.html#fit-model-to-training-data",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nemail_fit <- email_wflow |> \n  fit(data = train_data)\n\ntidy(email_fit) |> print(n = 27)\n\n# A tibble: 27 × 5\n   term            estimate std.error statistic  p.value\n   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)     -0.651     0.254     -2.57   1.03e- 2\n 2 cc               0.0214    0.0229     0.936  3.49e- 1\n 3 image           -2.99      1.31      -2.28   2.28e- 2\n 4 attach           0.512     0.116      4.41   1.03e- 5\n 5 dollar          -0.0651    0.0307    -2.12   3.40e- 2\n 6 inherit          0.440     0.205      2.15   3.14e- 2\n 7 password        -0.723     0.302     -2.39   1.67e- 2\n 8 num_char         0.0585    0.0240     2.43   1.50e- 2\n 9 line_breaks     -0.00548   0.00139   -3.94   8.24e- 5\n10 exclaim_subj     0.0998    0.268      0.373  7.09e- 1\n11 exclaim_mess     0.0103    0.00198    5.20   2.02e- 7\n12 to_multiple_X1  -2.56      0.339     -7.56   4.11e-14\n13 winner_yes       2.24      0.430      5.21   1.90e- 7\n14 format_X1       -0.953     0.157     -6.06   1.38e- 9\n15 re_subj_X1      -3.00      0.444     -6.76   1.39e-11\n16 urgent_subj_X1   3.69      1.15       3.20   1.37e- 3\n17 number_small    -0.840     0.162     -5.20   1.98e- 7\n18 number_big      -0.0915    0.244     -0.375  7.07e- 1\n19 time_dow_Mon    -0.326     0.303     -1.08   2.82e- 1\n20 time_dow_Tue     0.0813    0.275      0.296  7.67e- 1\n21 time_dow_Wed    -0.260     0.275     -0.946  3.44e- 1\n22 time_dow_Thu    -0.220     0.279     -0.788  4.31e- 1\n23 time_dow_Fri    -0.0612    0.275     -0.223  8.24e- 1\n24 time_dow_Sat     0.0646    0.292      0.221  8.25e- 1\n25 time_month_Feb   0.760     0.178      4.26   2.03e- 5\n26 time_month_Mar   0.506     0.178      2.85   4.40e- 3\n27 time_month_Apr -12.0     394.        -0.0306 9.76e- 1"
  },
  {
    "objectID": "W08.html#make-predictions-for-test-data",
    "href": "W08.html#make-predictions-for-test-data",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nemail_pred <- predict(email_fit, test_data, type = \"prob\") |> \n  bind_cols(test_data) \n\nemail_pred\n\n# A tibble: 785 × 23\n   .pred_0  .pred_1 spam  to_multiple from     cc sent_email time               \n     <dbl>    <dbl> <fct> <fct>       <fct> <int> <fct>      <dttm>             \n 1   0.993 0.00653  0     1           1         0 1          2012-01-01 18:55:06\n 2   0.998 0.00169  0     0           1         1 1          2012-01-01 20:38:32\n 3   0.987 0.0127   0     0           1         0 0          2012-01-02 06:42:16\n 4   0.999 0.000825 0     0           1         1 0          2012-01-02 16:12:51\n 5   0.991 0.00876  0     0           1         4 0          2012-01-02 17:45:36\n 6   0.878 0.122    0     0           1         0 0          2012-01-02 22:55:03\n 7   0.959 0.0414   0     0           1         0 0          2012-01-03 02:07:17\n 8   0.852 0.148    0     0           1         0 0          2012-01-03 06:41:35\n 9   0.938 0.0619   0     0           1         0 0          2012-01-03 17:02:35\n10   0.896 0.104    0     0           1         0 0          2012-01-03 12:14:51\n# ℹ 775 more rows\n# ℹ 15 more variables: image <dbl>, attach <dbl>, dollar <dbl>, winner <fct>,\n#   inherit <dbl>, viagra <dbl>, password <dbl>, num_char <dbl>,\n#   line_breaks <int>, format <fct>, re_subj <fct>, exclaim_subj <dbl>,\n#   urgent_subj <fct>, exclaim_mess <dbl>, number <fct>"
  },
  {
    "objectID": "W08.html#evaluate-the-performance-2",
    "href": "W08.html#evaluate-the-performance-2",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nemail_pred |>\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) |>\n  autoplot()"
  },
  {
    "objectID": "W08.html#evaluate-the-performance-3",
    "href": "W08.html#evaluate-the-performance-3",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nemail_pred |>\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.860\n\n\nThis is better than our former model (without the feature engineering workflow), which had AUC = 0.857."
  },
  {
    "objectID": "W08.html#cutoff-probability-0.5",
    "href": "W08.html#cutoff-probability-0.5",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Cutoff probability: 0.5",
    "text": "Cutoff probability: 0.5\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5. (That is the default.)\nConfusion matrix:\n\ncutoff_prob <- 0.5\nemail_pred |>\n  mutate(\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(.pred_1 > cutoff_prob, \"Email labelled spam\", \"Email labelled not spam\")\n    ) |>\n  count(spam_pred, spam) |>\n  pivot_wider(names_from = spam_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n\n\n\n\nspam\nEmail labelled spam\nEmail labelled not spam\n\n\n\n\nEmail is spam\n14\n54\n\n\nEmail is not spam\n10\n707\n\n\n\n\n\nSensitivity: 14/(14+54) = 0.206\nSpecificity: 707/(707+10) = 0.986"
  },
  {
    "objectID": "W08.html#cutoff-probability-0.25",
    "href": "W08.html#cutoff-probability-0.25",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Cutoff probability: 0.25",
    "text": "Cutoff probability: 0.25\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25.\nConfusion matrix:\n\n\n\n\n\nspam\nEmail labelled spam\nEmail labelled not spam\n\n\n\n\nEmail is spam\n32\n36\n\n\nEmail is not spam\n61\n656\n\n\n\n\n\nSensitivity: 32/(32+36) = 0.471\nSpecificity: 656/(656 + 61) = 0.915"
  },
  {
    "objectID": "W08.html#cutoff-probability-0.75",
    "href": "W08.html#cutoff-probability-0.75",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Cutoff probability: 0.75",
    "text": "Cutoff probability: 0.75\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75.\nConfusion matrix:\n\n\n\n\n\nspam\nEmail labelled spam\nEmail labelled not spam\n\n\n\n\nEmail is spam\n3\n65\n\n\nEmail is not spam\n1\n716\n\n\n\n\n\nSensitivity: 3/(3+65) = 0.044\nSpecificity: 716/(716+1) = 0.999"
  },
  {
    "objectID": "W08.html#check-our-very-first-model",
    "href": "W08.html#check-our-very-first-model",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Check our very first model",
    "text": "Check our very first model\nWe make a new simple recipe and draw workflow and fitting re-using the same specified logisitc regression model email_mod.\n\nsimple_email_rec <- recipe(\n spam ~ num_char,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n)\nsimple_email_pred <- \n workflow() |> \n add_model(email_mod) |> \n add_recipe(simple_email_rec) |> \n fit(data = train_data) |> \n predict(test_data, type = \"prob\") |> \n bind_cols(test_data |> select(spam,num_char,time)) \nsimple_email_pred \n\n# A tibble: 785 × 5\n   .pred_0 .pred_1 spam  num_char time               \n     <dbl>   <dbl> <fct>    <dbl> <dttm>             \n 1   0.889  0.111  0        4.84  2012-01-01 18:55:06\n 2   0.936  0.0644 0       15.1   2012-01-01 20:38:32\n 3   0.945  0.0547 0       18.0   2012-01-02 06:42:16\n 4   0.989  0.0113 0       45.8   2012-01-02 16:12:51\n 5   0.922  0.0784 0       11.4   2012-01-02 17:45:36\n 6   0.868  0.132  0        1.48  2012-01-02 22:55:03\n 7   0.933  0.0667 0       14.4   2012-01-03 02:07:17\n 8   0.864  0.136  0        0.978 2012-01-03 06:41:35\n 9   0.905  0.0953 0        7.79  2012-01-03 17:02:35\n10   0.864  0.136  0        0.978 2012-01-03 12:14:51\n# ℹ 775 more rows"
  },
  {
    "objectID": "W08.html#evaluate-the-performance-4",
    "href": "W08.html#evaluate-the-performance-4",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nsimple_email_pred |> roc_curve(\n    truth = spam, \n    .pred_1,\n    event_level = \"second\" # this adjusts the location above the diagonal\n  ) |> autoplot()\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.753\n\n\nConclusion: It is not as good compared to AUC 0.86"
  },
  {
    "objectID": "W08.html#predict-penguins-sex",
    "href": "W08.html#predict-penguins-sex",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Predict Penguins’ Sex",
    "text": "Predict Penguins’ Sex\nFollowing all the steps introduced before:\n\nset.seed(1)\n# Put 80% of the data into the training set \npeng_split <- initial_split(penguins, prop = 0.80)\n# Create data frames for the two sets:\ntrain_data_peng <- training(peng_split)\ntest_data_peng  <- testing(peng_split)\n\n# Create a recipe for the model\npeng_rec <- recipe(\n sex ~ .,\n data = train_data_peng \n) |> \n step_naomit(all_predictors()) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\n\n# Create a logistic regression model \npeng_mod <- logistic_reg() |> \n set_engine(\"glm\")\n\n# Create a workflow\npeng_wflow <- workflow() |> \n add_model(peng_mod) |> \n add_recipe(peng_rec)\n\n# Fitting model to training data\npeng_fit <- peng_wflow |> \n fit(data = train_data_peng)\n\n# Make predictions for test data\npeng_pred <- peng_fit |> \n predict(test_data_peng, type = \"prob\") |> \n bind_cols(test_data_peng)"
  },
  {
    "objectID": "W08.html#penguins-evaluate-performance",
    "href": "W08.html#penguins-evaluate-performance",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Penguins: Evaluate Performance",
    "text": "Penguins: Evaluate Performance\n\npeng_pred |> roc_curve(\n    truth = sex,  .pred_male, event_level = \"second\"\n  ) |>  autoplot()\n\n\n\npeng_pred |> roc_auc(\n truth = sex, .pred_male, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.958"
  },
  {
    "objectID": "W08.html#penguins-cutoff-probability-0.5",
    "href": "W08.html#penguins-cutoff-probability-0.5",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Penguins: Cutoff probability 0.5",
    "text": "Penguins: Cutoff probability 0.5\nConfusion matrix:\n\ncutoff_prob <- 0.5\npeng_pred |>\n  mutate(\n    sex      = if_else(sex == \"male\", \"Penguin is male\", \"Penguin is female\"),\n    sex_pred = if_else(.pred_male > cutoff_prob, \"Penguin labelled male\", \"Penguin labelled female\")\n    ) |>\n  count(sex_pred, sex) |>\n  pivot_wider(names_from = sex_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n\n\n\n\nsex\nPenguin labelled male\nPenguin labelled female\n\n\n\n\nPenguin is male\n31\n4\n\n\nPenguin is female\n4\n28\n\n\n\n\n\nSensitivity: 31/(31+4) = 0.886\nSpecificity: 28/(28+4) = 0.875"
  },
  {
    "objectID": "W08.html#penguins-cutoff-probability-0.25",
    "href": "W08.html#penguins-cutoff-probability-0.25",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Penguins: Cutoff probability 0.25",
    "text": "Penguins: Cutoff probability 0.25\nConfusion matrix:\n\ncutoff_prob <- 0.25\npeng_pred |>\n  mutate(\n    sex      = if_else(sex == \"male\", \"Penguin is male\", \"Penguin is female\"),\n    sex_pred = if_else(.pred_male > cutoff_prob, \"Penguin labelled male\", \"Penguin labelled female\")\n    ) |>\n  count(sex_pred, sex) |>\n  pivot_wider(names_from = sex_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n\n\n\n\nsex\nPenguin labelled male\nPenguin labelled female\n\n\n\n\nPenguin is male\n34\n1\n\n\nPenguin is female\n7\n25\n\n\n\n\n\nSensitivity: 34/(34+1) = 0.971\nSpecificity: 25/(25+7) = 0.781"
  },
  {
    "objectID": "W08.html#penguins-cutoff-probability-0.75",
    "href": "W08.html#penguins-cutoff-probability-0.75",
    "title": "W#08: Logistic Regression, Classification Problems, Prediction",
    "section": "Penguins: Cutoff probability 0.75",
    "text": "Penguins: Cutoff probability 0.75\nConfusion matrix:\n\ncutoff_prob <- 0.75\npeng_pred |>\n  mutate(\n    sex      = if_else(sex == \"male\", \"Penguin is male\", \"Penguin is female\"),\n    sex_pred = if_else(.pred_male > cutoff_prob, \"Penguin labelled male\", \"Penguin labelled female\")\n    ) |>\n  count(sex_pred, sex) |>\n  pivot_wider(names_from = sex_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n\n\n\n\nsex\nPenguin labelled male\nPenguin labelled female\n\n\n\n\nPenguin is male\n29\n6\n\n\nPenguin is female\n2\n30\n\n\n\n\n\nSensitivity: 29/(29+6) = 0.829\nSpecificity: 30/(30+2) = 0.938\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "hw-02.html",
    "href": "hw-02.html",
    "title": "Homework 02",
    "section": "",
    "text": "The goal of this assignment is to introduce you to Data Visualization and Data Wrangling in R and python.\nTo that end, we provide the repository hw2-USERNAME two starter quarto documents\n\none for R\none for python\n\nFirst, solve the exercises in RStudio with R.\nOnce you had the introduction to python you do the same in python.\nThe learning goal is to see the differences but, more important, the similarity of how the concepts are implemented.\nFind the instructions in the README in your repository!"
  },
  {
    "objectID": "W12.html#probability-topics-for-data-science",
    "href": "W12.html#probability-topics-for-data-science",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability Topics for Data Science",
    "text": "Probability Topics for Data Science\nToday concepts and topics\n\nThe concept of probability form the mathematical perspective.\nWhat are probabilistic events, probability functions and random variables.\nHow do random variables relate to data?\n(Binomial distribution)"
  },
  {
    "objectID": "W12.html#sample-space-atomic-events-events",
    "href": "W12.html#sample-space-atomic-events-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Sample space, atomic events, events",
    "text": "Sample space, atomic events, events\nIn the following, we say \\(S\\) is the sample space which is a set of atomic events.\nExample for sample spaces:\n\nFor a coin toss the atomic events are \\(H\\) (for HEADS) and \\(T\\) for TAILS, and the sample space is \\(S = \\{H,T\\}\\).\n\nFor the selection of one person of a group of \\(N\\) individuals labeled \\(1,\\dots,N\\), the sample space is \\(S = \\{1,\\dots,N\\}\\).\nFor two successive coin tosses the atomic events are \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\). The sample space is \\(\\{HH,HT, TH, TT\\}\\). Important: The atomic events for two coin tosses tare not \\(H\\) and \\(T\\).\n\nAn event \\(A\\) is a subset of the sample space \\(A \\subset S\\).\nImportant: Note the difference of atomic events and events."
  },
  {
    "objectID": "W12.html#example-events-for-one-coin-toss",
    "href": "W12.html#example-events-for-one-coin-toss",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example events for one coin toss",
    "text": "Example events for one coin toss\n\nThe set with one atomic event is a subset \\(\\{H\\} \\subset \\{H,T\\}\\).\nAlso the sample space \\(S = \\{H,T\\} \\subset \\{H,T\\}\\) is an event. It is called the sure event.\nAlso the empty set \\(\\{\\} = \\emptyset \\subset \\{H,T\\}\\) is an event. It is called the impossible event.\nIn interpretation, the event \\(\\{H,T\\}\\) means: The coin comes up HEAD or TAIL.\nThe empty set is interpreted as the event that it comes up neither HEADS nor TAILS."
  },
  {
    "objectID": "W12.html#more-example-events",
    "href": "W12.html#more-example-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "More example events",
    "text": "More example events\n\n2 coin tosses: The event \\(\\{HH, TH\\}\\) means “The first toss comes up HEAD or TAIL and the second is HEADS.”\n\nThe event \\(\\{HT, TH, HH\\}\\) means that “We have HEAD once or twice and it does not matter what coins.”\nThe event \\(\\{TT, HH\\}\\) means “Both coins show the same side.”\n\nQuiz questions for three coin tosses:\n\nWhat is the event “The coins show one HEAD”? \\(\\{HTT, THT, TTH\\}\\)\nWhat is the event “The first and the third coin are not HEAD? \\(\\{THT, TTT\\}\\)\nHow many atomic events exist? \\(2^3=8\\)\n\n\nFor selecting one random person:\nThe event \\(\\{2,5,6\\}\\) means that the selected person is either 2, 5, or 6. (Not all three people which is a different random variable!)"
  },
  {
    "objectID": "W12.html#the-set-of-all-events",
    "href": "W12.html#the-set-of-all-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The set of all events",
    "text": "The set of all events\nThe collection of all events is called a sigma-algebra. (This is a mathematical term which linguistic meaning we do not analyze deeper here.)\nDefinition: A sigma-algebra \\(\\mathcal{F}(S)\\) is a collection of subsets of a sample space \\(S\\) when it has the following properties\n\nThe empty set (the impossible event) is part of it \\(\\emptyset \\in \\mathcal{F}(S)\\)\nWhen \\(A \\in \\mathcal{F}(S)\\) then its complement \\(A^c \\in \\mathcal{F}(S)\\). That means: For any event \\(A\\) also its opposite \\(A^c = S \\setminus A\\) (read \\(S\\) minus the elements of \\(A\\)) is an event.\n\\(\\mathcal{F}(S)\\) is closed under the countable set union of its members. That means if \\(A_1,A_2,A_3, \\dots \\in \\mathcal{F}(S)\\) the \\(\\bigcup_{i}^\\infty A_i = A_1 \\cup A_2 \\cup A_3 \\cup \\dots \\in \\mathcal{F}(S)\\).\n\nThe mathematical technicality is not central here. Important is: The sigma-algebra is the set of all possible events and this is usually larger / more complex then one may naively think."
  },
  {
    "objectID": "W12.html#the-power-set",
    "href": "W12.html#the-power-set",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The power set",
    "text": "The power set\n\n\n\nA sigma-algebra \\(\\mathcal{F}(S)\\) is a subset of the set of all subsets (also called power set) of the sample space sometimes denoted \\(\\mathcal{P}(S)\\) or \\(2^S\\).\nThe notation \\(2^S\\) matches the fact that the power set of a set with \\(n\\) elements has \\(2^n\\) elements.\n\n\n\nExample powerset of a three element set."
  },
  {
    "objectID": "W12.html#example-for-the-set-of-all-events",
    "href": "W12.html#example-for-the-set-of-all-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example for the set of all events",
    "text": "Example for the set of all events\n\nFor 3 coin tosses: How many events exist? \\(2^3=8\\) atomic events \\(\\to\\) \\(2^8=256\\) event\nHow is it for four coin tosses? \\(2^{(2^4)} = 65536\\)\nWe select two out of five people at random (without replacement). How many atomic events? How many events?\n\n\nAtomic events: 12, 13, 14, 15, 23, 24, 25, 34, 35, 45 (here the order does not matter)\nThe number can be computed by “n choose k” \\({n \\choose k} =\\frac{n!}{(n-k)!k!}\\). Here: \\({5\\choose 2}\\).\n\nchoose(5,2)\n\n[1] 10\n\n\nThus there are \\(2^{10} = 1024\\) events.\nExample event: “Person 1 is among the selected.” = \\(\\{12, 13, 14, 15\\}\\)\nThese are typical problems of combinatorics, the theory of counting, which is basic for many probability models. We do not go deeper into it here."
  },
  {
    "objectID": "W12.html#probability-function",
    "href": "W12.html#probability-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability function",
    "text": "Probability function\nDefinition: For a collection of events (in a sigma-algebra \\(\\mathcal{F}(S)\\)) a function \\(\\text{Pr}: \\mathcal{F}(S) \\to \\mathbb{R}\\) is a probability function when\n\nThe probability of any event is between 0 and 1: \\(0\\leq \\text{Pr}(A) \\leq 1\\). (So, actually a probability function is a function \\(\\text{Pr}: \\mathcal{F}(S) \\to [0,1]\\).)\nThe probability of the event coinciding with the whole sample space (the sure event) is 1: \\(\\text{Pr}(S) = 1\\).\nFor events \\(A_1, A_2, \\dots, A_n \\in \\mathcal{F}(S)\\) which are pairwise disjoint we can sum up their probabilities:\n\\[\\text{Pr}(A_1 \\cup A_2\\cup\\dots\\cup A_n) = \\text{Pr}(A_1) + \\text{Pr}(A_2) + \\dots + \\text{Pr}(A_n) \\]\n\nThis captures the essence of how we think about probabilities mathematically. Most important: We can only easily add probabilities when they do not share atomic events."
  },
  {
    "objectID": "W12.html#some-basic-probability-rules",
    "href": "W12.html#some-basic-probability-rules",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Some basic probability rules",
    "text": "Some basic probability rules\n\nWe can compute the probabilities of all events by summing the probabilities of the atomic events in it. So, the probabilities of the atomic events are building blocks for the whole probability function.\n\\(\\text{Pr}(\\emptyset) = 0\\)\nFor any events \\(A,B \\subset S\\) it holds\n\n\\(\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cap B)\\)\n\\(\\text{Pr}(A \\cap B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cup B)\\)\n\\(\\text{Pr}(A^c) = 1 - \\text{Pr}(A)\\)\n\n\nRecap from the motivation of logistic regression: When the probability of an event is \\(A\\) is \\(\\text{Pr}(A)=p\\), then its odds (in favor of the event) are \\(\\frac{p}{1-p}\\). The logistic regression model “raw” predictions are log-odds \\(\\log\\frac{p}{1-p}\\)."
  },
  {
    "objectID": "W12.html#random-variable",
    "href": "W12.html#random-variable",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Random variable",
    "text": "Random variable\n\nA random variable is a numerical function where values come with probabilities.\nIn some statistical model, we consider variables in a data frame as random variables, for example the response variable in a generalized linear model.\n\nFormally, a random variable is\n\na function \\(X: S \\to \\mathbb{R}\\)\nwhich assigns a value to each atomic event in the sample space.\n\nTogether with a probability function \\(\\text{Pr}: \\mathcal{F}(S)\\to [0,1]\\) probabilities can be assigned to values of the random variable (see the probability mass function in two slides)."
  },
  {
    "objectID": "W12.html#examples-of-random-variables",
    "href": "W12.html#examples-of-random-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Examples of random variables",
    "text": "Examples of random variables\n\n\nFor two coin tosses a random variable can be the number of HEADS. In this case, each atomic event is mapped to a number: Either 0, 1, or 2.\nFor 62 randomly selected organ donations a random variable can be the number of complications. Each atomic event is mapped to an integer from 0 to 62. (Note, an atomic event are 62 randomly selected organ donations. So, the set of events is \\(2^{62} \\approx 4.61\\cdot 10^{18}\\).)\nIn the palmer penguins dataset we can consider a variable, e.g. flipper length, to be a random variable. The atomic event would be the random selection of a penguin and the random variable is its flipper length. So we map each penguin to its flipper length.\n\n\n\nA random variable is a way to look at a numerical aspect of a sample space. It often simplfies because many atomic events may be mapped to the same number."
  },
  {
    "objectID": "W12.html#probability-mass-function-pmf",
    "href": "W12.html#probability-mass-function-pmf",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability mass function (pmf)",
    "text": "Probability mass function (pmf)\nFor\n\na random variable \\(X\\) and\na probability function \\(\\text{Pr}\\)\n\nthe probability mass function \\(f_X: \\mathbb{R} \\to [0,1]\\) is defined as\n\\[f_X(x) = \\text{Pr}(X=x),\\]\nwhere \\(\\text{Pr}(X=x)\\) is an abbreviation for \\(\\text{Pr}(\\{a\\in S\\text{ for which } X(a) = x\\})\\)."
  },
  {
    "objectID": "W12.html#example-pmf-for-2-coin-tosses",
    "href": "W12.html#example-pmf-for-2-coin-tosses",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example pmf for 2 coin tosses",
    "text": "Example pmf for 2 coin tosses\nTwo coin tosses \\(S = \\{HH, HT, TH, TT\\}\\)\n\nWe define \\(X\\) to be the number of heads:\n\\(X(HH) = 2\\), \\(X(TH) = 1\\), \\(X(HT) = 1\\), and \\(X(TT) = 0\\).\n\nWe assume the probability function \\(\\text{Pr}\\) assigns for each atomic event a probability of 0.25.\nThen the probability mass function is \\[\\begin{align} f_X(0) = & \\text{Pr}(X=0) = \\text{Pr}(\\{TT\\}) & = 0.25 \\\\\nf_X(1) = & \\text{Pr}(X=1) = \\text{Pr}(\\{HT,TH\\}) & = 0.25 + 0.25 = 0.5 \\\\\nf_X(2) = &\\text{Pr}(X=2) = \\text{Pr}(\\{HH\\}) & = 0.25\\end{align}\\]\nNote that \\(\\text{Pr}(\\{HT,TH\\}) = \\text{Pr}(\\{HT\\}) + \\text{Pr}(\\{HT\\})\\) by adding the probabilities of the atomic events.\nFor all \\(x\\) which are not 0, 1, or 2 it is obviously \\(f_X(x) = 0\\)."
  },
  {
    "objectID": "W12.html#example-roll-two-dice",
    "href": "W12.html#example-roll-two-dice",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example: Roll two dice 🎲 🎲",
    "text": "Example: Roll two dice 🎲 🎲\nRandom variable: The sum of both dice.\n\nEvents: All 36 combinations of rolls 1+1, 1+2, 1+3, 1+4, 1+5, 1+6, 2+1, 2+2, 2+3, 2+4, 2+5, 2+6, 3+1, 3+2, 3+3, 3+4, 3+5, 3+6, 4+1, 4+2, 4+3, 4+4, 4+5, 4+6, 5+1, 5+2, 5+3, 5+4, 5+5, 5+6, 6+1, 6+2, 6+3, 6+4, 6+5, 6+6\n\n\nPossible values of the random variable:  2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 (These are numbers.)\n\n\nProbability mass function: (Assuming each number has probability of \\(\\frac{1}{6}\\) for each die.)\n\n\n\\(\\text{Pr}(2) = \\text{Pr}(12) = \\frac{1}{36}\\)\n\\(\\text{Pr}(3) = \\text{Pr}(11) = \\frac{2}{36}\\)\n\\(\\text{Pr}(4) = \\text{Pr}(10) = \\frac{3}{36}\\)\n\\(\\text{Pr}(5) = \\text{Pr}(9) = \\frac{4}{36}\\)\n\\(\\text{Pr}(6) = \\text{Pr}(8) = \\frac{5}{36}\\)\n\\(\\text{Pr}(7) = \\frac{6}{36}\\)\n\n\ntibble(Value = 0:15, pmf=c(0,c(0:6,5:0)/36,0,0)) |> \n ggplot(aes(Value,pmf)) + geom_line() + geom_point() +\n theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W12.html#binomial-distribution-1",
    "href": "W12.html#binomial-distribution-1",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nThe number of HEADS in several coin tosses and the number of complications in randomly selected organ donations are examples of random variable which have a binomial distribution.\n\nDefinition: The binomial distribution with parameters \\(n\\) and \\(p\\) is the number of successes in a sequence of \\(n\\) independent Bernoulli trials which each delivers a success with probability \\(p\\) and a failure with probability \\((1-p)\\).\n\nThe default model for the number of successes drawn from a sample of size \\(n\\) drawn from a population of size \\(N\\) with replacement.\nWhen \\(N\\) is much larger than \\(n\\) it is also a good approximation for drawing without replacement."
  },
  {
    "objectID": "W12.html#binomial-probability-mass-function",
    "href": "W12.html#binomial-probability-mass-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Binomial probability mass function",
    "text": "Binomial probability mass function\n\\[f(k,n,p) = \\Pr(k;n,p) = \\Pr(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\\]\nwhere \\(k\\) is the number of successes, \\(n\\) is the number of Bernoulli trials, and \\(p\\) the success probability.\nProbability to have exactly 3 complications in 62 randomly selected organ donations with complication probability \\(p=0.1\\) is\n\n# x represents k, and size represents n\ndbinom(x = 3, size = 62, prob = 0.1)\n\n[1] 0.07551437\n\n\nThe probability to have 3 complications or less can be computed as\n\ndbinom(3, 62, 0.1) + dbinom(2, 62, 0.1) + dbinom(1, 62, 0.1) + dbinom(0, 62, 0.1)\n\n[1] 0.1209787\n\n\n\nThis was the p-value we computed with simulation for the hypothesis testing example."
  },
  {
    "objectID": "W12.html#distribution-functions-are-vectorized",
    "href": "W12.html#distribution-functions-are-vectorized",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Distribution functions are vectorized!",
    "text": "Distribution functions are vectorized!\nCompute the p-value:\n\ndbinom(0:3, 62, 0.1) |> sum()\n\n[1] 0.1209787\n\n\nPlotting the probability mass function\n\n\ntibble(x = 0:62) |> \n mutate(pr = dbinom(x, size = 62, prob = 0.1)) |> \n ggplot(aes(x, pr)) + \n geom_col() + \n theme_minimal(base_size = 24)"
  },
  {
    "objectID": "W12.html#other-plots-of-binomial-mass-function",
    "href": "W12.html#other-plots-of-binomial-mass-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Other plots of binomial mass function",
    "text": "Other plots of binomial mass function\nChanging the sample size:\n\n\ntibble(samplesize = 0:62) |> \n mutate(pr = dbinom(3, size = samplesize, prob = 0.1)) |> \n ggplot(aes(samplesize, pr)) + \n geom_col() + \n theme_minimal(base_size = 24)\n\n\n\n\n\n\nThe probability of 3 successes is most likely for sample sizes around 30. Sensible?\n\nChanging the success probability:\n\n\ntibble(probs = seq(0,0.3,0.01)) |> \n mutate(pr = dbinom(3, size = 62, prob = probs)) |> \n ggplot(aes(probs, pr)) + \n geom_col() + \n scale_x_continuous(breaks = seq(0,0.3,0.05)) + \n theme_minimal(base_size = 24)\n\n\n\n\n\n\nThe probability of 3 successes is most likely for success probabilities around 0.05."
  },
  {
    "objectID": "W12.html#expected-value-discrete-rv",
    "href": "W12.html#expected-value-discrete-rv",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Expected value discrete RV",
    "text": "Expected value discrete RV\nA discrete random variable takes only a finite (or at least discrete) set of values.\nFor \\(X: S \\to \\mathbb{R}\\), when \\(S\\) is finite, there is naturally only a set of values \\(x_1,\\dots,x_k\\in\\mathbb{R}\\) which \\(X\\) can be. We call their probabilities \\(p_1,\\dots,p_k\\) with \\(p_i = \\text{Pr}(X=x_i) = \\text{Pr}(\\{a \\in S \\text{ for which } X(a) = x_i \\})\\).\n(The probability of all other values in \\(\\mathbb{R}\\) is zero.).\nThe expected value of \\(X\\) is \\(E(X) = \\sum_{i=1}^k p_i x_i = p_1x_1 + \\dots + p_kx_k.\\)\nExamples: \\(X\\) is a die roll 🎲. \\(E(X) = 1\\cdot\\frac{1}{6} + 2\\cdot\\frac{1}{6} + 3\\cdot\\frac{1}{6} + 4\\cdot\\frac{1}{6} + 5\\cdot\\frac{1}{6} + 6\\cdot\\frac{1}{6} = \\frac{21}{6} = 3.5\\)\n\\(X\\) sum of two die rolls 🎲🎲.\n\\(E(X) = 2\\cdot\\frac{1}{36} + 3\\cdot\\frac{2}{36} + 4\\cdot\\frac{3}{36} + 5\\cdot\\frac{4}{36} + 6\\cdot\\frac{5}{36} + 7\\cdot\\frac{6}{36} + 8\\cdot\\frac{5}{36} +\\)\n\\(+ 9\\cdot\\frac{4}{36} + 10\\cdot\\frac{3}{36} + 11\\cdot\\frac{2}{36} + 12\\cdot\\frac{1}{36} = 7\\)"
  },
  {
    "objectID": "W12.html#expected-value-binomial-distribution",
    "href": "W12.html#expected-value-binomial-distribution",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Expected value binomial distribution",
    "text": "Expected value binomial distribution\nFor \\(X \\sim \\text{Binom}(n,p)\\) (read “\\(X\\) has a binomial distribution with samplesize \\(n\\) and success probability \\(p\\)”)\nThe expected value of \\(X\\) is by definition\n\\[E(X) = \\underbrace{\\sum_{k = 0}^n k}_{\\text{sum over successes}} \\cdot \\underbrace{\\binom{n}{k}p^k(1-p)^{n-k}}_{\\text{probability of successes}}\\]\nComputation shows that \\(E(X) = p\\cdot n\\).\nExample: For \\(n = 62\\) organ donations with complication probability \\(p=0.1\\), the expected number of complications is \\(E(X) = 6.2\\)."
  },
  {
    "objectID": "W12.html#general-systematic-of-functions-for-distributions-in-r",
    "href": "W12.html#general-systematic-of-functions-for-distributions-in-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "General systematic of functions for distributions in R",
    "text": "General systematic of functions for distributions in R\nIn R we usually have 4 function for each distribution: The d, p, q, and r version. For the binomial distribution:\n\ndbinom the density function (more on the name later)\npbinom distribution function\nqbinom the quantile function, and\nrbinom random number generator."
  },
  {
    "objectID": "W12.html#probability-mass-function-d",
    "href": "W12.html#probability-mass-function-d",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability mass function d",
    "text": "Probability mass function d\n\nThe mass function (or density function, more on this later) dbinom\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = dbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nGives the probability for the number \\(x\\): \\(\\text{Pr}(X = x)\\) or \\(f_X(x)\\)."
  },
  {
    "objectID": "W12.html#distribution-function-p",
    "href": "W12.html#distribution-function-p",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Distribution function p",
    "text": "Distribution function p\n\nThe distribution function, or cumulative probability function pbinom\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = pbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nGives the probability that the random variable is less or equal to \\(x\\):\n\\(\\text{Pr}(X \\leq x)\\)."
  },
  {
    "objectID": "W12.html#quantile-function-q",
    "href": "W12.html#quantile-function-q",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Quantile function q",
    "text": "Quantile function q\n\nThe quantile function, qbinom with argument \\(p\\) representing the fraction of lowest values of \\(X\\) among all values for which we want the \\(x\\) value for.\n\n\n\nprobs <- seq(0, 1, by = 0.01)\ntibble(p = probs) |> \n mutate(x = qbinom(p, size = 10, prob = 0.5)) |> \n ggplot(aes(p, x)) + geom_line() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nA point \\((p,x)\\) means: When we want a \\(p\\)-fraction of the probability mass, we need all events with values lower or equal to \\(x\\)."
  },
  {
    "objectID": "W12.html#calculus-relations",
    "href": "W12.html#calculus-relations",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Calculus relations",
    "text": "Calculus relations\n\nQuantile, distribution and mass function all carry the full information about the distribution of a random variable \\(X\\).\nThe mass function is the derivative of the distribution function.\n(The distribution function is the anti-derivative of the mass function.)\n\n\npbinom(0:5, size = 5, prob = 0.5) \n\n[1] 0.03125 0.18750 0.50000 0.81250 0.96875 1.00000\n\n# Next comes its derivative (have to append a 0 before first)\npbinom(0:5, size = 5, prob = 0.5) |> append(0, after = 0) |> diff()\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\ndbinom(0:5, size = 5, prob = 0.5)\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n# Next comes its anti-derivative\ndbinom(0:5, size = 5, prob = 0.5) |> cumsum()\n\n[1] 0.03125 0.18750 0.50000 0.81250 0.96875 1.00000"
  },
  {
    "objectID": "W12.html#more-calculus-relations",
    "href": "W12.html#more-calculus-relations",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "More calculus relations",
    "text": "More calculus relations\n\nThe quantile function is the inverse of the distribution function.\nWe plot the inverse function by interchanging the x and y aesthetic.\n\n\n\nprobs <- seq(0, 1, by = 0.01)\nx <- 0:10\nq <- tibble(p = probs) |> mutate(x = qbinom(p, size = 10, prob = 0.5)) \np <- tibble(x = x) |> mutate(p = pbinom(x, size = 10, prob = 0.5)) \nq_plot <- q |> ggplot(aes(p, x)) + geom_line()\nqinv_plot <- q |> ggplot(aes(x, p)) + geom_line()\np_plot <- p |> ggplot(aes(x, p)) + geom_col()\npinv_plot <- p |> ggplot(aes(p, x)) + geom_col(orientation = \"y\")\nlibrary(patchwork)\n(q_plot | p_plot) / (pinv_plot | qinv_plot)"
  },
  {
    "objectID": "W12.html#random-number-generator-r",
    "href": "W12.html#random-number-generator-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Random number generator r",
    "text": "Random number generator r\n\nRandom binomial numbers are drawn with rbinom\n\n\n# 10 random binomial numbers for 62 trials with success probability 0.1\nrbinom(10, size = 62, prob = 0.1)\n\n [1]  5  8  6  8  6  5  9  7 10  6\n\n\n\nWe can reproduce the null distribution from hypothesis testing with 62 organ donations and 10% complication probability this way.\n\nWe produce 100,000 random consultants\nThen we compute the fraction of which have 3 or less complications\n\n\n\nset.seed(2022)\ns <- rbinom(100000, size = 62, prob = 0.1)\nsum(s<=3)/100000\n\n[1] 0.12038\n\n# Two other samples\nsum(rbinom(100000, size = 62, prob = 0.1)<=3)/100000\n\n[1] 0.11918\n\nsum(rbinom(100000, size = 62, prob = 0.1)<=3)/100000\n\n[1] 0.12034"
  },
  {
    "objectID": "W12.html#empirical-distributions",
    "href": "W12.html#empirical-distributions",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Empirical distributions",
    "text": "Empirical distributions\n\n\n\n\n\\(X\\): select a random person from Europe (in 2018, willing to answer survey) and ask its attitude towards the European union from 0 to 10\n\n\n\n\nWhat is the distribution of the answer?\n\n\neu <- ess |> select(euftf) |> drop_na() |> \n count(euftf) |> mutate(prob = n/sum(n)) \neu\n\n# A tibble: 11 × 3\n   euftf     n   prob\n   <dbl> <int>  <dbl>\n 1     0  3361 0.0736\n 2     1  1787 0.0391\n 3     2  2830 0.0620\n 4     3  3586 0.0786\n 5     4  3739 0.0819\n 6     5 10286 0.225 \n 7     6  4589 0.101 \n 8     7  5165 0.113 \n 9     8  4692 0.103 \n10     9  1786 0.0391\n11    10  3826 0.0838\n\n\n\nMass function and distribution function\n\neu_mass <- eu |> ggplot(aes(euftf, prob)) + geom_col()\neu_distr <- eu |> mutate(cumprob = cumsum(prob)) |> \n ggplot(aes(euftf, cumprob)) + geom_col()\neu_mass | eu_distr"
  },
  {
    "objectID": "W12.html#what-could-be-next",
    "href": "W12.html#what-could-be-next",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "What could be next?",
    "text": "What could be next?\n\nContinuous distribution function\n\nTheoretical and empirical\n\nThe central limit theorem and why it is important empirically\nIndependence of probabilistic events\nConditional probability and the confusion matrix\nMarkov chains"
  },
  {
    "objectID": "W12.html#pca-description",
    "href": "W12.html#pca-description",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "PCA Description",
    "text": "PCA Description\nPrinciple component analysis\n\nis a dimensionality-reduction technique, that means it can be used to reduce the number of variables\ncomputes new variables which represent the data in a different way\ntransforms the data linearly to a new coordinate system where most of the variation in the data can be described with fewer variables than the original data\ncan be seen as unsupervised learning technique because there is no response variable. (Response variable are often produced/supervised by humans for training, e.g., a spam dummy.)\n\nToday: Quick walk through how to use and interpret it."
  },
  {
    "objectID": "W12.html#other-data-of-owid-corona-data",
    "href": "W12.html#other-data-of-owid-corona-data",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "“Other” data of OWiD Corona data",
    "text": "“Other” data of OWiD Corona data\n\n\n\nSelect the variables listed as Others in the OWiD corona data documentation\nRemove those which have many NAs\n\n\nowid <- read_csv(\"data/owid-covid-data.csv\")\nowid_inds <- owid |> \n # Filter for one day and remove rows where continent is NA\n # These are rows with data for continents or world regions\n filter(date == \"2022-10-01\", !is.na(continent)) |> \n # These are the \"Other\" variables\n select(iso_code, continent, location, \n        population:human_development_index) |>\n # We remove the ones with many NA's\n select(-handwashing_facilities, -male_smokers, \n        - female_smokers, -extreme_poverty) |> \n drop_na()\nowid_inds |> count(continent)\n\n# A tibble: 6 × 2\n  continent         n\n  <chr>         <int>\n1 Africa           39\n2 Asia             42\n3 Europe           39\n4 North America    20\n5 Oceania           6\n6 South America    12\n\n\n\nWe have 158 countries and 11 numeric variables.\n\nnames(owid_inds)\n\n [1] \"iso_code\"                   \"continent\"                 \n [3] \"location\"                   \"population\"                \n [5] \"population_density\"         \"median_age\"                \n [7] \"aged_65_older\"              \"aged_70_older\"             \n [9] \"gdp_per_capita\"             \"cardiovasc_death_rate\"     \n[11] \"diabetes_prevalence\"        \"hospital_beds_per_thousand\"\n[13] \"life_expectancy\"            \"human_development_index\""
  },
  {
    "objectID": "W12.html#two-variables",
    "href": "W12.html#two-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Two Variables",
    "text": "Two Variables\nExample for the new axes.\n\n\n\n\n\n\n\nThe two arrows show the two eigenvectors of the covariance matrix of the two variables scaled by the square root of the corresponding eigenvalues, and shifted so their origins are at the means of both variables."
  },
  {
    "objectID": "W12.html#computation-in-r",
    "href": "W12.html#computation-in-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Computation in R",
    "text": "Computation in R\nThe basic function is base-R’s prcomp (there is an older princomp which is not advisable to use).\nThese 3 commands all deliver identical results for prcomp\n\n# prcomp can take a formula with no response variable as 1st argument\nP <- prcomp(~ median_age + life_expectancy, data= owid_inds)\n# prcomp can take a data frame with all numerical vectors as 1st argument\nP <- owid_inds |> select(median_age, life_expectancy) |> prcomp()\n# This is an example using the formula placeholder \".\" for 'take all\n# and the pipe's placeholder \"_\" which one uses when the output should be \n# piped to an argument other than the 1st\nP <- owid_inds |> select(median_age, life_expectancy) |> prcomp(~ ., data= _)\n\n\n\nThe standard output\n\nP\n\nStandard deviations (1, .., p=2):\n[1] 10.69296  2.95809\n\nRotation (n x k) = (2 x 2):\n                      PC1        PC2\nmedian_age      0.8091382  0.5876184\nlife_expectancy 0.5876184 -0.8091382\n\n\n\nThe summary output\n\nsummary(P)\n\nImportance of components:\n                           PC1     PC2\nStandard deviation     10.6930 2.95809\nProportion of Variance  0.9289 0.07109\nCumulative Proportion   0.9289 1.00000"
  },
  {
    "objectID": "W12.html#the-prcomp-object",
    "href": "W12.html#the-prcomp-object",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The prcomp object",
    "text": "The prcomp object\nIncludes 4 different related entities.\n\n\nThe standard deviations related to each principal component.\n\nP$sdev\n\n[1] 10.69296  2.95809\n\n\nThe matrix of variable loadings. (It is also the matrix which rotates the original data vectors.)\n\nP$rotation\n\n                      PC1        PC2\nmedian_age      0.8091382  0.5876184\nlife_expectancy 0.5876184 -0.8091382\n\n\n\nThe means for each original variable.\n\nP$center\n\n     median_age life_expectancy \n       31.15823        73.57741 \n\n\nNote, there are also standard deviations of original variables in $scale when this is set to be used.\nThe centered (scaled, if set) and rotated data.\n\nP$x\n\n             PC1         PC2\n1   -15.30147798 -0.30158529\n2     8.46967981 -0.01934856\n3     0.27527455 -3.88170827\n4     2.78495581 -2.23213257\n5     2.41746174 -2.06645786\n6     4.56375002  1.44493027\n7    11.25046735 -4.01861358\n8    15.39339097  1.33826834\n9     0.66547151  1.19688861\n10    2.74344303  1.56895631\n11    3.18635423 -2.27431439\n12   -3.54022934 -1.35069463\n13   10.29045200  0.53669877\n14    8.10950037  4.39071615\n15   13.34251723 -0.26236181\n16   -4.37020966 -4.46229111\n17  -16.93776248  2.29190137\n18   -3.12614815 -0.04891248\n19   -5.87404744 -1.71082388\n20   11.42328837  3.57162574\n21   -6.67861938  0.07776886\n22    3.24786441 -0.48704930\n23    2.34605999 -1.11724672\n24   11.82248935  6.76586095\n25  -18.02037585  1.74049561\n26  -18.10128967  1.68173378\n27   -6.70529480 -0.22585660\n28  -18.39505599  4.29856418\n29   13.48895662 -1.14471974\n30   -4.76750699 -2.72397157\n31  -22.33121139  8.86767576\n32    7.31198595 -2.84986882\n33    8.06063011  1.73515374\n34    3.02452659 -2.39183806\n35  -14.14471452  1.16878824\n36    5.91429898 -3.98849568\n37   13.27749971  3.57109260\n38    9.31944327 -2.38070454\n39   13.23408329  2.43962673\n40   13.31812495  0.62211827\n41   -8.45956818  1.84938433\n42   -2.58376417 -2.49754876\n43   -0.45747329 -4.57451459\n44   -5.67290445 -2.15797206\n45   -3.03035412 -1.88260370\n46  -15.80534846  6.85901626\n47  -13.85952988 -1.09586840\n48   12.37252460  2.60490418\n49  -15.68152626  5.15690932\n50  -13.29042762 -1.02861793\n51   -5.67641179  3.46274744\n52   14.31618860  0.09869783\n53   14.10959178 -0.97825052\n54  -10.69666185  1.01571059\n55  -17.82510905  1.30143881\n56    6.21550849  4.27584778\n57   17.05009523  2.80094774\n58  -13.72522235  1.78240564\n59   16.53294823  1.30072809\n60   -2.11451419 -0.08048350\n61   -6.25743781 -5.43736541\n62  -16.87582996  2.54697854\n63   -6.08601239  0.11265381\n64  -11.17711330  3.71942404\n65   -4.06917156 -5.04699279\n66   11.84595123  4.52123414\n67   10.50055615 -4.00707238\n68   -4.69555434  1.43141323\n69   -2.59500849  0.41096866\n70    2.82790704 -1.78074007\n71  -10.77812658 -4.14764718\n72   11.22789301 -2.62610131\n73    5.06757765 -7.92793256\n74   19.38298293  1.80093025\n75    0.72013226 -0.58016293\n76   20.28385695  1.07098094\n77   -5.87954413 -5.44718170\n78   -0.43840629 -0.34630735\n79  -13.06983814 -0.99200808\n80   -9.49927316 -0.46289021\n81    3.18052090 -0.05396172\n82   -5.18108013 -1.13341907\n83   -8.79273555  0.60636402\n84   11.31620717  6.10157310\n85    3.09816863 -4.36520494\n86  -15.24495644  0.64165663\n87   -2.13848412 -0.72819134\n88   11.36862762  5.34867729\n89   12.00765033 -1.99802605\n90  -13.19370320 -1.50216242\n91  -16.04098955 -0.13418567\n92    0.49949992 -2.82903407\n93  -20.32523538  2.87209737\n94   14.35685668 -0.63801521\n95    5.88052317  2.52479529\n96   -0.63823938 -2.28346164\n97    4.22661012  5.14255609\n98   -4.24849920  1.49654154\n99    8.36665685  1.99447523\n100   0.56232000 -3.42607145\n101 -18.36842342  2.38992835\n102  -5.45400436  4.00738947\n103  -6.62666413 -1.35520031\n104  14.85726268  0.03436403\n105  10.57470625 -3.08810461\n106  -2.59145830 -2.99748955\n107 -19.54962201 -0.40822638\n108   7.73202903  2.86834452\n109  12.09579308 -2.11939679\n110   2.14576170 -3.73447438\n111  -9.90291188  0.60344735\n112   1.71857540 -4.84803258\n113  -3.37392111 -3.28148244\n114   0.19300798 -3.76842892\n115  -6.20040822 -1.60178884\n116  11.63842401  2.08413905\n117  17.14952515  1.98332087\n118   4.50938307 -4.94698996\n119  11.03457271  4.95777153\n120   6.24446705  5.76757879\n121   4.56869580  0.07669216\n122  -0.09619205  1.22461257\n123 -11.95340613 -4.74163201\n124   1.51252948 -0.82038499\n125   3.97524412  3.10618305\n126  14.99736068 -1.51997588\n127  10.45367524  2.69444253\n128  15.34502876  1.57504058\n129  -8.72053195 -5.61948425\n130  -8.67330823  5.37709112\n131  15.45980409 -0.45496597\n132  17.47043209  0.35018930\n133   4.37972753 -1.02453034\n134 -14.12936912 -0.04359145\n135  -2.37577176  0.61961970\n136  13.38272012 -1.67914822\n137  15.65777638 -1.23810515\n138  -7.81416124 -2.61307574\n139 -15.65950282 -1.34019889\n140   9.33445203  2.36362626\n141 -13.04278322 -4.43283183\n142 -16.88124094  3.23514328\n143  -8.73494696 -3.04695782\n144   4.03988214  3.01717785\n145   3.08240087 -1.62063733\n146   2.77409099 -3.06806436\n147 -17.93950488 -0.41300384\n148   7.39535433  7.24604370\n149   4.88055588 -1.88433903\n150  12.35121731 -0.59914731\n151   8.88283060 -0.07771314\n152   6.13991994 -0.89560137\n153  -3.48506054 -0.23541152\n154  -2.63795972 -0.04042384\n155   2.23758320 -0.62751947\n156 -13.16791532 -0.34642241\n157 -16.58206363 -0.06985186\n158 -16.45498504  2.98855475"
  },
  {
    "objectID": "W12.html#pca-as-exploratory-data-analysis",
    "href": "W12.html#pca-as-exploratory-data-analysis",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "PCA as Exploratory Data Analysis",
    "text": "PCA as Exploratory Data Analysis\nSuppose we do a PCA with all 158 countries (rows) and all 11 numeric variables.\n\nHow long will the vector of standard deviations be? 11\n\nWhat dimensions will the rotation matrix have? 11 x 11\n\nWhat dimensions will the rotated data frame have? 158 x 11\n\n\nWhen we do a PCA for exploration there are 3 things to look at:\n\nThe data in PC coordinates - the centered (scaled, if set) and rotated data.\n\nThe rotation matrix - the variable loadings.\nThe variance explained by each PC - based on the standard deviations."
  },
  {
    "objectID": "W12.html#all-variables",
    "href": "W12.html#all-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "All variables",
    "text": "All variables\nNow, with scale = TRUE (recommended). Data will be centered and scaled (a.k.a. standardized) first.\n\nowid_PCA <- owid_inds |> select(-iso_code, -continent, -location) |> \n prcomp(~ ., data = _, scale = TRUE)\nowid_PCA\n\nStandard deviations (1, .., p=11):\n [1] 2.34241265 1.22235717 1.03373351 0.99489682 0.92287148 0.66475775\n [7] 0.58432716 0.45329540 0.25966139 0.21927321 0.06713354\n\nRotation (n x k) = (11 x 11):\n                                    PC1         PC2         PC3          PC4\npopulation                  0.008865647 -0.03734342  0.47595823  0.870416239\npopulation_density         -0.074719077 -0.46044638 -0.10288680  0.046602918\nmedian_age                 -0.409689271  0.06447942  0.12973022 -0.004468196\naged_65_older              -0.390007415  0.24159016 -0.02468961  0.056783114\naged_70_older              -0.385548522  0.26731058 -0.03864289  0.046944080\ngdp_per_capita             -0.307869646 -0.34923225 -0.10326975 -0.049084375\ncardiovasc_death_rate       0.203402666  0.30746105  0.55758803 -0.314193965\ndiabetes_prevalence        -0.015935611 -0.52365806  0.58702717 -0.310745215\nhospital_beds_per_thousand -0.281461985  0.34868645  0.26585730 -0.180587716\nlife_expectancy            -0.385997705 -0.16628186  0.03523644 -0.001063976\nhuman_development_index    -0.401390496 -0.11256612  0.07650908 -0.066730340\n                                   PC5        PC6          PC7         PC8\npopulation                  0.01039838 -0.1088643 -0.009530076 -0.01188854\npopulation_density          0.85694114  0.1686582 -0.030330909  0.05784099\nmedian_age                  0.02964348  0.1083349  0.151566064 -0.04089755\naged_65_older               0.06392873  0.3010299  0.133281270 -0.30838406\naged_70_older               0.04970870  0.2883236  0.134769694 -0.32266678\ngdp_per_capita             -0.04210924 -0.7155301  0.255572328 -0.38998232\ncardiovasc_death_rate       0.30899782 -0.1596115  0.556827122  0.11586467\ndiabetes_prevalence        -0.24595405  0.3128238 -0.194418712 -0.28853159\nhospital_beds_per_thousand  0.24614720 -0.3347129 -0.712831766  0.04133735\nlife_expectancy            -0.15876730  0.1057214  0.114444482  0.64382282\nhuman_development_index    -0.12068739 -0.1252574  0.075440136  0.36145260\n                                   PC9        PC10         PC11\npopulation                 -0.01568443  0.04316042  0.008103028\npopulation_density          0.01930238  0.04356815  0.017850825\nmedian_age                  0.43211922 -0.76462456  0.050093900\naged_65_older              -0.12286262  0.19597621 -0.724308013\naged_70_older              -0.18333355  0.25044108  0.687015497\ngdp_per_capita             -0.19559611 -0.02457951 -0.012540620\ncardiovasc_death_rate      -0.07816501  0.06233211 -0.006416554\ndiabetes_prevalence        -0.04883139  0.05032474  0.006819981\nhospital_beds_per_thousand -0.11397971 -0.01992701 -0.007573789\nlife_expectancy            -0.58309459 -0.13429476 -0.009463395\nhuman_development_index     0.60349415  0.53386017  0.010100958"
  },
  {
    "objectID": "W12.html#data-in-pc-coordinates",
    "href": "W12.html#data-in-pc-coordinates",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Data in PC coordinates",
    "text": "Data in PC coordinates\n\n\n\nStart plotting PC1 against PC2. By default these are the most important ones. Drill deeper later.\nUse the function augment to append the original data. Here used to draw labels and color by continent.\nNote: augment also created the variables names like .fittedPC1\n\n\n\nplotdata <- owid_PCA |> parsnip::augment(owid_inds)\nplotdata |> ggplot(aes(.fittedPC1, .fittedPC2, color = continent)) +\n geom_point() + \n geom_text(data = plotdata |> \n            filter(.fittedPC2< -3 | .fittedPC1< -5 | .fittedPC1>4), \n           mapping = aes(.fittedPC1, .fittedPC2, label = iso_code),\n           color = \"black\") +\n coord_fixed() + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W12.html#variable-loadings",
    "href": "W12.html#variable-loadings",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Variable loadings",
    "text": "Variable loadings\n\nThe columns of the rotation matrix shows how the original variables load on the principle components.\nWe can try to interpret these loadings and give names to components.\ntidy extracts the rotation matrix in long format with a PC, a column (for the original variable name), and a value variable .\n\n\nowid_PCA |> parsnip::tidy(matrix = \"rotation\") |> \n filter(PC<=6) |> \n ggplot(aes(value, column, fill=value)) + geom_col() +\n facet_wrap(~PC, nrow = 1)"
  },
  {
    "objectID": "W12.html#variance-explained",
    "href": "W12.html#variance-explained",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Variance explained",
    "text": "Variance explained\n\nPrinciple components are by default sorted by importance.\nThe squares of the standard deviation for each component gives its variances and variances have to sum up to the number of variables (in the standardized case).\nThis way the tidy command creates a variable percent which gives the fraction of the total variance explained by each component.\n\n\nowid_PCA |> tidy(matrix = \"eigenvalues\") |> \n ggplot(aes(PC, percent)) + geom_col()"
  },
  {
    "objectID": "W12.html#interpretations-1",
    "href": "W12.html#interpretations-1",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (1)",
    "text": "Interpretations (1)\n\n\nThe first component explains almost 50% of the variance. So most emphasize should be on this.\nTo reach more than 75% of the total variance the first four components are needed.\nAfter the fifth component the added explained variance drops substantially. This is another typical reason to cut off the rest.\nTaking the five components explains 89.9% of the variance of the original 11 variables!"
  },
  {
    "objectID": "W12.html#interpretations-2",
    "href": "W12.html#interpretations-2",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (2)",
    "text": "Interpretations (2)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population.\nPC4 focuses mostly on population.\nPC5 mostly on population density."
  },
  {
    "objectID": "W12.html#interpretations-3",
    "href": "W12.html#interpretations-3",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (3)",
    "text": "Interpretations (3)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths."
  },
  {
    "objectID": "W12.html#interpretations-4",
    "href": "W12.html#interpretations-4",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (4)",
    "text": "Interpretations (4)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population."
  },
  {
    "objectID": "W12.html#interpretations-5",
    "href": "W12.html#interpretations-5",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (5)",
    "text": "Interpretations (5)\n\n\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population."
  },
  {
    "objectID": "W12.html#interpretations-6",
    "href": "W12.html#interpretations-6",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (6)",
    "text": "Interpretations (6)\n\n\nPC4 focuses mostly on population.\nPC5 mostly on population density."
  },
  {
    "objectID": "W12.html#apply-pca",
    "href": "W12.html#apply-pca",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Apply PCA",
    "text": "Apply PCA\n\nBesides standardization, PCA may benefit by preprocessing steps of data transformation with variables with skew distributions (log, square-root, or Box-Cox transformation). This may result in less outliers.\nPCA is a often a useful step of exploratory data analysis when you have a large number of numerical variables to show the empirical dimensionality of the data and its structure\nLimitation: PCA is only sensitive for linear relation ships (no U-shaped) or the like\nThe can be used as predictors in a model instead of the raw variables."
  },
  {
    "objectID": "W12.html#properties-and-relations-of-pca",
    "href": "W12.html#properties-and-relations-of-pca",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Properties and relations of PCA",
    "text": "Properties and relations of PCA\n\nThe principal components (the columns of the rotation matrix) are maximally uncorrelated (actually they are even orthogonal).\nThis also holds for the columns of the rotated data.\nThe total variances of all prinicipal components sum up to the number of variables (when variables are standardized)\nThe PCA is unique. All principle components together are a complete representation of the data. (Unlike other technique of dimensionality reduction which may rely on starting values, random factors, or tuning parameters)\nA technique similar in spirit is factor analysis (e.g. factanal). It is more theory based as it requires to specify to the theoriezed number of factors up front.\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Learning Resources",
    "section": "",
    "text": "All the materials here are freely accessible, many are community standards. The Schedule might refer to specific sections in these materials."
  },
  {
    "objectID": "literature.html#r",
    "href": "literature.html#r",
    "title": "Learning Resources",
    "section": "1 R",
    "text": "1 R\n\nR for Data Science (2nd Edition) https://r4ds.hadley.nz// by Hadley Wickham, Garrett Grolemand, Mine Çetinkaya-Rundel and the R data science community around is our core resource for self-learning data science with R using tidyverse tools.\nTidy Modeling with R https://www.tmwr.org/ by Max Kuhn and Julia Silge is the resource for learning to work with tidymodels building upon the tidyverse tools.\nPart of the course builds on or be inspired by material in Data Science in the Box https://datasciencebox.org/ by Mine Çetinkaya-Rundel and the data science education community around. You can also use the website for accompanying self-study on selected topics."
  },
  {
    "objectID": "literature.html#python",
    "href": "literature.html#python",
    "title": "Learning Resources",
    "section": "2 python",
    "text": "2 python\n\nPython Data Science Handbook https://jakevdp.github.io/PythonDataScienceHandbook/ by Jake VanderPlas is a core resource for data science with python. Also there is a good video playlist https://www.youtube.com/playlist?list=PLWKjhJtqVAbkmRvnFmOd4KhDdlK1oIq23 and a 4-hour “full-course” video https://youtu.be/rfscVS0vtbw."
  },
  {
    "objectID": "literature.html#data-visualization",
    "href": "literature.html#data-visualization",
    "title": "Learning Resources",
    "section": "3 Data Visualization",
    "text": "3 Data Visualization\nggplot2: Elegant Graphics for Data Analysis https://ggplot2-book.org is a resource on understanding the logic of ggplot better.\nA great source for Graphic Design with ggplot2: https://rstudio-conf-2022.github.io/ggplot2-graphic-design/. Look at the Introduction to see what cool things are possible. Work through Concepts of the ggplot2 Package Pt. 1 and Concepts of the ggplot2 Package Pt. 2 for a full introduction to all of ggplot2.\nWebsites on how to decide for what visualization to choose:\nhttps://www.data-to-viz.com/\nhttps://datavizcatalogue.com/search.html"
  },
  {
    "objectID": "literature.html#mathematics",
    "href": "literature.html#mathematics",
    "title": "Learning Resources",
    "section": "4 Mathematics",
    "text": "4 Mathematics\n\nHow to read math https://www.youtube.com/watch?v=Kp2bYWRQylk"
  },
  {
    "objectID": "literature.html#project-based-learning",
    "href": "literature.html#project-based-learning",
    "title": "Learning Resources",
    "section": "5 Project-based learning",
    "text": "5 Project-based learning\nhttps://github.com/practical-tutorials/project-based-learning/tree/master"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Need help for Homework?",
    "section": "",
    "text": "Do homework together with other students. Working together is encouraged!\nCommit and push your intermediate results often, instructors may check your code directly.\nRead error messages carefully! Often they are informative or even tell you what to do! (Hint: Uninformative error messages are also common. Maybe you get at least a hint where the problem lies. You will become more advanced on this, error messages which are not helpful now may become helpful in the future.)\nDo the homework before the next Data Science Tools Session and prepare questions to ask in class.\nIt is absolutely OK to decide to ask instructors for help outside of class. We try to care, if we are not too busy with other things. If you do, please do first: Document and describe where you get stuck as precise as possible. Try to find out why as much as possible.\n\nThe instructors prefer that you ask for help in the Discussions section of the Organization repo https://github.com/CU-F23-MDSSB-01-Concepts-Tools/Organization/discussions. Only registered students and instructors can see these discussions. Maybe, there is already an answer.\nYou can also write to instructors directly. Use a Teams chat! You can also write an email."
  },
  {
    "objectID": "W06.html#sir-model",
    "href": "W06.html#sir-model",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "SIR model",
    "text": "SIR model\n\nAssume a population of \\(N\\) individuals.\n\nIndividuals can have different states, e.g.: Susceptible, Infectious, Recovered, …\nThe population divides into compartments of these states which change over time, e.g.: \\(S(t), I(t), R(t)\\) number of susceptible, infectious, recovered individuals\n\nNow we define dynamics like\n\nwhere the numbers on the arrows represent transition probabilities."
  },
  {
    "objectID": "W06.html#agent-based-simulation",
    "href": "W06.html#agent-based-simulation",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Agent-based Simulation",
    "text": "Agent-based Simulation\nAgent-based model: Individual agents are simulated and interact with each other.\nExplore and analyze with computer simulations.\nA tool: NetLogo https://ccl.northwestern.edu/netlogo/\n\nWe look at the model “Virus on a Network” from the model library."
  },
  {
    "objectID": "W06.html#virus-on-a-network-6-links-initial",
    "href": "W06.html#virus-on-a-network-6-links-initial",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Virus on a Network: 6 links, initial",
    "text": "Virus on a Network: 6 links, initial\nAgents connected in a network with on average 6 links per agent. 3 are infected initially."
  },
  {
    "objectID": "W06.html#virus-on-a-network-6-links-final",
    "href": "W06.html#virus-on-a-network-6-links-final",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Virus on a Network: 6 links, final",
    "text": "Virus on a Network: 6 links, final\nThe outbreak dies out after some time."
  },
  {
    "objectID": "W06.html#virus-on-a-network-15-links-initial",
    "href": "W06.html#virus-on-a-network-15-links-initial",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Virus on a Network: 15 links, initial",
    "text": "Virus on a Network: 15 links, initial\nRepeat the simulation with 15 links per agent. 3 are infected initially."
  },
  {
    "objectID": "W06.html#virus-on-a-network-15-links-final",
    "href": "W06.html#virus-on-a-network-15-links-final",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Virus on a Network: 15 links, final",
    "text": "Virus on a Network: 15 links, final\nThe outbreak had infected most agents."
  },
  {
    "objectID": "W06.html#si-model",
    "href": "W06.html#si-model",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "SI model",
    "text": "SI model\nNow, we only treat the SI part of the model. We ignore recovery.\n\nPeople who are susceptible can become infected through contact with infectious\nPeople who are infectious stay infectious\n\nThe parameter \\(\\beta\\) is the average number of contacts per unit time multiplied with the probability that an infection happens during such a contact."
  },
  {
    "objectID": "W06.html#si-model-simulation-in-r",
    "href": "W06.html#si-model-simulation-in-r",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "SI-model: Simulation in R",
    "text": "SI-model: Simulation in R\n\nWe produce a vector of length \\(N\\) with entries representing the state of each individual as \"S\" or \"I\".\nWe model the random infection process in each step of unit time\n\nSetup\nParameters: \\(N=150, \\beta=0.3\\), a function to produce randomly infect individuals\n\n\nN <- 150\nbeta <- 0.3\nrandomly_infect <- function(N, prob) { \n runif(N) < prob \n # Gives a logical vector of length N\n # where TRUE appears with probability beta\n}\n# Test\nrandomly_infect(N, beta) |> head() # First 6\n\n\n[1]  TRUE FALSE  TRUE FALSE FALSE  TRUE\n\n\n\n\n\ninit <- rep(\"S\",N) # All susceptible\ninit[1] <- \"I\" # Infect one individual\ninit |> head() # First 6\n\n\n[1] \"I\" \"S\" \"S\" \"S\" \"S\" \"S\""
  },
  {
    "objectID": "W06.html#si-model-simulation-in-r-1",
    "href": "W06.html#si-model-simulation-in-r-1",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "SI-model: Simulation in R",
    "text": "SI-model: Simulation in R\nIteration over 75 time steps.\n\n\ntmax <- 75\nsim_run <- list(init) # list with one element\n# This list will collect the states of \n# all individuals over tmax time steps \nfor (i in 2:tmax) {\n # Every agents has a contact with a random other\n contacts <- sample(sim_run[[i-1]], size = N)\n sim_run[[i]] <- if_else( # vectorised ifelse\n  # conditions vector: contact is infected\n  # and a random infection happens\n  contacts == \"I\" & randomly_infect(N, beta), \n  true = \"I\", \n  false = sim_run[[i-1]]\n  ) # otherwise state stays the same\n}\nsim_output <- tibble( # create tibble for ggplot\n # Compute a vector with length tmax \n # with the count of \"I\" in sim_run list\n t = 0:(tmax-1), # times steps\n # count of infected, notice map_dbl\n infected = map_dbl(sim_run, \\(x) sum(x == \"I\"))) \nsim_output |> \n ggplot(aes(t,infected)) + geom_line() +\n theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W06.html#si-model-simulation-in-r-2",
    "href": "W06.html#si-model-simulation-in-r-2",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "SI-model: Simulation in R",
    "text": "SI-model: Simulation in R\nRun with \\(N = 10000\\)\n\n\nN <- 10000\ninit <- rep(\"S\",N) # All susceptible\ninit[1] <- \"I\" # Infect one individual\ntmax <- 75\nsim_run <- list(init) # list with one element\n# This list will collect the states of \n# all individuals over tmax time steps \nfor (i in 2:tmax) {\n # Every agents has a contact with a random other\n contacts <- sample(sim_run[[i-1]], size = N)\n sim_run[[i]] <- if_else( # vectorised ifelse\n  # conditions vector: contact is infected\n  # and a random infection happens\n  contacts == \"I\" & randomly_infect(N, beta), \n  true = \"I\", \n  false = sim_run[[i-1]]\n  ) # otherwise state stays the same\n}\nsim_output <- tibble( # create tibble for ggplot\n # Compute a vector with length tmax \n # with the count of \"I\" in sim_run list\n t = 0:(tmax-1), # times steps\n # count of infected, notice map_dbl\n infected = map_dbl(sim_run, \\(x) sum(x == \"I\"))) \nsim_output |> \n ggplot(aes(t,infected)) + geom_line() +\n theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W06.html#new-programming-concepts",
    "href": "W06.html#new-programming-concepts",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "New programming concepts",
    "text": "New programming concepts\nFrom base R:\nrunif random numbers from uniform distribution\nsample random sampling from a vector\nfor loop over commands with index (i) taking values of a vector (2:tmax) one by one if_else vectorized version of conditional statements"
  },
  {
    "objectID": "W06.html#motivation-si-model-with-population-compartments",
    "href": "W06.html#motivation-si-model-with-population-compartments",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Motivation: SI model with  population compartments",
    "text": "Motivation: SI model with  population compartments\nTwo compartments:\n\\(S(t)\\) is the number of susceptible people at time \\(t\\).\n\\(I(t)\\) is the number of infected people at time \\(t\\).\nIt always holds \\(S(t) + I(t) = N\\). (The total population is constant.)"
  },
  {
    "objectID": "W06.html#how-many-infections-per-time",
    "href": "W06.html#how-many-infections-per-time",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "How many infections per time?",
    "text": "How many infections per time?\nThe change of the number of infectious\n\\[\\frac{dI}{dt} = \\underbrace{\\beta}_\\text{infection prob.} \\cdot \\underbrace{\\frac{S}{N}}_\\text{frac. of $S$ still there} \\cdot \\underbrace{\\frac{I}{N}}_\\text{frac. $I$ to meet} \\cdot N = \\frac{\\beta\\cdot S\\cdot I}{N}\\]\nwhere \\(dI\\) is the change of \\(I\\) (the newly infected here) and \\(dt\\) the time interval.\n\nInterpretation: The newly infected are from the fraction of susceptible times the probability that they meet an infected times the infection probability times the total number of individuals.\nSame logic as our Simulation in R!\n\n\nUsing \\(S = N - I\\) we rewrite\n\\[\\frac{dI}{dt} = \\frac{\\beta (N-I)I}{N}\\]"
  },
  {
    "objectID": "W06.html#ordinary-differential-equation",
    "href": "W06.html#ordinary-differential-equation",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Ordinary differential equation",
    "text": "Ordinary differential equation\nWe interpret \\(I(t)\\) as a function of time which gives us the number of infectious at each point in time. The change function is now\n\\[\\frac{dI(t)}{dt} = \\frac{\\beta (N-I(t))I(t)}{N}\\]\nand \\(\\frac{dI(t)}{dt}\\) is also called the derivative of \\(I(t)\\)."
  },
  {
    "objectID": "W06.html#derivatives",
    "href": "W06.html#derivatives",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Derivatives",
    "text": "Derivatives\n\n\n\nThe derivative of a function is also a function with the same domain.\nMeasures the sensitivity to change of the function output when the input changes (a bit)\nExample from physics: The derivative of the position of a moving object is its speed. The derivative of its speed is its acceleration.\nGraphically: The derivative is the slope of a tangent line of the graph of a function."
  },
  {
    "objectID": "W06.html#differentiation",
    "href": "W06.html#differentiation",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Differentiation",
    "text": "Differentiation\nis the process to compute the derivative. For parameters \\(a\\) and \\(b\\) and other functions \\(g\\) and \\(h\\), rules of differentiation are\n\n\nFunction \\(f(x)\\)\n\nIts derivative \\(\\frac{df(x)}{dx}\\) or \\(\\frac{d}{dx}f(x)\\) or \\(f'(x)\\)\n\n\n\n\n\\(a\\cdot x\\)\n\\(b\\)\n\\(x^2,\\ x^{-1} = \\frac{1}{x},\\ x^k\\)\n\\(g(x) + h(x)\\)\n\\(g(x)\\cdot h(x)\\)\n\\(g(h(x))\\)\n\\(e^x,\\ 10^x = e^{\\log(10)x}\\)\n\\(\\log(x)\\)\n\n\n\\(a\\)\n\n\n\\(0\\)\n\n\n\\(2\\cdot x,\\ -x^{-2} = -\\frac{1}{x^2},\\ k\\cdot x^{k-1}\\)\n\n\n\\(g'(x) + h'(x)\\)\n\n\n\\(g'(x)\\cdot h(x) + g(x)\\cdot h'(x)\\) (product rule)\n\n\n\\(g'(h(x))\\cdot h'(x)\\) (chain rule)\n\n\n\\(e^x,\\ 10^x = \\log(10)\\cdot10^x\\)\n\n\n\\(\\frac{1}{x}\\) (A surprising relation to me at least)"
  },
  {
    "objectID": "W06.html#differential-equation",
    "href": "W06.html#differential-equation",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Differential equation",
    "text": "Differential equation\nIn a differential equation the unknown is a function!\nWe are looking for a function which derivative is a function of the function itself.\nExample: SI-model\n\\[\\frac{dI(t)}{dt} = \\frac{\\beta (N-I(t))I(t)}{N}\\]\nWhich function \\(I(t)\\) fulfills this equation?\nThe analytical solution1\n\\(I(t) = \\frac{N}{1 + (\\frac{N}{I(0)} - 1)e^{-\\beta t}}\\)\nWhich is called the logistic equation. Note, we need to specify the initial number of infectious individuals \\(I(0)\\).\nCan you check that this is correct? Compute \\(I'(t)\\) (\\(=\\frac{dI(t)}{dt}\\)) and check if \\(I'(t) = \\frac{\\beta (N-I(t))I(t)}{N}\\). It is a bit of work, but try it! Let me know, if you want a solution."
  },
  {
    "objectID": "W06.html#si-model-logistic-equation",
    "href": "W06.html#si-model-logistic-equation",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "SI-model: Logistic Equation",
    "text": "SI-model: Logistic Equation\n\\(I(t) = \\frac{N}{1 + (\\frac{N}{I(0)} - 1)e^{-\\beta t}}\\)\nPlot the equation for \\(N = 10000\\), \\(I_0 = 1\\), and \\(\\beta = 0.3\\)\n\nN <- 10000\nI0 <- 1\nbeta <- 0.3\nggplot() + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ) + \n xlim(c(0,75))"
  },
  {
    "objectID": "W06.html#si-model-numerical-integration",
    "href": "W06.html#si-model-numerical-integration",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "SI-model: Numerical integration",
    "text": "SI-model: Numerical integration\nAnother way of solution using, e.g., using Euler’s method.\nWe compute the solution step-by-step using increments of, e.g. \\(dt = 1\\).\n\n\nN <- 10000\nI0 <- 1\ndI <- function(I,N,b) b*I*(N - I)/N\nbeta <- 0.3\ndt <- 1 # time increment, \n# supposed to be infinitesimally small\ntmax <- 75\nt <- seq(0,tmax,dt) \n# this is the vector of timesteps\nIt <- I0 # this will become the vector \n# of the number infected I(t) over time\nfor (i in 2:length(t)) { \n # We iterate over the vector of time steps \n # and incrementally compute It\n It[i] = It[i-1] + dt * dI(It[i-1], N, beta) \n # This is called Euler's method\n}\ntibble(t, It) |> ggplot(aes(t,It)) + \n geom_function( \n  fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t))) + \n # Analytical solution for comparison\n geom_line(color = \"red\") + # The numerical solution in black\n theme_minimal(base_size = 20)\n\n\n\n\n\n\nWhy do the graphs deviate? The step size must be “infinitely” small"
  },
  {
    "objectID": "W06.html#numerical-integration-with-smaller-dt",
    "href": "W06.html#numerical-integration-with-smaller-dt",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Numerical integration with smaller \\(dt\\)",
    "text": "Numerical integration with smaller \\(dt\\)\nWe compute the solution step-by-step using small increments of, e.g. \\(dt = 0.01\\).\n\n\nN <- 10000\nI0 <- 1\ndI <- function(I,N,b) b*I*(N - I)/N\nbeta <- 0.3\ndt <- 0.01 # time increment, \n# supposed to be infinitesimally small\ntmax <- 75\nt <- seq(0,tmax,dt) \n# this is the vector of timesteps\nIt <- I0 # this will become the vector \n# of the number infected I(t) over time\nfor (i in 2:length(t)) { \n # We iterate over the vector of time steps \n # and incrementally compute It\n It[i] = It[i-1] + dt * dI(It[i-1], N, beta) \n # This is called Euler's method\n}\ntibble(t, It) |> ggplot(aes(t,It)) + \n geom_function( \n  fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t))) + \n # Analytical solution for comparison\n geom_line(color = \"red\") + # The numerical solution in black\n theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W06.html#mechanistic-model",
    "href": "W06.html#mechanistic-model",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Mechanistic model",
    "text": "Mechanistic model\nThe SI model is a potential answer to the mechanistic question How do epidemics spread?\nThe examples above show 3 different ways to explore the model:\n\nAgent-based simulation\n\nWe model every individual explicitly\nSimulation involve random numbers! So simulation runs can be different!\n\n\n\n\nNumerical integration of differential equation\n\nNeeds a more abstract concept of compartments\n\n\n\n\n\nAnalytical solutions of differential equation\n\noften not possible (therefore numerical integration is common)"
  },
  {
    "objectID": "W06.html#differentiation-with-data",
    "href": "W06.html#differentiation-with-data",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Differentiation with data",
    "text": "Differentiation with data\nWe can do calculus operations with data!\nIn empirical data we can compute the increase in a vector with the function diff:\n\nx <- c(1,2,4,5,5,3,0)\ndiff(x)\n\n[1]  1  2  1  0 -2 -3\n\n\n\nMore convenient in a data frame is to use x - lag(x) because the vector has the same length.\n\nx - lag(x)\n\n[1] NA  1  2  1  0 -2 -3"
  },
  {
    "objectID": "W06.html#the-diff-of-our-simulation-output",
    "href": "W06.html#the-diff-of-our-simulation-output",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "The diff of our simulation output",
    "text": "The diff of our simulation output\n\ng1 <- sim_output |> ggplot(aes(x = t)) + geom_line(aes(y = infected))\ng1\n\n\n\ng2 <- sim_output |> \n mutate(derivative_infected = infected - lag(infected)) |> \n ggplot(aes(x = t)) + geom_line(aes(y = derivative_infected))\ng2"
  },
  {
    "objectID": "W06.html#nd-derivative-change-of-change",
    "href": "W06.html#nd-derivative-change-of-change",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "2nd derivative: Change of change",
    "text": "2nd derivative: Change of change\n\ng3 <- sim_output |> \n mutate(derivative_infected = infected - lag(infected),\n        derivative2_infected = derivative_infected - lag(derivative_infected)) |> \n ggplot(aes(x = t)) + geom_line(aes(y = derivative2_infected))\ng3\n\n\nIn empirical data: Derivatives of higher order tend to show fluctuation"
  },
  {
    "objectID": "W06.html#interpretation-in-si-model",
    "href": "W06.html#interpretation-in-si-model",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Interpretation in SI-model",
    "text": "Interpretation in SI-model\n\n\n\\(I(t)\\) total number of infected\n\\(I'(t)\\) number of new cases per day (time step)\n\\(I''(t)\\) how the number of new cases has changes compared to yesterday\n\n2nd derivatives are a good early indicator for the end of a wave."
  },
  {
    "objectID": "W06.html#integration",
    "href": "W06.html#integration",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Integration",
    "text": "Integration\nThe integral of the daily new cases from the beginning to day \\(s\\) is \\(\\int_{-\\infty}^s f(t)dt\\) and represents the total cases at day \\(s\\).\n\nThe integral of a function \\(f\\) up to time \\(s\\) is also called the anti-derivative \\(F(s) = \\int_{-\\infty}^s f(t)dt\\).\n\nThe symbol \\(\\int\\) comes from an S and means “sum”.\n\nCompute the anti-derivative of data vector with cumsum.\n\n\nx <- c(1,2,4,5,5,3,0)\ncumsum(x)\n\n[1]  1  3  7 12 17 20 20\n\n\n\nEmpirically: Derivatives tend to become noisy, while integrals tend to become smooth."
  },
  {
    "objectID": "W06.html#the-fundamental-theorem-of-calculus",
    "href": "W06.html#the-fundamental-theorem-of-calculus",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "The fundamental theorem of calculus",
    "text": "The fundamental theorem of calculus\nThe integral of the derivative is the function itself.\nThis is not a proof but shows the idea:\n\nf <- c(1,2,4,5,5,3,0)\nantiderivative <- cumsum(f)\nantiderivative\n\n[1]  1  3  7 12 17 20 20\n\ndiff(c(0, antiderivative)) \n\n[1] 1 2 4 5 5 3 0\n\n# We have to put 0 before to regain the full vector\nderivative <- diff(f)\nderivative\n\n[1]  1  2  1  0 -2 -3\n\ncumsum(c(1,derivative)) \n\n[1] 1 2 4 5 5 3 0\n\n# We have to put in the first value (here 1) \n# manually because it was lost during the diff"
  },
  {
    "objectID": "W06.html#different-purposes-of-models",
    "href": "W06.html#different-purposes-of-models",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Different purposes of models",
    "text": "Different purposes of models\nAgent-based models and differential equations are usually used to explain the dynamics of one or more variables typically over time. They are used to answer mechanistic questions.\n\nIn the following we treat variable-based models which we use to\n\nexplain relations between variables\nmake predictions\n\nThese are often used to answer inferential and predictive questions.\n(With experimental or more theoretical effort also for causal questions.)\nFirst, we focus on linear models."
  },
  {
    "objectID": "W06.html#hello-again-penguins",
    "href": "W06.html#hello-again-penguins",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Hello again penguins!",
    "text": "Hello again penguins!\nWe use the dataset Palmer Penguins\nChinstrap, Gentoo, and Adélie Penguins\n  \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "W06.html#body-mass-in-grams",
    "href": "W06.html#body-mass-in-grams",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Body mass in grams",
    "text": "Body mass in grams\n\npenguins |>\n  ggplot(aes(body_mass_g)) +\n  geom_histogram()"
  },
  {
    "objectID": "W06.html#flipper-length-in-millimeters",
    "href": "W06.html#flipper-length-in-millimeters",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Flipper length in millimeters",
    "text": "Flipper length in millimeters\n\npenguins |>\n  ggplot(aes(flipper_length_mm)) +\n  geom_histogram()"
  },
  {
    "objectID": "W06.html#relate-variables-as-a-line",
    "href": "W06.html#relate-variables-as-a-line",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Relate variables as a line",
    "text": "Relate variables as a line\nA line is a shift-scale transformation of the identity function usually written in the form\n\\[f(x) = a\\cdot x + b\\]\nwhere \\(a\\) is the slope, \\(b\\) is the intercept.1\n\n\na <- 0.5\nb <- 1\nfunc <- function(x) a*x + b\nggplot() + geom_function(fun = func, size = 2) +\n # Set axis limits and make axis equal\n    xlim(c(-0.5,2)) + ylim(c(0,2)) + coord_fixed() + \n    geom_line( # intercept line:\n     data=tibble(x=c(0,0),y=c(0,1)), \n     mapping = aes(x,y), \n     color = \"blue\", size = 2) +\n    geom_line( # slope:\n     data=tibble(x=c(1.5,1.5),y=c(1.25,1.75)), \n     mapping = aes(x,y), \n     color = \"red\", size = 2) +\n    geom_line( # x-interval of length one:\n     data=tibble(x=c(0.5,1.5),y=c(1.25,1.25)), \n     mapping = aes(x,y), color = \"gray\") +\n    theme_classic(base_size = 24)\n\n\n\n\n\n\nThis a scale and a shift in the \\(y\\) direction. Note: For lines there are always an analog transformations on the \\(x\\) direction."
  },
  {
    "objectID": "W06.html#penguins-linear-model",
    "href": "W06.html#penguins-linear-model",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Penguins: Linear model",
    "text": "Penguins: Linear model\nFlipper length as a function of body mass.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W06.html#penguins-a-smooth-line",
    "href": "W06.html#penguins-a-smooth-line",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Penguins: A smooth line",
    "text": "Penguins: A smooth line\nFlipper length as a function of body mass with loess1 smoothing.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"loess\") + \n theme_classic(base_size = 24)\n\n\n\n\n\n\nThis is a less theory-driven and more data-driven model. Why?\nWe don’t have a simple mathematical form of the function.\nloess = locally estimated scatterplot smoothing"
  },
  {
    "objectID": "W06.html#terminology-variable-based-models",
    "href": "W06.html#terminology-variable-based-models",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Terminology variable-based models",
    "text": "Terminology variable-based models\n\nResponse variable:1 Variable whose behavior or variation you are trying to understand, on the y-axis\nExplanatory variable(s):2 Other variable(s) that you want to use to explain the variation in the response, on the x-axis\nPredicted value: Output of the model function.\n\nThe model function gives the (expected) average value of the response variable conditioning on the explanatory variables\nResidual(s): A measure of how far away a case is from its predicted value (based on the particular model)\nResidual = Observed value - Predicted value\nThe residual tells how far above/below the expected value each case is\n\n\nAlso dependent variable in statistics or empirical social sciences.Also independent variable(s) in statistics or empirical social sciences."
  },
  {
    "objectID": "W06.html#more-explanatory-variables",
    "href": "W06.html#more-explanatory-variables",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "More explanatory variables",
    "text": "More explanatory variables\nHow does the relation between flipper length and body mass change with different species?\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm, \n            color = species)) +\n geom_point() +\n geom_smooth(method = \"lm\",\n             se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W06.html#ggplot-hint-how-to-color-penguins-but-keep-one-model",
    "href": "W06.html#ggplot-hint-how-to-color-penguins-but-keep-one-model",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "ggplot-hint: How to color penguins but keep one model?",
    "text": "ggplot-hint: How to color penguins but keep one model?\nPut the mapping of the color aesthetic into the geom_point command.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n                                                y = flipper_length_mm)) +\n geom_point(aes(color = species)) +\n geom_smooth(method = \"lm\",\n                                                    se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W06.html#beware-of-simpsons-paradox",
    "href": "W06.html#beware-of-simpsons-paradox",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Beware of Simpson’s paradox",
    "text": "Beware of Simpson’s paradox\nSlopes for all groups can be in the opposite direction of the main effect’s slope!"
  },
  {
    "objectID": "W06.html#the-paradox-is-real",
    "href": "W06.html#the-paradox-is-real",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "The paradox is real!",
    "text": "The paradox is real!\nHow does the relation between bill length and body mass change with different species?\n\n\npenguins |>\n ggplot(aes(x = bill_length_mm, \n            y = bill_depth_mm)) +\n geom_point(aes(color = species)) +\n geom_smooth(mapping = aes(color = species),\n             method = \"lm\",\n             se = FALSE) + \n geom_smooth(method = \"lm\",\n                                                    se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W06.html#models---upsides-and-downsides",
    "href": "W06.html#models---upsides-and-downsides",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can reveal patterns that are not evident in a graph of the data. This is an advantage of modeling over simple visual inspection of data.\n\nHow would you visualize dependencies of more than two variables?\n\nThe risk is that a model is imposing structure that is not really there in the real world data.\n\nPeople imagined animal shapes in the stars. This is maybe a good model to detect and memorize shapes, but it has nothing to do with these animals.\nEvery model is a simplification of the real world, but there are good and bad models (for particular purposes).\nA skeptical (but constructive) approach to a model is always advisable."
  },
  {
    "objectID": "W06.html#variation-around-a-model",
    "href": "W06.html#variation-around-a-model",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Variation around a model",
    "text": "Variation around a model\n… is as interesting and important as the model!\nStatistics is the explanation of uncertainty of variation in the context of what remains unexplained.\n\nThe scattered data of flipper length and body mass suggests that there maybe other factors that account for some parts of the variability.\nOr is it randomness?\nAdding more explanatory variables can help (but need not)"
  },
  {
    "objectID": "W06.html#all-models-are-wrong",
    "href": "W06.html#all-models-are-wrong",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "All models are wrong …",
    "text": "All models are wrong …\n… but some are useful. (George Box)\nExtending the range of the model:\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE, \n                                                fullrange = TRUE) +\n    xlim(c(0,7000)) + ylim(c(0,230)) +\n theme_classic(base_size = 24)\n\n\n\n\n\n\n\nThe model predicts that penguins with zero weight still have flippers of about 140 mm on average.\nIs the model useless? Yes, around zero body mass. No, it works OK in the range of the body mass data."
  },
  {
    "objectID": "W06.html#two-model-purposes",
    "href": "W06.html#two-model-purposes",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Two model purposes",
    "text": "Two model purposes\nLinear models can be used for:\n\nExplanation: Understand the relationship of variables in a quantitative way.\nFor the linear model, interpret slope and intercept.\nPrediction: Plug in new values for the explanatory variable(s) and receive the expected response value.\nFor the linear model, predict the flipper length of new penguins by their body mass."
  },
  {
    "objectID": "W06.html#in-r-tidymodels",
    "href": "W06.html#in-r-tidymodels",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "In R: tidymodels",
    "text": "In R: tidymodels\n\n\n\nFrom https://datasciencebox.org"
  },
  {
    "objectID": "W06.html#our-goal",
    "href": "W06.html#our-goal",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Our goal",
    "text": "Our goal\nPredict flipper length from body mass\naverage flipper_length_mm \\(= \\beta_0 + \\beta_1\\cdot\\) body_mass_g"
  },
  {
    "objectID": "W06.html#step-1-specify-model",
    "href": "W06.html#step-1-specify-model",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlibrary(tidymodels)\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "W06.html#step-2-set-the-model-fitting-engine",
    "href": "W06.html#step-2-set-the-model-fitting-engine",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Step 2: Set the model fitting engine",
    "text": "Step 2: Set the model fitting engine\n\nlinear_reg() |> \n    set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "W06.html#step-3-fit-model-and-estimate-parameters",
    "href": "W06.html#step-3-fit-model-and-estimate-parameters",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Step 3: Fit model and estimate parameters",
    "text": "Step 3: Fit model and estimate parameters\nOnly now, the data and the variable selection comes in.\nUse of formula syntax\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\nparsnip is package in tidymodels which is to provide a tidy, unified interface to models that can be used to try a range of models.\n\n\nNote: The fit command does not follow the tidyverse principle the data comes first. Instead, the formula comes first. This is to relate to existing traditions of a much older established way of modeling in base R."
  },
  {
    "objectID": "W06.html#what-does-the-output-say",
    "href": "W06.html#what-does-the-output-say",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "What does the output say?",
    "text": "What does the output say?\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\n\naverage flipper_length_mm \\(= 136.72956 + 0.01528\\cdot\\) body_mass_g\n\n\nInterpretation:\nThe penguins have a flipper length of 138 mm plus 0.01528 mm for each gram of body mass (that is 15.28 mm per kg). Penguins with zero mass have a flipper length of 138 mm. However, this is not in the range where the model was fitted."
  },
  {
    "objectID": "W06.html#show-output-in-tidy-form",
    "href": "W06.html#show-output-in-tidy-form",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Show output in tidy form",
    "text": "Show output in tidy form\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n    tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107"
  },
  {
    "objectID": "W06.html#parameter-estimation",
    "href": "W06.html#parameter-estimation",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nNotation from statistics: \\(\\beta\\)’s for the population parameters and \\(b\\)’s for the parameters estimated from the sample statistics.\n\\[\\hat y = \\beta_0 + \\beta_1 x\\]\nIs what we cannot have. (\\(\\hat y\\) stands for predicted value of \\(y\\). )\n\nWe estimate \\(b_0\\) and \\(b_1\\) in\n\\[\\hat y = b_0 + b_1 x\\]\n\nA typical follow-up data analysis question is what the fitted values \\(b_0\\) and \\(b_1\\) tell us about the population-wide values \\(\\beta_0\\) and \\(\\beta_1\\)?\nThis is the typical inferential question."
  },
  {
    "objectID": "W06.html#fitting-linear-models-part-2",
    "href": "W06.html#fitting-linear-models-part-2",
    "title": "W#06: Epidemic Modeling, Calculus, Linear Model, Fitting a Linear Model",
    "section": "Fitting Linear Models Part 2",
    "text": "Fitting Linear Models Part 2\nNext week!\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "hw-01.html",
    "href": "hw-01.html",
    "title": "Homework 01",
    "section": "",
    "text": "The goal of this assignment is to introduce you to R, RStudio, Git, and GitHub, which you’ll be using throughout the course both to learn the data science concepts discussed in the course and to analyze real data and come to informed conclusions."
  },
  {
    "objectID": "hw-01.html#prerequisites",
    "href": "hw-01.html#prerequisites",
    "title": "Homework 01",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites\nFirst, you need to achieve the following\n\nReview the Lecture of Week 1\nRead Happy Git with R: Why Git? Why GitHub? (It is OK do a fast read only. You may reconsider later.)\nYou have a GitHub account to become a member of the GitHub organization. If you do not have an account follow the instructions on the front page of https://github.com/CU-F23-MDSSB-01-Concepts-Tools.\nYou have provided your GitHub username in this form https://forms.office.com/e/fraU0w0gQq (If you provided your name wrongly, write an email to the instructor.)\nSoftware installations on your computer:\n\nRead Happy Git with R: Install or upgrade R and RStudio\nFollow the instructions to install R and RStudio\nRead Happy Git with R: Install Git\nFollow the instructions (choose the highly recommended) to install Git SCM (= Source Control Managment)\nInstall quarto CLI (= Command Line Interface) https://quarto.org/docs/get-started/.\nNote: In the following you will do similar things as described in Tutorial: Hello, Quarto and the following two tutorials.\n\nIn R, install some packages from the following list. You can use the command install.packages(\"tidyverse\",\"gitcreds\",\"usethis\")\nNotes: tidyverse is a collection of many packages it can take some time to install. usethis an gitcreds provide convenient functions to tell your git-installation your GitHub-credentials to push your work back to github via a Personal Access Token (PAT). Read Happy Git with R: Personal access token for HTTPS for how PATs are used similar but not identical to passwords.\n\n\n\n\n\n\n\nMore software\n\n\n\n\n\nSoon, we will need more software: python using the IDE (integrated development environment) Visual Studio Code."
  },
  {
    "objectID": "hw-01.html#terminology",
    "href": "hw-01.html#terminology",
    "title": "Homework 01",
    "section": "1.2 Terminology",
    "text": "1.2 Terminology\nWe’ve already thrown around a few new terms, so let’s define them before we proceed.\n\nR and python: Names of the programming language we will be using throughout the course.\nRStudio: An integrated development environment developed for R. In other words, a convenient interface for writing and running code.\nGit: A version control system.\nGitHub: A web platform for hosting version controlled files and facilitating collaboration among users.\nquarto: A command line tool which can render (among other things) qmd-files (with text and code chunks) to nice looking html\nRepository: A Git repository contains all of your project’s files and stores each file’s revision history.\nIt’s common to refer to a repository as a repo.\n\nIn this course, assignment you work on will be contained in a personalized git repo.\nFor individual assignments, only you will have access to the repo. For team assignments, all team members will have access to a single repo where they work collaboratively.\nAll repos associated with this course are housed in the GitHub organization https://github.com/CU-F23-MDSSB-01-Concepts-Tools. The organization is set up such that students can only see repos they have access to, but the course instructors can see all of them."
  },
  {
    "objectID": "hw-01.html#starting-slowly-step-by-step",
    "href": "hw-01.html#starting-slowly-step-by-step",
    "title": "Homework 01",
    "section": "1.3 Starting slowly step by step",
    "text": "1.3 Starting slowly step by step\nAs the course progresses, you are encouraged to explore beyond what the assignments dictate; a willingness to experiment will make you a much better programmer! Before we get to that stage, however, you need to build some basic fluency in the tools and the workflow we use. First, we will explore the fundamental building blocks of all of these tools.\nBefore you can get started with the analysis, you need to make sure you:\n\nhave a GitHub account\nare a member of the course GitHub organization https://github.com/CU-F23-MDSSB-01-Concepts-Tools\nhave the needed software stack installation on your local machine (see the Prerequisites Checklist above)\n\nIf you failed to confirm any of these, it means you have not yet completed the prerequisites for this assignment. Please go back to Prerequisites and complete them before continuing the assignment."
  },
  {
    "objectID": "hw-01.html#step-0.-authenticate-git-to-access-your-github-content",
    "href": "hw-01.html#step-0.-authenticate-git-to-access-your-github-content",
    "title": "Homework 01",
    "section": "2.1 Step 0. Authenticate git to access your GitHub content",
    "text": "2.1 Step 0. Authenticate git to access your GitHub content\nBefore you can clone your repository you need to tell GitHub that you are authorized to do this, and to that end you need to make a Personal Access Token (PAT) in your GitHub account and make this available to git and RStudio on your local machine.\nThere are several ways to do this (e.g. from the command line) but as we will use RStudio anyway, we can use a convenient way provided there. Read more about PATs and how to use them in Happy Git with R: Personal access token for HTTPS (in particular the TL;DR which describes what we use).\nOpen RStudio and install the packages usethis and gitcreds if you haven’t done already: Go to the “Console” pane at the bottom left. Type in\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nand hit Enter. Now the packages should be installed.\nNow, use two commands. Copy them to the console and click Enter:\nusethis::create_github_token()\nThis opens http://github.com and you may need to log in. Then you can make the PAT (read more details in “Happy Git with R”). For today, you can go the fast way and do not think about the options and click “Generate token”. (If you feel safe enough you can also go for a token without an expiration day against advise from GitHub, if not you just have to do the “dance” again when the PAT expires.)\nUse the clipboard icon 📋 to copy the PAT. Go back to RStudio and do in the console:\ngitcreds::gitcreds_set()\nIn the dialog in the console paste your PAT from the clipboard and press Enter. That should be it and you do not need to repeat these steps until the PAT expires. (If the PAT expires you have to make a new one in the same way.)"
  },
  {
    "objectID": "hw-01.html#step-1.-get-url-of-repo-to-be-cloned",
    "href": "hw-01.html#step-1.-get-url-of-repo-to-be-cloned",
    "title": "Homework 01",
    "section": "2.2 Step 1. Get URL of repo to be cloned",
    "text": "2.2 Step 1. Get URL of repo to be cloned\nOn GitHub https://github.com/CU-F23-MDSSB-01-Concepts-Tools, open your repository for Homework 1.\n\nOn GitHub, click on the green Code button, select HTTPS (this might already be selected by default, and if it is, you’ll see the text Use Git or checkout with SVN using the web URL). Click on the clipboard icon 📋 to copy the repo URL."
  },
  {
    "objectID": "hw-01.html#step-2.-go-to-rstudio",
    "href": "hw-01.html#step-2.-go-to-rstudio",
    "title": "Homework 01",
    "section": "2.3 Step 2. Go to RStudio",
    "text": "2.3 Step 2. Go to RStudio\nGo to your RStudio. Select “New Project…” either from the File menu or from the special project menu on the top right of the RStudio window.\n\nIn the New Project Wizard, click on “Version Control” and then “Git”.\n\n\n\n\n\n\nImportant\n\n\n\nIf “Version Control” or “Git” is not available in your RStudio, then either you haven’t installed git on you computer or your RStudio installation has not recognized it properly. In a correct installation RStudio would recognize git on your machine automatically when started. You have to solve this issue first to continue.\n\n\nThen paste the repositories URL (which should still be in your clipboard) into the “Repository URL:” field. Leave directory name as it is automatically chosen, but make sure that you create the directory in a the folder where you want to store the course material on your computer via the “Browse …” button.\n\n\n\n\n\n\nOrganize your Computer for your Studies!\n\n\n\n\n\n\nCreate a folder for Data Science Concepts and Tools\nPut all repos there\n\n\n\n\nWhen you click “Create Project”\n\ngit will create a new directory in the folder, copies all the files from github to it, and initializes your git repository locally\nRStudio will switch to that as the new project"
  },
  {
    "objectID": "hw-01.html#step-1.-update-the-yaml",
    "href": "hw-01.html#step-1.-update-the-yaml",
    "title": "Homework 01",
    "section": "4.1 Step 1. Update the YAML",
    "text": "4.1 Step 1. Update the YAML\nOpen the quarto (qmd) file in your project. In the YAML change the author name to your name, and “Render” the document.\n This calls quarto, and quarto renders the document. In this case, that means, quarto creates a new file “hw-01-R.md” in the html format as specified in the YAML.\nWhen the file was rendered successfully, RStudio shows it in the “Viewer” pane at the bottom right. At the same place you can look in the “Files” pane you can check if the file is there. You can click on it an select to show it in your Brwoser.\nNow you see a nice html-page in the Browser! You made a page like every ordinary site on the internet. So you made an important step towards publishing you data science work on the internet. However now the html page is only locally on your computer.\n\nIf you do not find the “Render” button in your RStudio installation, then either quarto is not installed or RStudio has not recognized. You have to fix this issue first before you can continue. Another source of error while rendering could be that you haven’t installed the tidyverse package."
  },
  {
    "objectID": "hw-01.html#step-2-commit",
    "href": "hw-01.html#step-2-commit",
    "title": "Homework 01",
    "section": "4.2 Step 2: Commit",
    "text": "4.2 Step 2: Commit\nGo to the Git pane in your RStudio (top right corner).\nYou should see that your qmd (quarto) file and its output, your html-file, are listed there as recently changed files.\nNext, click on Diff. This will pop open a new window that shows you the difference between the last committed state of the document and its current state that includes your changes. (Click on the file “hw-01-R.qmd”.) If you’re happy with these changes, click on the checkboxes of all files in the list, and type “Update author name” in the Commit message box and hit Commit and then close the window.\n\nYou don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we may tell you exactly when to commit and what commit message to use. As the semester progresses we will let you make these decisions."
  },
  {
    "objectID": "hw-01.html#step-3.-push",
    "href": "hw-01.html#step-3.-push",
    "title": "Homework 01",
    "section": "4.3 Step 3. Push",
    "text": "4.3 Step 3. Push\nNow that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub.\nWhy?\nSo that others can see your changes.\nAnd by others, we mean the course instructors (your repos in this course are private to you and us, only).\nGo the Git pane and click on Push.\nThought exercise: Which of the steps “updating the YAML”, “committing”, and “pushing” involves talking to GitHub?1"
  },
  {
    "objectID": "hw-01.html#check-what-you-did",
    "href": "hw-01.html#check-what-you-did",
    "title": "Homework 01",
    "section": "4.4 Check what you did",
    "text": "4.4 Check what you did\nGo to your repository on https://github.com/JU-F22-MDSSB-MET-01 and check if the files appear there as in you local folder.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nYou may wonder why clicking on the the html-file does not show the page as you saw it locally in your browser.\nThe reason is, that GitHub is essentially a code editor. So, it shows you the html-code. This code was created by rendering. DO not touch it “by hand”. It will be overwritten every time you render again.\nViewing the html as in the browser is not possible here. The instructor will clone you repository and can then look at your html locally on their machines.\nContext Information: It is possible to publish webpages from GitHub. You can learn this later.\nContext Information: GitHub provides its own gfm = GitHub Flavoured Markdown. The README.md files are written in this. gfm is very similar to the Markdown language we use."
  },
  {
    "objectID": "W10.html#model-purpose-predict-eu-attitudes",
    "href": "W10.html#model-purpose-predict-eu-attitudes",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model purpose: Predict EU attitudes",
    "text": "Model purpose: Predict EU attitudes\n\ness <- ess_raw |> filter(essround == 9) |> \n select(cntry, euftf, atchctr, atcherp, imueclt, lrscale) |> \n mutate(euftf = euftf |> na_if(77) |> na_if(88) |> na_if(99), \n        atchctr = atchctr |> na_if(77) |> na_if(88) |> na_if(99),\n        atcherp = atcherp |> na_if(77) |> na_if(88) |> na_if(99),\n        imueclt = imueclt |> na_if(77) |> na_if(88) |> na_if(99),\n        lrscale = lrscale |> na_if(77) |> na_if(88) |> na_if(99))\n\nFor the ESS dataset\n\nwe filter for people from round 9 (2018)\nselect 5 attitude variables and cntry with 29 countries: AT, BE, BG, CH, CY, CZ, DE, DK, EE, ES, FI, FR, GB, HR, HU, IE, IS, IT, LT, LV, ME, NL, NO, PL, PT, RS, SE, SI, SK\nrecode NA’s properly for five variables:\neuftf: European Union: European unification go further (=10) or gone too far (=0)\natchctr: How emotionally attached to [country] (0 to 10)\natcherp: How emotionally attached to Europe (0 to 10)\nimueclt: Country’s cultural life undermined (=0) or enriched (=10) by immigrants\nlrscale: Placement on left (=0) right (=10) scale"
  },
  {
    "objectID": "W10.html#model-1-2-and-3",
    "href": "W10.html#model-1-2-and-3",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model 1, 2, and 3",
    "text": "Model 1, 2, and 3\n\n\n\nCreate an initial split with 80% training data\nCreate a linear model ess_mod\nCreate three recipes\n\ness_rec1 without using the country variable\ness_rec2 with main effects for all 29 countries\ness_rec3 with additional interaction effects for all 29 countries\n\nCreate the workflow\nFit 3 models by adding the 3 recipes\n\n\n\nset.seed(7)\ness_split <- initial_split(ess, prop = 0.80)\ness_train <- training(ess_split)\ness_test <- testing(ess_split)\n\ness_model <- linear_reg() |> set_engine(\"lm\")\ness_rec1 <- ess_train |> \n recipe(euftf ~ .) |> \n step_rm(cntry)\ness_rec2 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry)\ness_rec3 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry) |> \n step_interact(~starts_with(\"cntry\"):c(atchctr,atcherp,imueclt,lrscale))\n\ness_wflow <- workflow() |> \n add_model(ess_model)\n\ness_fit1 <- ess_wflow |> \n add_recipe(ess_rec1) |> \n fit(ess_train)\ness_fit2 <- ess_wflow |> \n add_recipe(ess_rec2) |> \n fit(ess_train)\ness_fit3 <- ess_wflow |> \n add_recipe(ess_rec3) |> \n fit(ess_train)"
  },
  {
    "objectID": "W10.html#model-fits",
    "href": "W10.html#model-fits",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model fits",
    "text": "Model fits\n\n\n\ntidy(ess_fit1) |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 5 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   2.92  \n2 atchctr      -0.0952\n3 atcherp       0.285 \n4 imueclt       0.287 \n5 lrscale      -0.0401\n\n\n\n\ntidy(ess_fit2)  |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 33 × 2\n   term        estimate\n   <chr>          <dbl>\n 1 (Intercept)  2.10   \n 2 atchctr     -0.101  \n 3 atcherp      0.294  \n 4 imueclt      0.299  \n 5 lrscale     -0.0196 \n 6 cntry_BE     0.747  \n 7 cntry_BG     1.20   \n 8 cntry_CH    -0.0476 \n 9 cntry_CY     1.39   \n10 cntry_CZ     0.184  \n11 cntry_DE     1.32   \n12 cntry_DK     1.01   \n13 cntry_EE     0.532  \n14 cntry_ES     1.14   \n15 cntry_FI     0.00996\n16 cntry_FR     0.743  \n17 cntry_GB    -0.0705 \n18 cntry_HR     0.729  \n19 cntry_HU     0.216  \n20 cntry_IE     0.294  \n21 cntry_IS     0.0371 \n22 cntry_IT     0.593  \n23 cntry_LT     2.15   \n24 cntry_LV     0.426  \n25 cntry_ME     2.41   \n26 cntry_NL     0.600  \n27 cntry_NO    -0.170  \n28 cntry_PL     1.19   \n29 cntry_PT     1.42   \n30 cntry_RS     1.21   \n31 cntry_SE    -0.0761 \n32 cntry_SI     1.53   \n33 cntry_SK     0.178  \n\n\n\n\ntidy(ess_fit3)  |> select(term, estimate) |> print(n = 145)\n\n# A tibble: 145 × 2\n    term               estimate\n    <chr>                 <dbl>\n  1 (Intercept)         2.65   \n  2 atchctr            -0.135  \n  3 atcherp             0.259  \n  4 imueclt             0.412  \n  5 lrscale            -0.141  \n  6 cntry_BE            0.0202 \n  7 cntry_BG           -0.264  \n  8 cntry_CH            1.70   \n  9 cntry_CY            0.242  \n 10 cntry_CZ           -0.914  \n 11 cntry_DE            0.248  \n 12 cntry_DK            0.637  \n 13 cntry_EE           -0.709  \n 14 cntry_ES            1.15   \n 15 cntry_FI            0.143  \n 16 cntry_FR            0.136  \n 17 cntry_GB            0.486  \n 18 cntry_HR            0.177  \n 19 cntry_HU           -0.190  \n 20 cntry_IE            0.186  \n 21 cntry_IS            0.881  \n 22 cntry_IT           -0.582  \n 23 cntry_LT            0.925  \n 24 cntry_LV           -1.60   \n 25 cntry_ME            0.542  \n 26 cntry_NL           -0.804  \n 27 cntry_NO            0.763  \n 28 cntry_PL            2.24   \n 29 cntry_PT            2.18   \n 30 cntry_RS            0.220  \n 31 cntry_SE           -0.507  \n 32 cntry_SI            1.19   \n 33 cntry_SK           -0.604  \n 34 cntry_BE_x_atchctr -0.0249 \n 35 cntry_BE_x_atcherp  0.128  \n 36 cntry_BE_x_imueclt -0.130  \n 37 cntry_BE_x_lrscale  0.148  \n 38 cntry_BG_x_atchctr  0.220  \n 39 cntry_BG_x_atcherp -0.0614 \n 40 cntry_BG_x_imueclt -0.159  \n 41 cntry_BG_x_lrscale  0.151  \n 42 cntry_CH_x_atchctr -0.0286 \n 43 cntry_CH_x_atcherp  0.0194 \n 44 cntry_CH_x_imueclt -0.197  \n 45 cntry_CH_x_lrscale -0.106  \n 46 cntry_CY_x_atchctr  0.0956 \n 47 cntry_CY_x_atcherp  0.0496 \n 48 cntry_CY_x_imueclt -0.336  \n 49 cntry_CY_x_lrscale  0.300  \n 50 cntry_CZ_x_atchctr -0.0375 \n 51 cntry_CZ_x_atcherp -0.0267 \n 52 cntry_CZ_x_imueclt  0.0735 \n 53 cntry_CZ_x_lrscale  0.273  \n 54 cntry_DE_x_atchctr  0.0299 \n 55 cntry_DE_x_atcherp  0.159  \n 56 cntry_DE_x_imueclt -0.127  \n 57 cntry_DE_x_lrscale  0.0836 \n 58 cntry_DK_x_atchctr -0.0884 \n 59 cntry_DK_x_atcherp  0.0752 \n 60 cntry_DK_x_imueclt  0.0109 \n 61 cntry_DK_x_lrscale  0.0970 \n 62 cntry_EE_x_atchctr  0.0895 \n 63 cntry_EE_x_atcherp  0.0464 \n 64 cntry_EE_x_imueclt -0.137  \n 65 cntry_EE_x_lrscale  0.180  \n 66 cntry_ES_x_atchctr  0.0735 \n 67 cntry_ES_x_atcherp -0.0706 \n 68 cntry_ES_x_imueclt -0.0704 \n 69 cntry_ES_x_lrscale  0.0210 \n 70 cntry_FI_x_atchctr -0.106  \n 71 cntry_FI_x_atcherp  0.0602 \n 72 cntry_FI_x_imueclt -0.104  \n 73 cntry_FI_x_lrscale  0.176  \n 74 cntry_FR_x_atchctr -0.0254 \n 75 cntry_FR_x_atcherp  0.185  \n 76 cntry_FR_x_imueclt -0.195  \n 77 cntry_FR_x_lrscale  0.139  \n 78 cntry_GB_x_atchctr -0.0968 \n 79 cntry_GB_x_atcherp  0.161  \n 80 cntry_GB_x_imueclt -0.196  \n 81 cntry_GB_x_lrscale  0.0630 \n 82 cntry_HR_x_atchctr  0.145  \n 83 cntry_HR_x_atcherp -0.0138 \n 84 cntry_HR_x_imueclt -0.213  \n 85 cntry_HR_x_lrscale  0.120  \n 86 cntry_HU_x_atchctr  0.0531 \n 87 cntry_HU_x_atcherp -0.0656 \n 88 cntry_HU_x_imueclt -0.0588 \n 89 cntry_HU_x_lrscale  0.160  \n 90 cntry_IE_x_atchctr  0.0630 \n 91 cntry_IE_x_atcherp -0.0230 \n 92 cntry_IE_x_imueclt -0.138  \n 93 cntry_IE_x_lrscale  0.0874 \n 94 cntry_IS_x_atchctr -0.186  \n 95 cntry_IS_x_atcherp  0.111  \n 96 cntry_IS_x_imueclt -0.0756 \n 97 cntry_IS_x_lrscale  0.0596 \n 98 cntry_IT_x_atchctr  0.0381 \n 99 cntry_IT_x_atcherp -0.0405 \n100 cntry_IT_x_imueclt  0.114  \n101 cntry_IT_x_lrscale  0.111  \n102 cntry_LT_x_atchctr  0.145  \n103 cntry_LT_x_atcherp -0.113  \n104 cntry_LT_x_imueclt -0.0917 \n105 cntry_LT_x_lrscale  0.219  \n106 cntry_LV_x_atchctr  0.130  \n107 cntry_LV_x_atcherp -0.0431 \n108 cntry_LV_x_imueclt -0.161  \n109 cntry_LV_x_lrscale  0.365  \n110 cntry_ME_x_atchctr  0.198  \n111 cntry_ME_x_atcherp  0.136  \n112 cntry_ME_x_imueclt -0.179  \n113 cntry_ME_x_lrscale  0.0701 \n114 cntry_NL_x_atchctr  0.0416 \n115 cntry_NL_x_atcherp  0.138  \n116 cntry_NL_x_imueclt -0.130  \n117 cntry_NL_x_lrscale  0.182  \n118 cntry_NO_x_atchctr -0.0643 \n119 cntry_NO_x_atcherp -0.0283 \n120 cntry_NO_x_imueclt -0.202  \n121 cntry_NO_x_lrscale  0.187  \n122 cntry_PL_x_atchctr -0.00472\n123 cntry_PL_x_atcherp  0.0737 \n124 cntry_PL_x_imueclt -0.231  \n125 cntry_PL_x_lrscale -0.0290 \n126 cntry_PT_x_atchctr  0.0377 \n127 cntry_PT_x_atcherp -0.114  \n128 cntry_PT_x_imueclt -0.178  \n129 cntry_PT_x_lrscale  0.126  \n130 cntry_RS_x_atchctr  0.139  \n131 cntry_RS_x_atcherp  0.0996 \n132 cntry_RS_x_imueclt -0.285  \n133 cntry_RS_x_lrscale  0.167  \n134 cntry_SE_x_atchctr  0.0493 \n135 cntry_SE_x_atcherp -0.00505\n136 cntry_SE_x_imueclt -0.138  \n137 cntry_SE_x_lrscale  0.160  \n138 cntry_SI_x_atchctr  0.199  \n139 cntry_SI_x_atcherp -0.165  \n140 cntry_SI_x_imueclt -0.256  \n141 cntry_SI_x_lrscale  0.204  \n142 cntry_SK_x_atchctr  0.0955 \n143 cntry_SK_x_atcherp -0.0549 \n144 cntry_SK_x_imueclt -0.0424 \n145 cntry_SK_x_lrscale  0.137  \n\n\n\n\nNote: We omit std.error, p-values and so on in the display here because they are usually small in this large dataset, and we will not look at them now."
  },
  {
    "objectID": "W10.html#recap-interpreting-interaction-effects",
    "href": "W10.html#recap-interpreting-interaction-effects",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Recap: Interpreting interaction effects",
    "text": "Recap: Interpreting interaction effects\nIn Model 3 the reference country is Austria (AT) therefore the intercept and main coefficient are valid for Austria and all interaction coefficients have to be added to these to be interpreted.\nCross check: A linear model with the data filtered for Austria only without a country effect:\n\ness_train |> \n filter(cntry==\"AT\") |> \n select(-cntry) |> \n lm(formula = euftf ~ . ) |> \n tidy()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    2.65     0.364       7.28 5.06e-13\n2 atchctr       -0.135    0.0353     -3.83 1.34e- 4\n3 atcherp        0.259    0.0296      8.76 4.51e-18\n4 imueclt        0.412    0.0268     15.3  6.49e-50\n5 lrscale       -0.141    0.0338     -4.16 3.34e- 5\n\n\nThe coefficients are identical to the full model with all interaction effects."
  },
  {
    "objectID": "W10.html#make-predictions-for-training-data",
    "href": "W10.html#make-predictions-for-training-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\ness_train_pred1 <- predict(ess_fit1, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred1\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.27     3 HR          8       7       5       8\n 2  4.93     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.20    NA BG         10       7       5       5\n 6  5.67     8 BG          9       6       8      10\n 7  5.52     3 AT         10      10       3       4\n 8  7.22     3 FR          9       8      10       0\n 9  4.17     1 FI          8       5       3       7\n10  4.47     4 BG         10       7       3       9\n# ℹ 39,605 more rows\n\n\nNote:\n\nWe can make predictions when the response in NA\nWe cannot make predictions when on predictor is NA"
  },
  {
    "objectID": "W10.html#make-predictions-for-training-data-1",
    "href": "W10.html#make-predictions-for-training-data-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\n\n\ness_train_pred2 <- predict(ess_fit2, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred2\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.42     3 HR          8       7       5       8\n 2  5.53     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.75    NA BG         10       7       5       5\n 6  6.36     8 BG          9       6       8      10\n 7  4.86     3 AT         10      10       3       4\n 8  7.28     3 FR          9       8      10       0\n 9  3.54     1 FI          8       5       3       7\n10  5.08     4 BG         10       7       3       9\n# ℹ 39,605 more rows\n\n\n\n\ness_train_pred3 <- predict(ess_fit3, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred3\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.45     3 HR          8       7       5       8\n 2  5.22     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.94    NA BG         10       7       5       5\n 6  6.47     8 BG          9       6       8      10\n 7  4.56     3 AT         10      10       3       4\n 8  7.06     3 FR          9       8      10       0\n 9  3.63     1 FI          8       5       3       7\n10  5.47     4 BG         10       7       3       9\n# ℹ 39,605 more rows"
  },
  {
    "objectID": "W10.html#r-squared",
    "href": "W10.html#r-squared",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "R-squared",
    "text": "R-squared\nRecap R-squared: Percentage of variability in euftf explained by the model\n\n\nrsq(ess_train_pred1, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.178\n\n\n\n\n\nrsq(ess_train_pred2, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.230\n\n\n\n\n\nrsq(ess_train_pred3, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.253\n\n\n\nWhich model is better in prediction?"
  },
  {
    "objectID": "W10.html#root-mean-squared-error-rmse",
    "href": "W10.html#root-mean-squared-error-rmse",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Root mean squared error (RMSE)",
    "text": "Root mean squared error (RMSE)\nRMSE is an alternative measure of performance.\n\\[\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}\\]\nwhere \\(\\hat{y}_i\\) is the predicted value and \\(y_i\\) the true value.\n(The name RMSE pretty much describes what the measure does.)\n\n\nrmse(ess_train_pred1, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.44\n\n\n\n\n\nrmse(ess_train_pred2, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.36\n\n\n\n\n\nrmse(ess_train_pred3, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.32"
  },
  {
    "objectID": "W10.html#should-we-prefer-larger-or-lower-rmse",
    "href": "W10.html#should-we-prefer-larger-or-lower-rmse",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Should we prefer larger or lower RMSE?",
    "text": "Should we prefer larger or lower RMSE?\nLower. The lower the error, the better the model’s prediction.\nNotes:\n\nThe common method to fit a linear model is the ordinary least squares (OLS) method\nThat means the fitted parameters should deliver the lowest possible sum of squared errors (SSE) between predicted and observed values.\nMinimizing the sum of squared errors (SSE) is identical to minimizing the mean of squared errors (MSE) because it only adds the factor \\(1/n\\).\nMinimizing the mean of squared errors (MSE) is identical to minimizing the root mean of squared errors (RMSE) because the square root is strictly monotone function.\n\nConclusion: RMSE can be seen as a definition of the OLS optimization goal."
  },
  {
    "objectID": "W10.html#interpreting-rmse",
    "href": "W10.html#interpreting-rmse",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Interpreting RMSE",
    "text": "Interpreting RMSE\nIn contrast to R-squared, RMSE can only be interpreted with knowledge about the range and of the response variable.\nThe values of euftf range from 0 to 10\n\nThe RMSE of 2.3244529 shows how much predicted values deviate from the true value on average. (Taking the squaring of differences and root of the average into account.)"
  },
  {
    "objectID": "W10.html#make-predictions-for-testing-data",
    "href": "W10.html#make-predictions-for-testing-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\ness_test_pred3 <- predict(ess_fit3, ess_test) |> \n bind_cols(ess_test |> select(euftf, everything()))\ness_test_pred3\n\n# A tibble: 9,904 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  3.93     5 AT          9       6       4       5\n 2 NA        2 AT         10       3       7      NA\n 3  4.49     4 AT          5       5       4       3\n 4  2.56     3 AT          8       6       0       4\n 5  7.73    10 AT         10      10      10       2\n 6  6.66     8 AT          5       7       8       3\n 7  2.87     2 AT         10       4       3       5\n 8  3.43     5 AT         10       3       5       5\n 9  7.19    10 AT          7       9       8       1\n10  3.24     8 AT         10       6       3       6\n# ℹ 9,894 more rows"
  },
  {
    "objectID": "W10.html#training-vs.-testing-data-prediction",
    "href": "W10.html#training-vs.-testing-data-prediction",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Training vs. testing data prediction",
    "text": "Training vs. testing data prediction\n\n\n\n\n\nModel\nMetric\nTrain\nTest\n\n\n\n\n1\nR-squared\n0.178\n0.169\n\n\n1\nRMSE\n2.440\n2.451\n\n\n2\nR-squared\n0.230\n0.216\n\n\n2\nRMSE\n2.360\n2.381\n\n\n3\nR-squared\n0.253\n0.242\n\n\n3\nRMSE\n2.324\n2.340\n\n\n\n\n\n\nR-squared is a little lower in the test data, RMSE a bit higher (both mean lower performance)\nOften, metrics are worse for the testing data, as here.\nHowever, in it can also be the other way round by chance."
  },
  {
    "objectID": "W10.html#how-to-evaluate-performance-on-training-data-only",
    "href": "W10.html#how-to-evaluate-performance-on-training-data-only",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "How to evaluate performance on training data only?",
    "text": "How to evaluate performance on training data only?\n\nModel performance changes with the random selection of the training data. How can we then reliably compare models?\nAnyway, the training data is not a good source for model performance. It is not an independent piece of information. Predicting the training data only reveals what the model already “knows”.\nAlso, we should save the testing data only for the final validation, so we should not use it systematically to compare models.\n\nA solution: Cross validation"
  },
  {
    "objectID": "W10.html#cross-validation-1",
    "href": "W10.html#cross-validation-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, \\(v\\)-fold cross validation:\n\nShuffle your data and make a partition with \\(v\\) parts\n\nRecall from set theory: A partition is a division of a set into mutually disjoint parts which union cover the whole set. Here applied to observations (rows) in a data frame.\n\nUse 1 part for validation, and the remaining \\(v-1\\) parts for training\nRepeat \\(v\\) times"
  },
  {
    "objectID": "W10.html#cross-validation-2",
    "href": "W10.html#cross-validation-2",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "W10.html#split-data-into-folds",
    "href": "W10.html#split-data-into-folds",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Split data into folds",
    "text": "Split data into folds\nWe split the ess data into ten parts.\n\n\n\nfolds <- vfold_cv(ess_train, v = 10)\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits               id    \n   <list>               <chr> \n 1 <split [35653/3962]> Fold01\n 2 <split [35653/3962]> Fold02\n 3 <split [35653/3962]> Fold03\n 4 <split [35653/3962]> Fold04\n 5 <split [35653/3962]> Fold05\n 6 <split [35654/3961]> Fold06\n 7 <split [35654/3961]> Fold07\n 8 <split [35654/3961]> Fold08\n 9 <split [35654/3961]> Fold09\n10 <split [35654/3961]> Fold10"
  },
  {
    "objectID": "W10.html#fit-resamples",
    "href": "W10.html#fit-resamples",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Fit resamples",
    "text": "Fit resamples\nWe use the workflow (model plus formula and recipe) we have on the folds with fit_resamples.\n\ness_fit3_rs <- ess_wflow |> add_recipe(ess_rec3) |> \n fit_resamples(folds)\ness_fit3_rs\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits               id     .metrics         .notes          \n   <list>               <chr>  <list>           <list>          \n 1 <split [35653/3962]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]>\n 2 <split [35653/3962]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]>\n 3 <split [35653/3962]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]>\n 4 <split [35653/3962]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]>\n 5 <split [35653/3962]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]>\n 6 <split [35654/3961]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]>\n 7 <split [35654/3961]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]>\n 8 <split [35654/3961]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]>\n 9 <split [35654/3961]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]>\n10 <split [35654/3961]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]>\n\n\nThis computes a set of performance metrics for each folds. For linear models the defaults are R-squared and RMSE."
  },
  {
    "objectID": "W10.html#collect-the-metrics",
    "href": "W10.html#collect-the-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Collect the metrics",
    "text": "Collect the metrics\n\ness_fit3_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   2.34     10 0.00794 Preprocessor1_Model1\n2 rsq     standard   0.245    10 0.00667 Preprocessor1_Model1\n\n\nThese values are indeed closer to the values we got for the test data."
  },
  {
    "objectID": "W10.html#deeper-look-into-the-metrics",
    "href": "W10.html#deeper-look-into-the-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Deeper look into the metrics",
    "text": "Deeper look into the metrics\n\n\n\ness_fit3_rs |> collect_metrics(summarize = FALSE)\n\n# A tibble: 20 × 5\n   id     .metric .estimator .estimate .config             \n   <chr>  <chr>   <chr>          <dbl> <chr>               \n 1 Fold01 rmse    standard       2.29  Preprocessor1_Model1\n 2 Fold01 rsq     standard       0.278 Preprocessor1_Model1\n 3 Fold02 rmse    standard       2.35  Preprocessor1_Model1\n 4 Fold02 rsq     standard       0.238 Preprocessor1_Model1\n 5 Fold03 rmse    standard       2.33  Preprocessor1_Model1\n 6 Fold03 rsq     standard       0.258 Preprocessor1_Model1\n 7 Fold04 rmse    standard       2.38  Preprocessor1_Model1\n 8 Fold04 rsq     standard       0.249 Preprocessor1_Model1\n 9 Fold05 rmse    standard       2.34  Preprocessor1_Model1\n10 Fold05 rsq     standard       0.250 Preprocessor1_Model1\n11 Fold06 rmse    standard       2.32  Preprocessor1_Model1\n12 Fold06 rsq     standard       0.261 Preprocessor1_Model1\n13 Fold07 rmse    standard       2.36  Preprocessor1_Model1\n14 Fold07 rsq     standard       0.215 Preprocessor1_Model1\n15 Fold08 rmse    standard       2.35  Preprocessor1_Model1\n16 Fold08 rsq     standard       0.214 Preprocessor1_Model1\n17 Fold09 rmse    standard       2.33  Preprocessor1_Model1\n18 Fold09 rsq     standard       0.257 Preprocessor1_Model1\n19 Fold10 rmse    standard       2.35  Preprocessor1_Model1\n20 Fold10 rsq     standard       0.226 Preprocessor1_Model1\n\n\n\n\n\n\n\n\nid\nrmse\nrsq\n\n\n\n\nFold01\n2.286402\n0.2780252\n\n\nFold02\n2.352520\n0.2378150\n\n\nFold03\n2.334960\n0.2579689\n\n\nFold04\n2.376705\n0.2491427\n\n\nFold05\n2.338995\n0.2503009\n\n\nFold06\n2.315747\n0.2607719\n\n\nFold07\n2.362725\n0.2147322\n\n\nFold08\n2.347958\n0.2137093\n\n\nFold09\n2.331361\n0.2568661\n\n\nFold10\n2.345771\n0.2258870"
  },
  {
    "objectID": "W10.html#cross-validation-for-logistic-regression",
    "href": "W10.html#cross-validation-for-logistic-regression",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation for logistic regression",
    "text": "Cross validation for logistic regression\nTake 2 simple models predicting the sex of penguins.\n\nlibrary(palmerpenguins)\npenguins <- na.omit(penguins)\nset.seed(9999)\npeng_split <- initial_split(penguins, prop = 0.8)\npeng_train <- training(peng_split)\npeng_test <- testing(peng_split)\npeng_folds <- vfold_cv(peng_train, v = 5)\n\npeng_rec1 <- peng_train |> \n recipe(sex ~ flipper_length_mm + body_mass_g, family = \"binomial\")\npeng_rec2 <- peng_train |> \n recipe(sex ~ bill_depth_mm + bill_length_mm, family = \"binomial\")  \npeng_mod <- logistic_reg() |> set_engine(\"glm\")\npeng_wflow1 <- workflow() |> add_model(peng_mod) |> add_recipe(peng_rec1)\npeng_wflow2 <- workflow() |> add_model(peng_mod) |> add_recipe(peng_rec2)\n\npeng_fit1 <- peng_wflow1 |> fit(peng_train)\npeng_fit2 <- peng_wflow2 |> fit(peng_train)\n\n\n\n\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        8.98     2.95          3.05 2.31e- 3\n2 flipper_length_mm -0.102    0.0219       -4.66 3.23e- 6\n3 body_mass_g        0.00276  0.000424      6.52 7.23e-11\n\n\n\n\n\n# A tibble: 3 × 5\n  term           estimate std.error statistic  p.value\n  <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     -24.7      3.25       -7.61 2.65e-14\n2 bill_depth_mm     0.789    0.109       7.23 4.79e-13\n3 bill_length_mm    0.258    0.0400      6.46 1.05e-10"
  },
  {
    "objectID": "W10.html#cross-validation-for-logistic-regression-1",
    "href": "W10.html#cross-validation-for-logistic-regression-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation for logistic regression",
    "text": "Cross validation for logistic regression\n\npeng_fit1_rs <- peng_wflow1 |> fit_resamples(peng_folds)\npeng_fit2_rs <- peng_wflow2 |> fit_resamples(peng_folds)\npeng_fit1_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.666     5  0.0281 Preprocessor1_Model1\n2 roc_auc  binary     0.769     5  0.0262 Preprocessor1_Model1\n\npeng_fit2_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.782     5  0.0239 Preprocessor1_Model1\n2 roc_auc  binary     0.859     5  0.0309 Preprocessor1_Model1\n\n\n\nFor the logistic regression fit_resamples has two performance measures per default: AUC (area under the ROC-curve) and accuracy.\nThe model using the two variables about penguin bills performs better than the model using body mass and flipper length."
  },
  {
    "objectID": "W10.html#accuracy-for-classifiers",
    "href": "W10.html#accuracy-for-classifiers",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Accuracy for classifiers",
    "text": "Accuracy for classifiers\n\n\nAccuracy is the fraction of correct predictions:\n(TP + TN) / (TP + FP + FN + TN)\n\nAccuracy is a good overall performance measure but it does not specify if errors are false positives or false negatives.\n\nFor logistic regression:\n\n\n\n\n\nAccuracy relies a decision threshold (default 50%). It has an easy interpretation.\nAUC is the area under the ROC-curve sweeping over all possible thresholds.\n\nRecall:\nSensitivity is the true positive rate: TP / (TP + FN)\nSpecificity is the true negative rate: TN / (TN + FP)"
  },
  {
    "objectID": "W10.html#comparison-of-metrics",
    "href": "W10.html#comparison-of-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Comparison of metrics",
    "text": "Comparison of metrics\n\n\nFlipper length, body mass\n\npeng_fit1_rs |> collect_metrics(summarize = FALSE) |> select(-.config) |> arrange(.metric)\n\n# A tibble: 10 × 4\n   id    .metric  .estimator .estimate\n   <chr> <chr>    <chr>          <dbl>\n 1 Fold1 accuracy binary         0.611\n 2 Fold2 accuracy binary         0.698\n 3 Fold3 accuracy binary         0.660\n 4 Fold4 accuracy binary         0.755\n 5 Fold5 accuracy binary         0.604\n 6 Fold1 roc_auc  binary         0.717\n 7 Fold2 roc_auc  binary         0.791\n 8 Fold3 roc_auc  binary         0.793\n 9 Fold4 roc_auc  binary         0.843\n10 Fold5 roc_auc  binary         0.702\n\npeng_test_pred1 <- predict(peng_fit1, new_data = peng_test, type = \"prob\") |> \n mutate(.pred_class_0.5 = if_else(.pred_female > 0.5, \"female\", \"male\") |> factor(),\n        .pred_class_0.45 = if_else(.pred_female > 0.45, \"female\", \"male\") |> factor(),\n        .pred_class_0.55 = if_else(.pred_female > 0.55, \"female\", \"male\") |> factor()\n        ) |> \n bind_cols(peng_test |> select(sex, everything())) \npeng_test_pred1 |> roc_auc(truth = sex, .pred_female)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.806\n\n\n\nBill length and depth\n\npeng_fit2_rs |> collect_metrics(summarize = FALSE) |> select(-.config) |> arrange(.metric)\n\n# A tibble: 10 × 4\n   id    .metric  .estimator .estimate\n   <chr> <chr>    <chr>          <dbl>\n 1 Fold1 accuracy binary         0.741\n 2 Fold2 accuracy binary         0.849\n 3 Fold3 accuracy binary         0.755\n 4 Fold4 accuracy binary         0.736\n 5 Fold5 accuracy binary         0.830\n 6 Fold1 roc_auc  binary         0.811\n 7 Fold2 roc_auc  binary         0.972\n 8 Fold3 roc_auc  binary         0.796\n 9 Fold4 roc_auc  binary         0.861\n10 Fold5 roc_auc  binary         0.855\n\npeng_test_pred2 <- predict(peng_fit2, new_data = peng_test, type = \"prob\") |> \n mutate(.pred_class_0.5 = if_else(.pred_female > 0.5, \"female\", \"male\") |> factor(),\n        .pred_class_0.45 = if_else(.pred_female > 0.45, \"female\", \"male\") |> factor(),\n        .pred_class_0.55 = if_else(.pred_female > 0.55, \"female\", \"male\") |> factor()\n        ) |> \n bind_cols(peng_test |> select(sex, everything())) \npeng_test_pred2 |> roc_auc(truth = sex, .pred_female)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.850"
  },
  {
    "objectID": "W10.html#comparison-of-metrics-1",
    "href": "W10.html#comparison-of-metrics-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Comparison of metrics",
    "text": "Comparison of metrics\n\n\nFlipper length, body mass\n\npeng_test_pred1 |> \n accuracy(truth = sex, .pred_class_0.5)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.731\n\npeng_test_pred1 |> \n accuracy(truth = sex, .pred_class_0.55)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.687\n\npeng_test_pred1 |> \n accuracy(truth = sex, .pred_class_0.45)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.791\n\n\n\nBill length and depth\n\npeng_test_pred2 |> \n accuracy(truth = sex, .pred_class_0.5)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.776\n\npeng_test_pred2 |> \n accuracy(truth = sex, .pred_class_0.55)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.776\n\npeng_test_pred2 |> \n accuracy(truth = sex, .pred_class_0.45)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.776"
  },
  {
    "objectID": "W10.html#overview-of-predictions",
    "href": "W10.html#overview-of-predictions",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Overview of predictions",
    "text": "Overview of predictions\nBill length and depth\n\npeng_test_pred2 |> select(1:6)\n\n# A tibble: 67 × 6\n   .pred_female .pred_male .pred_class_0.5 .pred_class_0.45 .pred_class_0.55\n          <dbl>      <dbl> <fct>           <fct>            <fct>           \n 1        0.655      0.345 female          female           female          \n 2        0.298      0.702 male            male             male            \n 3        0.629      0.371 female          female           female          \n 4        0.577      0.423 female          female           female          \n 5        0.751      0.249 female          female           female          \n 6        0.420      0.580 male            male             male            \n 7        0.863      0.137 female          female           female          \n 8        0.294      0.706 male            male             male            \n 9        0.833      0.167 female          female           female          \n10        0.118      0.882 male            male             male            \n# ℹ 57 more rows\n# ℹ 1 more variable: sex <fct>"
  },
  {
    "objectID": "W10.html#galtons-data",
    "href": "W10.html#galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Galton’s data",
    "text": "Galton’s data\nWhat is the weight of the meat of this ox?\n\nlibrary(readxl)\ngalton <- read_excel(\"data/galton_data.xlsx\")\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\")\n\n\n\n\n787 estimates, true value 1198, mean 1196.7\n\n\nWe focus on the arithmetic mean as aggregation function for the wisdom of the crowd here."
  },
  {
    "objectID": "W10.html#rmse-galtons-data",
    "href": "W10.html#rmse-galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "RMSE Galton’s data",
    "text": "RMSE Galton’s data\nDescribe the estimation game as a predictive model:\n\nAll estimates are made to predict the same value: the truth.\n\nIn contrast to the regression model, the estimate come from people and not from a regression formula.\n\nThe truth is the same for all.\n\nIn contrast to the regression model, the truth is one value and not a value for each prediction\n\n\n\nrmse_galton <- galton |> \n mutate(true_value = 1198) |>\n rmse(truth = true_value, Estimate)\nrmse_galton\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        73.6"
  },
  {
    "objectID": "W10.html#mse-variance-and-bias-of-estimates",
    "href": "W10.html#mse-variance-and-bias-of-estimates",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "MSE, Variance, and Bias of estimates",
    "text": "MSE, Variance, and Bias of estimates\nIn a crowd estimation, \\(n\\) estimators delivered the estimates \\(\\hat{y}_1,\\dots,\\hat{y}_n\\). Let us look at the following measures\n\n\\(\\bar{y} = \\frac{1}{n}\\sum_{i = 1}^n \\hat{y}_i^2\\) is the mean estimate, it is the aggregated estimate of the crowd\n\\(\\text{MSE} = \\text{RMSE}^2 = \\frac{1}{n}\\sum_{i = 1}^n (\\text{truth} - \\hat{y}_i)^2\\)\n\\(\\text{Variance} = \\frac{1}{n}\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2\\)\n\\(\\text{Bias-squared} = (\\bar{y} - \\text{truth})^2\\) which is the square difference between truth and mean estimate.\n\nThere is a mathematical relation (a math exercise to check):\n\\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\]"
  },
  {
    "objectID": "W10.html#testing-for-galtons-data",
    "href": "W10.html#testing-for-galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Testing for Galton’s data",
    "text": "Testing for Galton’s data\n\\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\]\n\nMSE <- (rmse_galton$.estimate)^2 \nMSE\n\n[1] 5409.795\n\nVariance <- var(galton$Estimate)*(nrow(galton)-1)/nrow(galton)\n# Note, we had to correct for the divisor (n-1) in the classical statistical definition\n# to get the sample variance instead of the estimate for the population variance\nVariance\n\n[1] 5408.132\n\nBias_squared <- (mean(galton$Estimate) - 1198)^2\nBias_squared\n\n[1] 1.663346\n\nBias_squared + Variance\n\n[1] 5409.795\n\n\n\n\nSuch nice mathematical properties are probably one reason why these squared measures are so popular."
  },
  {
    "objectID": "W10.html#the-diversity-prediction-theorem",
    "href": "W10.html#the-diversity-prediction-theorem",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "The diversity prediction theorem1",
    "text": "The diversity prediction theorem1\n\nMSE is a measure the average individuals error\nBias-squared is a measure the collective error\nVariance is a measure for the diversity of estimates around the mean\n\nThe mathematical relation \\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\] can be formulated as\nCollective error = Individual error - Diversity\nInterpretation: The higher the diversity the lower the collective error!\nNotion from: Page, S. E. (2007). The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton University Press."
  },
  {
    "objectID": "W10.html#why-is-this-message-a-bit-suggestive",
    "href": "W10.html#why-is-this-message-a-bit-suggestive",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Why is this message a bit suggestive?",
    "text": "Why is this message a bit suggestive?\nThe mathematical relation \\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\] can be formulated as\nCollective error = Individual error - Diversity\nInterpretation: The higher the diversity the lower the collective error!\n\n\n\\(\\text{MSE}\\) and \\(\\text{Variance}\\) are not independent!\nActivities to increase diversity (Variance) typically also increase the average individual error (MSE).\nFor example, if we just add more random estimates with same mean but wild variance to our sample we increase both and do not gain any decrease of the collective error."
  },
  {
    "objectID": "W10.html#accuracy-for-numerical-estimate",
    "href": "W10.html#accuracy-for-numerical-estimate",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Accuracy for numerical estimate",
    "text": "Accuracy for numerical estimate\n\nFor binary classifiers accuracy has a simple definition: Fraction of correct classifications.\n\nIt can be further informed by other more specific measures taken from the confusion matrix (sensitivity, specificity)\n\n\nHow about numerical estimators?\nFor example outcomes of estimation games, or linear regression models.\n\nAccuracy is for example measured by (R)MSE\n\\(\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\) shows us that we can make a\nbias-variance decomposition\nThat means some part of the error is a systematic (the bias) and another part due to random variation (the variance).\nLearn more about the bias-variance tradeoff in statistical learning independently! It is an important concept to understand predictive models."
  },
  {
    "objectID": "W10.html#d-accuracy-trueness-and-precision",
    "href": "W10.html#d-accuracy-trueness-and-precision",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "2-d Accuracy: Trueness and Precision",
    "text": "2-d Accuracy: Trueness and Precision\nAccording to ISO 5725-1 Standard: Accuracy (trueness and precision) of measurement methods and results - Part 1: General principles and definitions. there are two dimension of accuracy of numerical measurement."
  },
  {
    "objectID": "W10.html#what-is-a-wise-crowd",
    "href": "W10.html#what-is-a-wise-crowd",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "What is a wise crowd?",
    "text": "What is a wise crowd?\nAssume the dots are estimates. Which is a wise crowd?\n\n\n\nOf course, high trueness and high precision! But, …\nFocusing on the crowd being wise instead of its individuals: High trueness, low precision.\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W04.html#definition-sets-and-vectors",
    "href": "W04.html#definition-sets-and-vectors",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Definition: Sets and vectors",
    "text": "Definition: Sets and vectors\nA set is mathematical model for the collection of different things.\nExamples\n\n\\(\\{3, \\text{Hi}, 😀, 🖖 \\}\\)\n\\(\\{1,3,5\\}\\)\nThe natural numbers \\(\\mathbb{N} = \\{1, 2, 3, \\dots\\}\\) (infinite!)\n\\(\\{\\mathtt{\"EWR\"}, \\mathtt{\"LGA\"}, \\mathtt{\"JFK\"}\\}\\)\nthese are origin airports in flights"
  },
  {
    "objectID": "W04.html#math-sets-and-vectors-1",
    "href": "W04.html#math-sets-and-vectors-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Math: Sets and vectors",
    "text": "Math: Sets and vectors\nA vector is an ordered collection of things (elements) of the same type.\nIn a set each thing can only be once and the order does not matter!\n\\(\\{1,3,5\\} = \\{3,5,1\\} = \\{1,1,1,3,5,5\\}\\)\nFor vectors:\n\\([1\\ 3\\ 5] \\neq [3\\ 5\\ 1]\\) because we compare component-wise, so we cannot even compare with those with the vector \\([1\\ 1\\ 1\\ 3\\ 5\\ 5]\\)"
  },
  {
    "objectID": "W04.html#math-set-operations",
    "href": "W04.html#math-set-operations",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Math: Set operations",
    "text": "Math: Set operations\nSets \\(A = \\{🐺, 🦊, 🐶\\}\\) and \\(B = \\{🐶, 🐷, 🐹\\}\\), \\(C = \\{🐶, 🐷\\}\\):\n\n\nSet union \\(A \\cup B\\) = {🐺, 🦊, 🐶, 🐷, 🐹}\n\\(x \\in A \\cup B\\) when \\(x \\in A\\) | (or) \\(x\\in B\\)\nSet intersection \\(A \\cap B\\) = {🐶}\n\\(x \\in A \\cap B\\) when \\(x \\in A\\) & (and) \\(x\\in B\\)\nSet difference \\(A \\setminus B = \\{🐺, 🦊\\}\\), \\(B \\setminus A\\) = {🐷, 🐹}\nSubset: \\(C \\subset B\\) but \\(C \\not\\subset A\\)\n\n\n\nSee the analogy of set operations and logical operations."
  },
  {
    "objectID": "W04.html#set-operations-in-r",
    "href": "W04.html#set-operations-in-r",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Set operations in R",
    "text": "Set operations in R\nunique shows the set of elements in a vector\n\nunique(flights$origin)\n\n[1] \"EWR\" \"LGA\" \"JFK\"\n\n\n\nsetequal tests for set equality\n\nsetequal(c(\"EWR\",\"LGA\",\"JFK\"), c(\"EWR\",\"EWR\",\"LGA\",\"JFK\"))\n\n[1] TRUE\n\n\n\n\nunion, intersect, setdiff treat vectors as sets and operate as expected\n\nunion(1:5,3:7)\n\n[1] 1 2 3 4 5 6 7\n\nintersect(1:5,3:7)\n\n[1] 3 4 5\n\nsetdiff(1:5,3:7)\n\n[1] 1 2"
  },
  {
    "objectID": "W04.html#sets-take-away",
    "href": "W04.html#sets-take-away",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Sets: Take-away",
    "text": "Sets: Take-away\n\nSet operations are not a daily business in data science\nHowever, they are useful for data exploration!\nKnowing set operations is key to understand probability:\n\nA sample space is the set of all atomic events.\nAn event is a subset of the sample\nA probability function assigns probabilities to all events."
  },
  {
    "objectID": "W04.html#functions-mathematically",
    "href": "W04.html#functions-mathematically",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Functions mathematically",
    "text": "Functions mathematically\nConsider two sets: The domain \\(X\\) and the codomain \\(Y\\).\nA function \\(f\\) assigns each element of \\(X\\) to exactly one element of \\(Y\\).\n\n\nWe write \\(f : X \\to Y\\)\n“\\(f\\) maps from \\(X\\) to \\(Y\\)”\nand \\(x \\mapsto f(x)\\)\n“\\(x\\) maps to \\(f(x)\\)”\nThe yellow set is called the image of \\(f\\).\n\n\n\n\n\n\nPicture from wikipedia."
  },
  {
    "objectID": "W04.html#conventions-in-mathematical-text",
    "href": "W04.html#conventions-in-mathematical-text",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Conventions in mathematical text",
    "text": "Conventions in mathematical text\n\nSets are denoted with capital letters.\nTheir elements with (corresponding) small letters.\nFunctions are often called \\(f\\), \\(g\\), or \\(h\\).\nOther terminology can be used!\n\n\nImportant in math\n\nWhen you read math:\nKeep track of what objects are! What are functions, what are sets, what are numbers, …1\nWhen you write math: Define what objects are.\n\n\nWatch: How to read math https://www.youtube.com/watch?v=Kp2bYWRQylk"
  },
  {
    "objectID": "W04.html#is-this-a-mathematical-function",
    "href": "W04.html#is-this-a-mathematical-function",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Is this a mathematical function?",
    "text": "Is this a mathematical function?\n \\(\\ \\mapsto\\ \\) \nInput from \\(X = \\{\\text{A picture where a face can be recognized}\\}\\).\nFunction: Upload input at https://funny.pho.to/lion/ and download output.\nOutput from \\(Y = \\{\\text{Set of pictures with a specific format.}\\}\\)\n\nYes, it is a function. Important: Output is the same for the same input!"
  },
  {
    "objectID": "W04.html#is-this-a-mathematical-function-1",
    "href": "W04.html#is-this-a-mathematical-function-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Is this a mathematical function?",
    "text": "Is this a mathematical function?\nInput a text snippet. Function: Enter text at https://www.craiyon.com. Output a picture.\n\n\n\n\nOther examples:\n\n“Nuclear explosion broccoli”\n“The Eye of Sauron reading a newspaper”\n“The legendary attack of Hamster Godzilla wearing a tiny Sombrero”\n\n  \n\n\n\nNo, it is not a function. It has nine outcomes and these change when run again."
  },
  {
    "objectID": "W04.html#graphs-of-functions",
    "href": "W04.html#graphs-of-functions",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Graphs of functions",
    "text": "Graphs of functions\n\nA function is characterized by the set all possible pairs \\((x,f(x))\\).\nThis is called its graph.\nWhen domain and codomain are real numbers then the graph can be shown in a Cartesian coordinate system. Example \\(f(x) = x^3 - x^2\\)"
  },
  {
    "objectID": "W04.html#some-functions-f-mathbbr-to-mathbbr",
    "href": "W04.html#some-functions-f-mathbbr-to-mathbbr",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Some functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)",
    "text": "Some functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)\n\n\n\\(f(x) = x\\) identity function\n\\(f(x) = x^2\\) square function\n\\(f(x) = \\sqrt{x}\\) square root function\n\\(f(x) = e^x\\) exponential function\n\\(f(x) = \\log(x)\\) natural logarithm\n\nSquare and square root function are inverse of each other. Exponential and natural logarithm, too.\n\n\\(\\sqrt[2]{x}^2 = \\sqrt[2]{x^2} = x\\), \\(\\log(e^x) = e^{\\log(x)} = x\\)\n\nIdentity function graph as mirror axis.\n\n\n\n\n\n\n\n\n\n\n\n\\(e\\) is Euler’s number \\(2.71828\\dots\\). The natural logarithm is also often called \\(\\ln\\). The square root function is \\(\\mathbb{R}_{\\geq 0} \\to \\mathbb{R}\\), the logarithm \\(\\mathbb{R}_{>0} \\to \\mathbb{R}\\)."
  },
  {
    "objectID": "W04.html#shifts-and-scales",
    "href": "W04.html#shifts-and-scales",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Shifts and scales",
    "text": "Shifts and scales\nHow can we shift, stretch, or shrink a graph vertically and horizontally?\n\n\n\\(y\\)-shift\\(x\\)-shift\\(y\\)-scale\\(x\\)-scale\n\n\n\n\nAdd a constant to the function.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = x^3 - x^2 + a\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\n\n\n\n\n\n\n\n\n\n\n\n\nSubtract a constant from all \\(x\\) within the function definition.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = (x - a)^3 - (x - a)^2\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nAttention:\nShifting \\(a\\) units to the right needs subtracting \\(a\\)!\nYou can think of the coordinate system being shifted in direction \\(a\\) while the graph stays.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiply a constant to all \\(x\\) within the function definition.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = a(x^3 - x^2)\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nNegative numbers flip the graph around the \\(x\\)-axis.\n\n\n\n\n\n\n\n\n\n\n\n\nDivide all \\(x\\) within the function definition by a constant.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = (x/a)^3 - (x/a)^2\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nNegative numbers flip the graph around the \\(y\\)-axis.\nAttention: Stretching needs a division by \\(a\\)!\nYou can think of the coordinate system being stretched multiplicatively by \\(a\\) while the graph stays."
  },
  {
    "objectID": "W04.html#math-polynomials-and-exponentials",
    "href": "W04.html#math-polynomials-and-exponentials",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Math: Polynomials and exponentials",
    "text": "Math: Polynomials and exponentials\nA polynomial is a function which is composed of (many) addends of the form \\(ax^n\\) for different values of \\(a\\) and \\(n\\).\nIn an exponential the \\(x\\) appears in the exponent.\n\\(f(x) = x^3\\) vs. \\(f(x) = e^x\\)\n\nFor \\(x\\to\\infty\\), any exponential will finally “overtake” any polynomial."
  },
  {
    "objectID": "W04.html#rules-for-exponentiation",
    "href": "W04.html#rules-for-exponentiation",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Rules for exponentiation",
    "text": "Rules for exponentiation\n\n\n\\(x^0\\)\n\\(0^x\\)\n\\(0^0\\)\n\\((x\\cdot y)^a\\)\n\\(x^{-a}\\), \\(x^{-1}\\)\n\\(x^\\frac{a}{b}\\), \\(x^\\frac{1}{2}\\)\n\\((x^a)^b\\)\n\n\n\\(x^0 = 1\\)\n\n\n\\(0^x = 0\\) for \\(x\\neq 0\\)\n\n\n\\(0^0 = 1\\) (discontinuity in \\(0^x\\))\n\n\n\\((x\\cdot y)^a = x^a\\cdot x^b\\)\n\n\n\\(x^{-a} = \\frac{1}{x^a}\\), \\(x^{-1} = \\frac{1}{x}\\)\n\n\n\\(x^\\frac{a}{b} = \\sqrt[b]{x^a} = (\\sqrt[b]{x})^a,\\ x^\\frac{1}{2} = \\sqrt{x}\\)\n\n\n\\((x^a)^b = x^{a\\cdot b} = (x^b)^a \\neq x^{a^b} = x^{(a^b)}\\)\nExample: \\((4^3)^2 = 64^2 = 4096 \\qquad 4^{3^2} = 4^9 = 262144\\)"
  },
  {
    "objectID": "W04.html#more-rules-for-exponentiation",
    "href": "W04.html#more-rules-for-exponentiation",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "More rules for exponentiation",
    "text": "More rules for exponentiation\n\n\n\\(x^a\\cdot x^b\\)\n\n\n\\(x^a\\cdot x^b = x^{a+b}\\) Multiplication of powers (with same base \\(x\\)) becomes addition of exponents.\n\n\n\n\n\n\n\\((x+y)^a\\)\n\n\nNo “simple” form! For \\(a\\) integer use binomial expansion. \\((x+y)^2 = x^2 + 2xy + y^2\\)\n\\((x+y)^3 = x^3 + 3x^2y + 3xy^2 + y^3\\)\n\\((x+y)^n = \\sum_{k=0}^n {n \\choose k} x^{n-k}y^k\\)\n\n\n\n\n\nPascal’s triangle\n\n\n\n\n\nFrom wikipedia\n\n\n\nWe meet it again in Probability:\nA row represents a binomial distribution\nWhich tends to mimics the normal distribution more and more\nand is related to the central limit theorem"
  },
  {
    "objectID": "W04.html#logarithms",
    "href": "W04.html#logarithms",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Logarithms",
    "text": "Logarithms\nDefinition: A logarithm of \\(a\\) for some base \\(b\\) is the value of the exponent which brings \\(b\\) to \\(a\\): \\(\\log_b(a) = x\\) means that \\(b^x =a\\)\nMost common:\n\n\\(\\log_{10}\\) for logarithmic axes in plots\n\\(\\log_{e}\\) natural logarithm (also \\(\\log\\) or \\(\\ln\\))\n\n\n\n\n\\(\\log_{10}(100) =\\)\n\n\n\\(2\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(1) =\\)\n\n\n\\(0\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(6590) =\\)\n\n\n\\(3.818885\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(0.02) =\\)\n\n\n\\(-1.69897\\)"
  },
  {
    "objectID": "W04.html#rules-for-logarithms",
    "href": "W04.html#rules-for-logarithms",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Rules for logarithms",
    "text": "Rules for logarithms\nUsually only one base is used in the same context, because changing base is easy:\n\\(\\log_c(x) = \\frac{\\log_b(x)}{\\log_b(c)} = \\frac{\\log(x)}{\\log(c)}\\)\n\n\n\\(\\log(x\\cdot y)\\)\n\n\n\\(= \\log(x) + \\log(y)\\) Multiplication \\(\\to\\) addition.\n\n\n\n\n\n\n\\(\\log(x^y)\\)\n\n\n\\(= y\\cdot\\log(x)\\)\n\n\n\n\n\n\\(\\log(x+y)\\)\n\n\ncomplicated!\n\n\n\n\n\nAlso changing bases for powers is easy: \\(x^y = (e^{\\log(x)})^y = e^{y\\cdot\\log(x)}\\)"
  },
  {
    "objectID": "W04.html#input-to-output",
    "href": "W04.html#input-to-output",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Input \\(\\to\\) output",
    "text": "Input \\(\\to\\) output\n\n\nMetaphorically, a function is a machine or a blackbox that for each input yields an output.\nThe inputs of a function are also called arguments.\n\n\nDifference to math terminolgy:\nThe output need not be the same for the same input.\n\n\n\nPicture from wikipedia."
  },
  {
    "objectID": "W04.html#function-as-objects-in-r",
    "href": "W04.html#function-as-objects-in-r",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Function as objects in R",
    "text": "Function as objects in R\nfunction is a class of an object in R\n\nclass(c)\n\n[1] \"function\"\n\nclass(ggplot2::ggplot)\n\n[1] \"function\"\n\n\nCalling the function without brackets writes its code or some information.\n\nsd # This function is written in R, and we see its code\n\nfunction (x, na.rm = FALSE) \nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n<bytecode: 0x56366fe472d0>\n<environment: namespace:stats>\n\nc # This function is not written in R but is a R primitive\n\nfunction (...)  .Primitive(\"c\")\n\nggplot2::ggplot # This function is not written solely in R\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \n{\n    UseMethod(\"ggplot\")\n}\n<bytecode: 0x56366e396c50>\n<environment: namespace:ggplot2>"
  },
  {
    "objectID": "W04.html#define-your-own-functions-in-r",
    "href": "W04.html#define-your-own-functions-in-r",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Define your own functions! (in R)",
    "text": "Define your own functions! (in R)\n\nadd_one <- function(x) {\n  x + 1 \n}\n# Test it\nadd_one(10)\n\n[1] 11\n\n\nThe skeleton for a function definition is\nfunction_name <- function(input){\n  # do something with the input(s)\n  # return something as output\n}\n\nfunction_name should be a short but evocative verb.\nThe input can be empty or one or more name or name=expression terms as arguments.\nThe last evaluated expression is returned as output.\nWhen the body or the function is only one line {} can be omitted. For example\nadd_one <- function(x) x + 1"
  },
  {
    "objectID": "W04.html#flexibility-of-inputs-and-outputs",
    "href": "W04.html#flexibility-of-inputs-and-outputs",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Flexibility of inputs and outputs",
    "text": "Flexibility of inputs and outputs\n\nArguments can be specified by name=expression or just expression (then they are taken as the next argument)\nDefault values for arguments can be provided. Useful when an argument is a parameter.\n\n\n\nmymult <- function(x = 2, y = 3) x * (y - 1)\nmymult(3,4)\n\n\n[1] 9\n\n\n\n\n\nmymult()\n\n\n[1] 4\n\n\n\n\n\nmymult(y = 3, x = 6)\n\n\n[1] 12\n\n\n\n\n\nmymult(5)\n\n\n[1] 10\n\n\n\n\n\nmymult(y = 2)\n\n\n[1] 2\n\n\n\n\nFor complex output use a list\n\n\nmymult <- function(x = 2, y = 3) \n  list(out1 = x * (y - 1), out2 = x * (y - 2))\nmymult()\n\n\n$out1\n[1] 4\n\n$out2\n[1] 2"
  },
  {
    "objectID": "W04.html#vectorized-functions",
    "href": "W04.html#vectorized-functions",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Vectorized functions",
    "text": "Vectorized functions\nMathematical functions in programming are often “vectorized”:\n\nOperations on a single value are applied to each component of the vector.\nOperations on two values are applied “component-wise” (for vectors of the same length)\n\n\nlog10(c(1,10,100,1000,10000))\n\n[1] 0 1 2 3 4\n\nc(1,1,2) + c(3,1,0)\n\n[1] 4 2 2\n\n(0:5)^2\n\n[1]  0  1  4  9 16 25"
  },
  {
    "objectID": "W04.html#recall-vector-creation-functions",
    "href": "W04.html#recall-vector-creation-functions",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Recall: Vector creation functions",
    "text": "Recall: Vector creation functions\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from=-0.5, to=1.5, by=0.1)\n\n [1] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9\n[16]  1.0  1.1  1.2  1.3  1.4  1.5\n\nseq(from=0, to=1, length.out=10)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000\n\nrep(1:3, times=3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\nrep(1:3, each=3)\n\n[1] 1 1 1 2 2 2 3 3 3"
  },
  {
    "objectID": "W04.html#plotting-and-transformation",
    "href": "W04.html#plotting-and-transformation",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Plotting and transformation",
    "text": "Plotting and transformation\nVector creation and vectorized functions are key for plotting and transformation.\n\nfunc <- function(x) x^3 - x^2    # Create a vectorized function\ndata <- tibble(x = seq(-0.5,1.5,by =0.01)) |>    # Vector creation\n    mutate(y = func(x))        # Vectorized transformation using the function\ndata |> ggplot(aes(x,y)) + geom_line() + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W04.html#conveniently-ggploting-functions",
    "href": "W04.html#conveniently-ggploting-functions",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Conveniently ggploting functions",
    "text": "Conveniently ggploting functions\n\nggplot() +\n geom_function(fun = log) +\n geom_function(fun = function(x) 3*x - 4, color = \"red\") +\n theme_minimal(base_size = 20)\n\n\n\n\n\n\nLine 3 shows another important concept: anonymous functions. The function function(x) 3*x - 4 is defined on the fly without a name."
  },
  {
    "objectID": "W04.html#conditional-statements",
    "href": "W04.html#conditional-statements",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Conditional statements",
    "text": "Conditional statements\n\nif executes a code block if a condition is TRUE\nelse executes a code block if the condition is FALSE\n\nSkeleton\nif (condition) {\n  # code block\n} else {\n  # code block\n}\nExample: A piece-wise defined function\n\n\n\npiecewise <- function(x) {\n  if (x < 2) {\n    0.5 * x\n  } else {\n    x - 1\n  }\n}\n\n\n\npiecewise(1)\n\n[1] 0.5\n\npiecewise(2)\n\n[1] 1\n\npiecewise(3)\n\n[1] 2\n\n\n\n\n\nProblem: piecewise is not vectorized. piecewise(c(1,2,3)) does not work."
  },
  {
    "objectID": "W04.html#vectorized-operations-with-map",
    "href": "W04.html#vectorized-operations-with-map",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Vectorized operations with map",
    "text": "Vectorized operations with map\n\nmap functions apply a function to each element of a vector.1\nmap(.x, .f, ...) applies the function .f to each element of the vector of .x and returns a list.\nmap_dbl returns a double vector (other variants exist)\n\n\n\n\nmap(c(1,2,3), piecewise) \n\n[[1]]\n[1] 0.5\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 2\n\nmap_dbl(c(1,2,3), piecewise) \n\n[1] 0.5 1.0 2.0\n\npiecewise_vectorized <- \n function(x) map_dbl(x, piecewise) \n\n\n\npiecewise_vectorized(seq(0,3,by = 0.5))\n\n[1] 0.00 0.25 0.50 0.75 1.00 1.50 2.00\n\ntibble(x = seq(0,3,by = 0.5)) |> \n  mutate(y = piecewise_vectorized(x)) |> \n  ggplot(aes(x,y)) + geom_line() + theme_minimal(base_size = 20)\n\n\n\n\n\n\nIn tidyverse they are provided in the package purrr"
  },
  {
    "objectID": "W04.html#map-and-reduce",
    "href": "W04.html#map-and-reduce",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "map and reduce",
    "text": "map and reduce\nInstead of a list or a vector reduce returns a single value.\nTo that end it needs a function with two arguments. It applies it to the first two elements of the vector, then to the result and the third element, then the result and the fourth element, and so on.\n\n1:10 |> reduce(\\(x,y) x + y)\n\n[1] 55\n\n\nNote: \\(x) is a short way to write an anonymous function as function(x).\n\nExample: Reading multiple files\n\n\nInstead of\na <-read_csv(\"a.csv\")\nb <-read_csv(\"b.csv\")\nc <-read_csv(\"c.csv\")\nd <-read_csv(\"d.csv\")\ne <-read_csv(\"e.csv\")\nf <-read_csv(\"f.csv\")\ng <-read_csv(\"g.csv\")\n\nbind_rows(a,b,c,d,e,f,g)\n\nWrite\nletter[1:7] |> \n map(\\(x) read_csv(paste0(x,\".csv\"))) |> \n reduce(bind_rows)"
  },
  {
    "objectID": "W04.html#function-programming-take-away",
    "href": "W04.html#function-programming-take-away",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Function programming: Take away",
    "text": "Function programming: Take away\n\nFunctions are the most important building blocks of programming.\nFunctions can and often should be vectorized.\nVectorized functions are the basis for plotting and transformation.\nmap functions are powerful tools for iterative tasks!\nExpect to not get the idea first but to love them later."
  },
  {
    "objectID": "W04.html#descriptive-vs.-inferential-statistics",
    "href": "W04.html#descriptive-vs.-inferential-statistics",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Descriptive vs. Inferential Statistics",
    "text": "Descriptive vs. Inferential Statistics\n\nThe process of using and analyzing summary statistics\n\nSolely concerned with properties of the observed data.\n\nDistinct from inferential statistics:\n\nInference of properties of an underlying distribution given sampled observations from a larger population.\n\n\nSummary Statistics are used to summarize a set of observations to communicate the largest amount of information as simple as possible."
  },
  {
    "objectID": "W04.html#summary-statistics",
    "href": "W04.html#summary-statistics",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Summary statistics",
    "text": "Summary statistics\nUnivariate (for one variable)\n\nMeasures of location, or central tendency\nMeasures of statistical dispersion\nMeasure of the shape of the distribution like skewness or kurtosis\n\nBivariate (for two variables)\n\nMeasures of statistical dependence or correlation"
  },
  {
    "objectID": "W04.html#measures-of-central-tendency-1",
    "href": "W04.html#measures-of-central-tendency-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nGoal: For a sequence of numerical observations \\(x_1,\\dots,x_n\\) we want to measure\n\nthe “typical” value.\na value summarizing the location of values on the numerical axis.\n\nThree different ways:\n\nArithmetic mean (also mean, average): Sum of the all observations divided by the number of observations \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\)\nMedian: Assume \\(x_1 \\leq x_2 \\leq\\dots\\leq x_n\\). Then the median is middlemost values in the sequence \\(x_\\frac{n+1}{2}\\) when \\(n\\) odd. For \\(n\\) even there are two middlemost values and the median is \\(\\frac{x_\\frac{n}{2} + x_\\frac{n+1}{2}}{2}\\)\nMode: The value that appears most often in the sequence."
  },
  {
    "objectID": "W04.html#philosophy-of-aggregation",
    "href": "W04.html#philosophy-of-aggregation",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Philosophy of aggregation",
    "text": "Philosophy of aggregation\n\n\nThe mean represents total value per value.\nExample: per capita income in a town is the total income per individual\nThe median represents the value such that half of the values are lower and higher.\nIn a democracy where each value is represented by one voter preferring it, the median is the value which is unbeatable by an absolute majority. Half of the people prefer higher the other half lower values. (Median voter model)\nThe mode represents the most common value.\nIn a democracy, the mode represents the winner of a plurality vote where each value runs as a candidate and the winner is the one with the most votes."
  },
  {
    "objectID": "W04.html#mean-median-mode-properties",
    "href": "W04.html#mean-median-mode-properties",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Mean, Median, Mode properties",
    "text": "Mean, Median, Mode properties\nDo they deliver one unambiguous answer for any sequence?\n\nMean and median, yes.\nThe mode has no rules for a tie.\n\n\nCan they by generalized to variables with ordered or even unordered categories?\n\n\nMean: No.\nMedian: For ordered categories (except when even number and the two middlemost are not the same) Mode: For any categorical variable.\n\n\nIs the measure always also in the data sequence?\n\n\nMean: No.\nMedian: Yes, for sequences of odd length.\nMode: Yes."
  },
  {
    "objectID": "W04.html#generalized-means",
    "href": "W04.html#generalized-means",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Generalized means1",
    "text": "Generalized means1\nFor \\(x_1, \\dots, x_n > 0\\) and \\(p\\in \\mathbb{R}_{\\neq 0}\\) the generalized mean is\n\\[M_p(x_1, \\dots, x_n) = (\\frac{1}{n}\\sum_{i=1}^n x_i^p)^\\frac{1}{p}\\]\nFor \\(p = 0\\) it is \\(M_0(x_1, \\dots, x_n) = (\\prod_{i=1}^n x_i)^\\frac{1}{n}\\).\n\\(M_1\\) is the arithmetic mean. \\(M_0\\) is called the geometric mean. \\(M_{-1}\\) the harmonic mean.\nNote: Generalized means are often only reasonable when all values are positive \\(x_i > 0\\).\n\n\n\\(M_0\\) can also be expressed as the exponential (\\(\\exp(x) = e^x\\)) of the mean of the the \\(\\log\\)’s of the \\(x_i\\)’s: \\(\\exp(\\log((\\prod_{i=1}^n x_i)^\\frac{1}{n})) = \\exp(\\frac{1}{n}\\sum_{i=1}^n\\log(x_i))\\).\nAlso called power mean or \\(p\\)-mean."
  },
  {
    "objectID": "W04.html#box-cox-transformation-function",
    "href": "W04.html#box-cox-transformation-function",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Box-Cox transformation function1",
    "text": "Box-Cox transformation function1\nFor \\(p \\in \\mathbb{R}\\): \\(f(x) = \\begin{cases}\\frac{x^p - 1}{p} & \\text{for $p\\neq 0$} \\\\ \\log(x) & \\text{for $p= 0$}\\end{cases}\\)\n\n\nThe \\(p\\)-mean is\n\\[M_p(x) = f^{-1}(\\frac{1}{n}\\sum_{i=1}^n f(x_i))\\]\nwith \\(x = [x_1, \\dots, x_n]\\). \\(f^{-1}\\) is the inverse2 of \\(f\\).\n\n\n\n\n\n\n\n\nAlso a common transformation to pre-process data to make it closer to a normal distribution.That means \\(f^-1(f(x)) = x =f(f^-1(x))\\)."
  },
  {
    "objectID": "W04.html#application-the-wisdom-of-the-crowd",
    "href": "W04.html#application-the-wisdom-of-the-crowd",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Application: The Wisdom of the Crowd",
    "text": "Application: The Wisdom of the Crowd\n\n\n\nThe collective opinion of a diverse group of independent individuals rather than that of a single expert.\nThe classical wisdom-of-the-crowds finding is about point estimation of a continuous quantity.\nPopularized by James Surowiecki (2004).\nThe opening anecdote is about Francis Galton’s1 surprise in 1907 that the crowd at a county fair accurately guessed the weight of an ox’s meat when their individual guesses were averaged.\n\n\n\n\n\n\nGalton (1822-1911) was a half-cousin to Charles Darwin and one of the founding fathers of statistics. He also was a scientific racist, see https://twitter.com/kareem_carr/status/1575506343401775104?s=20&t=8T5TzrayAWNShmOSzJgCJQ.."
  },
  {
    "objectID": "W04.html#galtons-data",
    "href": "W04.html#galtons-data",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Galton’s data1",
    "text": "Galton’s data1\nWhat is the weight of the meat of this ox?\n\n\n\n\nlibrary(readxl)\ngalton <- read_excel(\"data/galton_data.xlsx\")\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\") + geom_vline(xintercept = median(galton$Estimate), color = \"blue\") + geom_vline(xintercept = Mode(galton$Estimate), color = \"purple\")\n\n\n\n\n787 estimates, true value 1198, mean 1196.7, median 1208, mode 1218\nKenneth Wallis dug out the data from Galton’s notebook and put it here https://warwick.ac.uk/fac/soc/economics/staff/kfwallis/publications/galton_data.xlsx"
  },
  {
    "objectID": "W04.html#viertelfest-bremen-2008",
    "href": "W04.html#viertelfest-bremen-2008",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Viertelfest Bremen 20081",
    "text": "Viertelfest Bremen 20081\nHow many lots will be sold by the end of the festival?\n\nviertel <- read_csv(\"data/Viertelfest.csv\")\nviertel |> ggplot(aes(`Schätzung`)) + geom_histogram() + geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\")\n\n\n\n\n1226 estimates, the maximal value is 29530000!\nWe should filter out the highest values for the histogram…\nData collected as additional guessing game at the Lottery “Haste mal ’nen Euro?”, data provided by Jan Lorenz https://docs.google.com/spreadsheets/d/1HiYhUrYrsbeybJ10mwsae_hQCawZlUQFOOZzcugXzgA/edit#gid=0"
  },
  {
    "objectID": "W04.html#viertelfest-bremen-2008-1",
    "href": "W04.html#viertelfest-bremen-2008-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Viertelfest Bremen 2008",
    "text": "Viertelfest Bremen 2008\nHow many lots will be sold by the end of the festival?\n\nviertel <- read_csv(\"data/Viertelfest.csv\")\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\") + geom_vline(xintercept = exp(mean(log(viertel$Schätzung))), color = \"orange\")\n\n\n1226 estimates, true value 10788, mean 53163.9, median 9843, mode 10000,\ngeometric mean 10510.1"
  },
  {
    "objectID": "W04.html#log_10-transformation-viertelfest",
    "href": "W04.html#log_10-transformation-viertelfest",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "\\(\\log_{10}\\) transformation Viertelfest",
    "text": "\\(\\log_{10}\\) transformation Viertelfest\n\nviertel |> mutate(log10Est = log10(Schätzung)) |> ggplot(aes(log10Est)) + geom_histogram(binwidth = 0.05) + geom_vline(xintercept = log10(10788), color = \"green\") + \n geom_vline(xintercept = log10(mean(viertel$Schätzung)), color = \"red\") + geom_vline(xintercept = log10(median(viertel$Schätzung)), color = \"blue\") + geom_vline(xintercept = log10(Mode(viertel$Schätzung)), color = \"purple\") + geom_vline(xintercept = mean(log10(viertel$Schätzung)), color = \"orange\")\n\n\n1226 estimates, true value 10788, mean 53163.9, median 9843, mode 10000,\ngeometric mean 10510.1"
  },
  {
    "objectID": "W04.html#wisdom-of-the-crowd-insights",
    "href": "W04.html#wisdom-of-the-crowd-insights",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Wisdom of the crowd insights",
    "text": "Wisdom of the crowd insights\n\n\nIn Galton’s sample the different measures do not make a big difference\nIn the Viertelfest data the arithmetic mean performs very bad!\nThe mean is vulnerable to extreme values.\nQuoting Galton on the mean as a democratic aggregation function:\n“The mean gives voting power to the cranks in proportion to their crankiness.”\nThe mode tends to be on focal values as round numbers (10,000). In Galton’s data this is not so pronounced beause estimators used several units which Galton had to convert.\nHow to choose a measure to aggregate the wisdom?\n\nBy the nature of the estimate problem? Is the scale mostly clear? (Are we in the hundreds, thousands, ten thousands, …)\nBy the nature of the distribution?\nThere is no real insurance against a systematic bias in the population."
  },
  {
    "objectID": "W04.html#measures-of-dispersion-1",
    "href": "W04.html#measures-of-dispersion-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Measures of dispersion1",
    "text": "Measures of dispersion1\nGoal: We want to measure\n\nHow spread out values are around the central tendency.\nHow stretched or squeezed is the distribution?\n\nVariance is the mean of the squared deviation from the mean: \\(\\text{Var}(x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2\\) where \\(\\mu\\) (mu) is the mean.\nStandard deviation is the square root of the variance \\(\\text{SD}(x) = \\sqrt{\\text{Var}(x)}\\).\nThe standard deviation is often denoted \\(\\sigma\\) (sigma) and the variance \\(\\sigma^2\\).\nMean absolute deviation (MAD) is the mean of the absolute deviation from the mean: \\(\\text{MAD}(x) = \\frac{1}{n}\\sum_{i=1}^n|x_i - \\mu|\\).\nRange is the difference of the maximal and the minimal value \\(\\max(x) - \\min(x)\\).\nAlso called variability, scatter, or spread."
  },
  {
    "objectID": "W04.html#examples-of-measures-of-dispersion",
    "href": "W04.html#examples-of-measures-of-dispersion",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Examples of measures of dispersion",
    "text": "Examples of measures of dispersion\n\n\n\nvar(galton$Estimate)\n\n[1] 5415.013\n\nsd(galton$Estimate)\n\n[1] 73.58677\n\nmad(galton$Estimate)\n\n[1] 51.891\n\nrange(galton$Estimate)\n\n[1]  896 1516\n\ndiff(range(galton$Estimate))\n\n[1] 620\n\n\n\n\nvar(viertel$Schätzung)\n\n[1] 719774887849\n\nsd(viertel$Schätzung)\n\n[1] 848395.5\n\nmad(viertel$Schätzung)\n\n[1] 8771.803\n\nrange(viertel$Schätzung)\n\n[1]      120 29530000\n\ndiff(range(viertel$Schätzung))\n\n[1] 29529880\n\n\n\n\n\n\nVariance (and standard deviation) in statistics is usually computed with \\(\\frac{1}{n-1}\\) instead of \\(\\frac{1}{n}\\) to provide an unbiased estimator of the potentially underlying population variance. We omit more detail here."
  },
  {
    "objectID": "W04.html#standardization",
    "href": "W04.html#standardization",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Standardization",
    "text": "Standardization\nVariables are standardized by subtracting their mean and then dividing by their standard deviations.\nA value from a standardized variable is called a standard score or z-score.\n\\(z_i = \\frac{x_i - \\mu}{\\sigma}\\)\nwhere \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation of the vector \\(x\\).\n\nThis is a shift-scale transformation. We shift by the mean and scale by the standard deviation.\nA standard score \\(z_i\\) shows how mean standard deviations \\(x_i\\) is away from the mean of \\(x\\)."
  },
  {
    "objectID": "W04.html#achievements-and-next-steps",
    "href": "W04.html#achievements-and-next-steps",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Achievements and next steps",
    "text": "Achievements and next steps\n\nWe have learned about the data science process\nYou made essential steps in data visualization and data wrangling with New York City Flights in the Homework\nYou can write and render reproducible reports\nWe had some math refreshment\nWe learned some data science data and programming concepts in R and in Python. Reconsider them in later homework!\n\nNext steps coming (you will receive individual repositories for this):\n\nHomework mimicking data science projects\nSome exploratory data analysis in a sandbox\nThinking about your own data science project (in groups of 2-3)\n\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "",
    "text": "Developing Syllabus\n\n\n\n\n\nThis syllabus may change during the course of the semester with some additions and clarifications.\nAll substantial changes will marked here with date.\nImportant Links:\nGitHub organization of the course: https://github.com/CU-F23-MDSSB-01-Concepts-Tools\nOrganization repository with non-public information and FAQ in Discussions: https://github.com/CU-F23-MDSSB-01-Concepts-Tools/Organization"
  },
  {
    "objectID": "index.html#module-names",
    "href": "index.html#module-names",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "1 Module Names",
    "text": "1 Module Names\nThis syllabus is for two modules which require each other.\n\nMDSSSB-DSOC-02 Data Science Concepts (Core area, 5 credits)\nMDSSSB-MET-01 Data Science Tools (Methods area, 5 credits)\n\nOffered in the Master program: Data Science for Society and Business (DSSB). See the module description in the DSSB Handbook."
  },
  {
    "objectID": "index.html#module-components",
    "href": "index.html#module-components",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "2 Module Components",
    "text": "2 Module Components\n\nData Science Concepts (5 Credits)\nData Science Tools in R (2.5 credits)\nData Science Tools in python (2.5 credits)\n\nAll are courses offered in the Fall term and have no entry requirement.\nData Science Concepts and Data Science Tools are co-requirements.\nThe Concepts treated in the lectures are applied in exercises and homework in the Tools course."
  },
  {
    "objectID": "index.html#class-meeting-information",
    "href": "index.html#class-meeting-information",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "3 Class Meeting Information",
    "text": "3 Class Meeting Information\nData Science Concepts:\nMonday 09:45-11:00 and 11:15-12:30\nLocation: East Hall Room 1 in-person\nVideo streaming in Teams provided for late-arriving students: Team F23_MDSSB-DSOC-02_Data Science Concepts, General Channel\nData Science Tools:\nThursday 09:45-11:00 and 11:15-12:30\nLocation: East Hall Room 2 in-person\nVideo streaming in Teams provided for late-arriving students: Teams F23_MDSSB-MET-01-A_Data Science Tools in R and F23_MDSSB-MET-01-B_Data Science Tools in python, General Channel\nNote: The R and python courses are provided in the same time slot. Details in the schedule."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "4 Instructors",
    "text": "4 Instructors\nConcepts:\nJan Lorenz Email: jlorenz@constructor.univeristy\nTools in R:\nArmin Müller Email: armmueller@constructor.university\nTools in python:\nPeter Steiglechner Email: psteiglechner@constructor.university"
  },
  {
    "objectID": "index.html#format-and-workload",
    "href": "index.html#format-and-workload",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "5 Format and Workload",
    "text": "5 Format and Workload\nConcepts: Lectures, sometimes with aspects of Tutorials (35 hours in presence, total expected workload 125 hours)\nTools: Tutorials, sometimes with aspects of Lectures (35 hours in presence, total expected workload 125 hours)\nBesides homework, both modules require a decent amount of self-study.\nWorkload homework and self-study is expect to be 90/125 = 72% of the total workload.\nHomework assignments are given along the concepts treated in Concepts to be solved with the tools treated in Tools."
  },
  {
    "objectID": "index.html#intended-learning-outcomes",
    "href": "index.html#intended-learning-outcomes",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "6 Intended Learning Outcomes",
    "text": "6 Intended Learning Outcomes\nThe module descriptions can be found in the DSSB Handbook\n\n6.1 From the handbook\nBy the end of the Concepts module, you will be able to:\n\nunderstand and use the mathematical foundations of statistical learning algorithms\nexplain and classify data science problems\nexplain and classify data-driven approaches\nunderstand the application of data science techniques to typical situations and tasks in business and societal research, including the search, retrieval, preparation, and statistical analysis of data\ninterpret complexity analysis and performance evaluation of data science problems and algorithms\n\nBy the end of the Tools module, you will be able to:\n\nexplain basic concepts of imperative and object-oriented programming\nwrite, test, and debug programs\nperform data handling and data manipulation tasks in R and Python\napply your knowledge to implement own functions in R and Python\neffectively use core packages and libraries of R and Python for data analysis\nknow about the typical applications of R and Python in data science\nimplement and apply advanced data mining methods with appropriate tools\nperform a full cycle of data analysis\n\n\n\n6.2 Main Learning Goal\nOur main goal to help you build a good basis for your more and more independent work in the whole study program. That means you can\n\nlearn core concepts in data science on your own, for example\n\nconcepts to explore data (import, wrangle, visualize)\nlearn and explore mathematics and statistics through the data science lens\nlearn concepts to model and draw conclusions from data (model, infer, predict)\n\ncreate and maintain a digital working environment on your computer to do data science\nlearn to program in the data science languages R and python, and become able to learn new skills in these independently\ndo a data science project of your own interest"
  },
  {
    "objectID": "index.html#examination-and-assessment",
    "href": "index.html#examination-and-assessment",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "7 Examination and Assessment",
    "text": "7 Examination and Assessment\n\n7.1 Concepts Module\nAssessment Type: Written Examination\nDuration: 120 min\nWeight: 100%\nScope: All intended learning outcomes of the module.\nCompletion: to pass this module, the examination has to be passed with at least 45%\nAn exam will take place after all lectures in December. The date will be published by the university adminstration later.\nThere are no additional achievements necessary and there are no bonus options.\n\n\n7.2 Tools Module\nModule achievement: 50% of the assignments correctly solved\nProgramming and analysis assignments will appear step by step during the courses.\nTo pass the module you have to solve half of them by the end of the semester.\nAssessment Type: Project Report\nLength: 4000 - 5000 words Weight: 100% Scope: All intended learning outcomes of the module.\nMore information of project formats and a rubric for the grading will be provided later.\nBonus option: Students receive 0.33 points grade improvements on their project grade (on the numerical grade as specified here https://constructor.university/sites/default/files/2023-02/Grading_Table_2023.pdf) when all assignments are solved by the end of the semester. (Note, the bonus is not necessary to reach the best grade in the module.)\nAll assignments and the final project must be delivered in personalized repositories in the GitHub organization https://github.com/CU-F23-MDSSB-01-Concepts-Tools."
  },
  {
    "objectID": "index.html#module-policies",
    "href": "index.html#module-policies",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "8 Module Policies",
    "text": "8 Module Policies\nThere are no formal requirement about attendance and active participation. However, we rely on your engagement in a many-facted way including:\n\nPreparation (looking at readings and material before and after class, being informed about syllabus and course material)\nFocus (avoid distraction during in class and self-learning activities)\nPresence (listening and responding during group activities)\nAsking questions (in class, out of class, online, offline, when you get stuck conclude by writing a question)\nSpecificity (being as specific as possible when describing your problem or question)\nSynthesizing (making connections between concepts from reading and discussion)\nPersistence (you don’t need to understand everything immediately, but stay engaged, try again, confusion shows that you pay attention)"
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "9 Academic Integrity",
    "text": "9 Academic Integrity\nAll involved parties (professors and lecturers, instructors and students) are expected to abide by the word and spirit of the “Code of Academic Integrity”: https://constructor.university/student-life/student-services/university-policies/academic-policies/code-of-academic-integrity. Violations of the Code might be brought to the attention of the Academic Integrity Committee."
  },
  {
    "objectID": "index.html#artifical-intelligence-ai-use-policy",
    "href": "index.html#artifical-intelligence-ai-use-policy",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "10 Artifical Intelligence (AI) Use Policy",
    "text": "10 Artifical Intelligence (AI) Use Policy\nThis policy covers any generative AI tool, such as ChatGPT, Elicit, etc. This includes text, code, slides, artwork/graphics/video/audio and other products.\nWe instructors encouraged to using and exploring AI tools for these purposes:\n\nLearning by dialog with a chatbot. AI chatbots can be very helpful to explain you concepts on your desired level and get a feeling about how certain topics are treated. You can ask for an easier or more detailed explanation or focus on certain aspects. Note: The capabilities are limited and you likely receive a lot of false information! Using chatbots should remain a small part of your learning process. Rule of thumb: Spend not more than 25% of the learning time with chatting. There is no way around reading textbooks, reading software documentation, learning and understanding concepts, searching for help online, asking instructors or fellow students.\nHave code snippets written. Tools like GitHub Copilot (free to use for students and teachers in VSCode) are heavily used and currently change code-writing. They can speed up writing your code. However, they do not deliver always correct solutions. There is no way around understanding yourself what a code is doing! Do not spend endless hours asking for new code with new prompts, spend time understanding a language and the functions and objects you are using! Copilots can be a great help to get a skeleton of code and an idea how your solution might look, they rarely deliver the complete code. Expect that the code does not work, expect that the code seems to works but the results are wrong. You are 100% accountable for the code you produce, with or without the help of a copilot!\nHave a draft text snippet written. Data science is also about formulating and describing research questions, describing data, documenting code, interpreting results, and deriving conclusion form the results. This is all verbal text and chatbots are good in writing text which often appears well readable. You can use this to inspire yourself and to polish and improve your texts. However, you are 100% accountable for the text you deliver. You are expected to know what your text is about and to be able to answer questions about what your text means! In text generated purely by a chatbot it is often evident that you do not. We consider such cases worse than incomplete but sensible text. Also text written by chatbots is often very generic and less specific. In general, we value more specific text higher than generic text. Large parts of very generic text is considered worse than a shorter more specific text. In extreme case, a long very generic text will be considered worse than no text at all\nNote: Philosophical and legal questions around the training and use of chatbots and code copilots are controversially contested and re-examined constantly! We encourage to engage with such questions and become aware of arguments and debates.\n\nIf any part of this AI policy is confusing or uncertain, please reach out to us for a conversation before submitting your work."
  },
  {
    "objectID": "index.html#schedule-and-homework",
    "href": "index.html#schedule-and-homework",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "11 Schedule and Homework",
    "text": "11 Schedule and Homework\nThe Schedule is on an extra page and will be updated continuously with\n\nLinks to slides\nNote on what homework you are expected to do\nSome questions which you should be able to answer after each week. Test yourself.\n\nHomework page will successively appear as extra pages. See the sidebar to the left."
  },
  {
    "objectID": "index.html#feedback-from-students",
    "href": "index.html#feedback-from-students",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "12 Feedback from Students",
    "text": "12 Feedback from Students\nWe are eager to constantly improve the quality of our teaching. We would be glad to obtain your feedback at any time of the course to improve your learning experience."
  },
  {
    "objectID": "W03.html#readr-and-readxl",
    "href": "W03.html#readr-and-readxl",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "readr and readxl",
    "text": "readr and readxl\n\n\n\n\nread_csv() - comma delimited files\nread_csv2() - semicolon delimited files (common where “,” is used as decimal place)\nread_tsv() - tab delimited files\nread_delim() - reads in files with any delimiter\n…\n\n\n\n\nread_excel() read xls or xlsx files from MS Excel\n…\n\n\n\n\n\nThere are also packages to write data from R to excel files (writexl, openxlsx, xlsx, …)."
  },
  {
    "objectID": "W03.html#other-data-formats",
    "href": "W03.html#other-data-formats",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Other data formats",
    "text": "Other data formats\nR packages, analog libraries will exist for python\n\ngooglesheets4: Google Sheets\nhaven: SPSS, Stata, and SAS files\nDBI, along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc): allows you to run SQL queries against a database and return a data frame\njsonlite: JSON\nxml2: xml\nrvest: web scraping\nhttr: web APIs\n…"
  },
  {
    "objectID": "W03.html#comma-separated-values-csv",
    "href": "W03.html#comma-separated-values-csv",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Comma-separated values (CSV)",
    "text": "Comma-separated values (CSV)\nWe use CSV file when there is no certain reason to do otherwise.1\nCSV files are delimited text file\n\nCan be viewed with any text editor\nShow each row of the data frame in a line\nSeparates the content of columns by commas (or the delimiter character)\nEach cell could be surrounded by quotes (when long text with commas (!) is in cells)\nThe first line is interpreted as listing the variable names by default\n\nreadr tries to guess the data type of variables\nYou can also customize it yourself!\nPotential reasons are: CSV being not provided, or the dataset being very larger and hard-disk storage is an issue. Other formats or more space efficient."
  },
  {
    "objectID": "W03.html#data-import-workflow",
    "href": "W03.html#data-import-workflow",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Data import workflow",
    "text": "Data import workflow\n\nYou download your CSV file to the data/ directory. You may use download.file() for this, but make sure you do not download large amounts of data each time you render your file!\nRead the data with data <- read_csv(\"data/FILENAME.csv\") and read the report in the console.\nExplore if you are happy and iterate by customizing the data import line using specifications until the data is as you want it to be.\n\nGood practices to document the data download:\n\nOne or low number of files: Put the download line(s) in you main document, but comment out # after usage.\nWrite a script (data-download.r) to document the download commands.\nMake your code check first if the file already exist, like this if (!(file.exists(\"DATA_FILE.csv\"))) {DOWNLOAD-CODE}"
  },
  {
    "objectID": "W03.html#download-2.-read",
    "href": "W03.html#download-2.-read",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "1. Download, 2. Read",
    "text": "1. Download, 2. Read\nThis downloads data only if the file does not exist. Then it loads it.\n\nif (!file.exists(\"data/hotels.csv\")) {\n  download.file(url = \"https://raw.githubusercontent.com/rstudio-education/datascience-box/main/course-materials/_slides/u2-d06-grammar-wrangle/data/hotels.csv\", \n                destfile = \"data/hotels.csv\")\n}\nhotels <- read_csv(\"data/hotels.csv\")\n\nRows: 119390 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): hotel, arrival_date_month, meal, country, market_segment, distrib...\ndbl  (18): is_canceled, lead_time, arrival_date_year, arrival_date_week_numb...\ndate  (1): reservation_status_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOutput is a summary how read_csv guessed the data types of columns."
  },
  {
    "objectID": "W03.html#explore-using-spec",
    "href": "W03.html#explore-using-spec",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "3. Explore using spec()",
    "text": "3. Explore using spec()\nAll details to check or customize:\n\nspec(hotels)\n\ncols(\n  hotel = col_character(),\n  is_canceled = col_double(),\n  lead_time = col_double(),\n  arrival_date_year = col_double(),\n  arrival_date_month = col_character(),\n  arrival_date_week_number = col_double(),\n  arrival_date_day_of_month = col_double(),\n  stays_in_weekend_nights = col_double(),\n  stays_in_week_nights = col_double(),\n  adults = col_double(),\n  children = col_double(),\n  babies = col_double(),\n  meal = col_character(),\n  country = col_character(),\n  market_segment = col_character(),\n  distribution_channel = col_character(),\n  is_repeated_guest = col_double(),\n  previous_cancellations = col_double(),\n  previous_bookings_not_canceled = col_double(),\n  reserved_room_type = col_character(),\n  assigned_room_type = col_character(),\n  booking_changes = col_double(),\n  deposit_type = col_character(),\n  agent = col_character(),\n  company = col_character(),\n  days_in_waiting_list = col_double(),\n  customer_type = col_character(),\n  adr = col_double(),\n  required_car_parking_spaces = col_double(),\n  total_of_special_requests = col_double(),\n  reservation_status = col_character(),\n  reservation_status_date = col_date(format = \"\")\n)"
  },
  {
    "objectID": "W03.html#finalize-data-import-option-1",
    "href": "W03.html#finalize-data-import-option-1",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Finalize data import, option 1",
    "text": "Finalize data import, option 1\nWhen\n\nall columns are how they should\nyou consider it not necessary to document the specifications\n\nThen use show_col_types = FALSE to quiet the reading message.\n\nhotels <- read_csv(\"data/hotels.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "W03.html#finalize-data-import-option-2",
    "href": "W03.html#finalize-data-import-option-2",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Finalize data import, option 2",
    "text": "Finalize data import, option 2\n\nCopy the spec(hotels) output into the col_types argument\nIf necessary, customize it\n\n\nhotels <- read_csv(\"data/hotels.csv\", col_types = cols(\n  hotel = col_character(),\n  is_canceled = col_logical(),\n  lead_time = col_integer(),\n  arrival_date_year = col_integer(),\n  arrival_date_month = col_character(),\n  arrival_date_week_number = col_integer(),\n  arrival_date_day_of_month = col_integer(),\n  stays_in_weekend_nights = col_integer(),\n  stays_in_week_nights = col_integer(),\n  adults = col_integer(),\n  children = col_integer(),\n  babies = col_integer(),\n  meal = col_character(),\n  country = col_character(),\n  market_segment = col_character(),\n  distribution_channel = col_character(),\n  is_repeated_guest = col_logical(),\n  previous_cancellations = col_integer(),\n  previous_bookings_not_canceled = col_integer(),\n  reserved_room_type = col_character(),\n  assigned_room_type = col_character(),\n  booking_changes = col_integer(),\n  deposit_type = col_character(),\n  agent = col_integer(),\n  company = col_integer(),\n  days_in_waiting_list = col_integer(),\n  customer_type = col_character(),\n  adr = col_double(),\n  required_car_parking_spaces = col_integer(),\n  total_of_special_requests = col_integer(),\n  reservation_status = col_character(),\n  reservation_status_date = col_date(format = \"\")\n))"
  },
  {
    "objectID": "W03.html#columns-types",
    "href": "W03.html#columns-types",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Columns types",
    "text": "Columns types\n\n\n\ntype function\ndata type\n\n\n\n\ncol_character()\ncharacter\n\n\ncol_date()\ndate\n\n\ncol_datetime()\nPOSIXct (date-time)\n\n\ncol_double()\ndouble (numeric)\n\n\ncol_factor()\nfactor\n\n\ncol_guess()\nlet readr guess (default)\n\n\ncol_integer()\ninteger\n\n\ncol_logical()\nlogical\n\n\ncol_number()\nnumbers mixed with non-number characters\n\n\ncol_skip()\ndo not read\n\n\ncol_time()\ntime"
  },
  {
    "objectID": "W03.html#hotels-data",
    "href": "W03.html#hotels-data",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Hotels data",
    "text": "Hotels data\n\nData from two hotels: one resort and one city hotel\nObservations: Each row represents a hotel booking"
  },
  {
    "objectID": "W03.html#first-look-on-data",
    "href": "W03.html#first-look-on-data",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "First look on data",
    "text": "First look on data\nType the name of the data frame\n\nhotels\n\n# A tibble: 119,390 × 32\n   hotel        is_canceled lead_time arrival_date_year arrival_date_month\n   <chr>        <lgl>           <int>             <int> <chr>             \n 1 Resort Hotel FALSE             342              2015 July              \n 2 Resort Hotel FALSE             737              2015 July              \n 3 Resort Hotel FALSE               7              2015 July              \n 4 Resort Hotel FALSE              13              2015 July              \n 5 Resort Hotel FALSE              14              2015 July              \n 6 Resort Hotel FALSE              14              2015 July              \n 7 Resort Hotel FALSE               0              2015 July              \n 8 Resort Hotel FALSE               9              2015 July              \n 9 Resort Hotel TRUE               85              2015 July              \n10 Resort Hotel TRUE               75              2015 July              \n# ℹ 119,380 more rows\n# ℹ 27 more variables: arrival_date_week_number <int>,\n#   arrival_date_day_of_month <int>, stays_in_weekend_nights <int>,\n#   stays_in_week_nights <int>, adults <int>, children <int>, babies <int>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <lgl>,\n#   previous_cancellations <int>, previous_bookings_not_canceled <int>, …"
  },
  {
    "objectID": "W03.html#look-on-variable-names",
    "href": "W03.html#look-on-variable-names",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Look on variable names",
    "text": "Look on variable names\n\nnames(hotels)\n\n [1] \"hotel\"                          \"is_canceled\"                   \n [3] \"lead_time\"                      \"arrival_date_year\"             \n [5] \"arrival_date_month\"             \"arrival_date_week_number\"      \n [7] \"arrival_date_day_of_month\"      \"stays_in_weekend_nights\"       \n [9] \"stays_in_week_nights\"           \"adults\"                        \n[11] \"children\"                       \"babies\"                        \n[13] \"meal\"                           \"country\"                       \n[15] \"market_segment\"                 \"distribution_channel\"          \n[17] \"is_repeated_guest\"              \"previous_cancellations\"        \n[19] \"previous_bookings_not_canceled\" \"reserved_room_type\"            \n[21] \"assigned_room_type\"             \"booking_changes\"               \n[23] \"deposit_type\"                   \"agent\"                         \n[25] \"company\"                        \"days_in_waiting_list\"          \n[27] \"customer_type\"                  \"adr\"                           \n[29] \"required_car_parking_spaces\"    \"total_of_special_requests\"     \n[31] \"reservation_status\"             \"reservation_status_date\""
  },
  {
    "objectID": "W03.html#second-look-with-glimpse",
    "href": "W03.html#second-look-with-glimpse",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Second look with glimpse",
    "text": "Second look with glimpse\n\nglimpse(hotels)\n\nRows: 119,390\nColumns: 32\n$ hotel                          <chr> \"Resort Hotel\", \"Resort Hotel\", \"Resort…\n$ is_canceled                    <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ lead_time                      <int> 342, 737, 7, 13, 14, 14, 0, 9, 85, 75, …\n$ arrival_date_year              <int> 2015, 2015, 2015, 2015, 2015, 2015, 201…\n$ arrival_date_month             <chr> \"July\", \"July\", \"July\", \"July\", \"July\",…\n$ arrival_date_week_number       <int> 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ arrival_date_day_of_month      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ stays_in_weekend_nights        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ stays_in_week_nights           <int> 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, …\n$ adults                         <int> 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ children                       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ babies                         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ meal                           <chr> \"BB\", \"BB\", \"BB\", \"BB\", \"BB\", \"BB\", \"BB…\n$ country                        <chr> \"PRT\", \"PRT\", \"GBR\", \"GBR\", \"GBR\", \"GBR…\n$ market_segment                 <chr> \"Direct\", \"Direct\", \"Direct\", \"Corporat…\n$ distribution_channel           <chr> \"Direct\", \"Direct\", \"Direct\", \"Corporat…\n$ is_repeated_guest              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ previous_cancellations         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_bookings_not_canceled <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ reserved_room_type             <chr> \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"C\", \"C\",…\n$ assigned_room_type             <chr> \"C\", \"C\", \"C\", \"A\", \"A\", \"A\", \"C\", \"C\",…\n$ booking_changes                <int> 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ deposit_type                   <chr> \"No Deposit\", \"No Deposit\", \"No Deposit…\n$ agent                          <int> NA, NA, NA, 304, 240, 240, NA, 303, 240…\n$ company                        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ days_in_waiting_list           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ customer_type                  <chr> \"Transient\", \"Transient\", \"Transient\", …\n$ adr                            <dbl> 0.00, 0.00, 75.00, 75.00, 98.00, 98.00,…\n$ required_car_parking_spaces    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_of_special_requests      <int> 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 3, …\n$ reservation_status             <chr> \"Check-Out\", \"Check-Out\", \"Check-Out\", …\n$ reservation_status_date        <date> 2015-07-01, 2015-07-01, 2015-07-02, 20…\n\n\nNow, comes the data wrangling, transformation, …"
  },
  {
    "objectID": "W03.html#grammar-of-data-wrangling",
    "href": "W03.html#grammar-of-data-wrangling",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Grammar of Data Wrangling",
    "text": "Grammar of Data Wrangling\n\n\n\n\n\nGrammar of data wrangling: Start with a dataset and pipe it through several manipulations with |>\nmpg |> \n  filter(cyl == 8) |> \n  select(manufacturer, hwy) |> \n  group_by(manufacturer) |> \n  summarize(mean_hwy = mean(hwy))\n\n\nSimilar in python: Make a chain using . to apply pandas methods for data frames one after the other.\nSimilar in ggplot2: Creating a ggplot object, then add graphical layers (geom_ functions) with + (instead of a pipe)\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = trans)) + \n  geom_point() + \n  geom_smooth()"
  },
  {
    "objectID": "W03.html#what-is-the-pipe",
    "href": "W03.html#what-is-the-pipe",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "What is the pipe |>?",
    "text": "What is the pipe |>?\nx |> f(a,b) is the same as f(x,a,b)\nThe outcome of a command is put into the first argument of the next function call. Practice it it see that it is exactly identical!\nReasons for using pipes:\n\nstructure the sequence of your data operations from left to right\navoid nested function calls:\nnested: filter(select(hotels, hotel, adults), adults == 2)\npiped: hotels |> select(hotel, adults) |> filter(adults == 2)\n(base R: hotels[hotels$adults == 2, c(\"hotel\", \"adults\")])\nYou’ll minimize the need for local variables and function definitions\nYou’ll make it easy to add steps anywhere in the sequence of operations\n\n\n\nSince R 4.1.0, the pipe is part of base R. Before you had to load the magrittr package and use %>%. You still find it in a lot of code out in the wild. It is almost the same."
  },
  {
    "objectID": "W03.html#dplyr-uses-verbs-to-manipulate",
    "href": "W03.html#dplyr-uses-verbs-to-manipulate",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "dplyr uses verbs to manipulate",
    "text": "dplyr uses verbs to manipulate\n\nselect: pick columns by name\narrange: reorder rows\nslice: pick rows using index(es)\nfilter: pick rows matching criteria\ndistinct: filter for unique rows\nmutate: add new variables\nsummarise: reduce variables to values\ngroup_by: for grouped operations\n… (many more)"
  },
  {
    "objectID": "W03.html#select-a-single-column",
    "href": "W03.html#select-a-single-column",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "select a single column",
    "text": "select a single column\n\n\n\nhotels |> select(lead_time)     \n\n\n\n# A tibble: 119,390 × 1\n   lead_time\n       <int>\n 1       342\n 2       737\n 3         7\n 4        13\n 5        14\n 6        14\n 7         0\n 8         9\n 9        85\n10        75\n# ℹ 119,380 more rows\n\n\nNote: select(hotels, lead_time) is identical.\n\nWhy does piping |> work?\nEvery dplyr function\n\ntakes a data frame (tibble) as first argument\noutputs a (manipulated) data frame (tibble)\n\n\n\n\n\nIn hotel business, lead time is the time betweeen booking and arrival."
  },
  {
    "objectID": "W03.html#select-more-columns",
    "href": "W03.html#select-more-columns",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Select more columns",
    "text": "Select more columns\n\nhotels |> select(hotel, lead_time)     \n\n\n\n# A tibble: 119,390 × 2\n   hotel        lead_time\n   <chr>            <int>\n 1 Resort Hotel       342\n 2 Resort Hotel       737\n 3 Resort Hotel         7\n 4 Resort Hotel        13\n 5 Resort Hotel        14\n 6 Resort Hotel        14\n 7 Resort Hotel         0\n 8 Resort Hotel         9\n 9 Resort Hotel        85\n10 Resort Hotel        75\n# ℹ 119,380 more rows\n\n\nNote that hotel is a variable, but hotels the data frame object name"
  },
  {
    "objectID": "W03.html#select-helper-starts_with",
    "href": "W03.html#select-helper-starts_with",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Select helper starts_with",
    "text": "Select helper starts_with\n\nhotels |> select(starts_with(\"arrival\"))\n\n\n\n# A tibble: 119,390 × 4\n   arrival_date_year arrival_date_month arrival_date_week_number\n               <int> <chr>                                 <int>\n 1              2015 July                                     27\n 2              2015 July                                     27\n 3              2015 July                                     27\n 4              2015 July                                     27\n 5              2015 July                                     27\n 6              2015 July                                     27\n 7              2015 July                                     27\n 8              2015 July                                     27\n 9              2015 July                                     27\n10              2015 July                                     27\n# ℹ 119,380 more rows\n# ℹ 1 more variable: arrival_date_day_of_month <int>"
  },
  {
    "objectID": "W03.html#bring-columns-to-the-front",
    "href": "W03.html#bring-columns-to-the-front",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Bring columns to the front",
    "text": "Bring columns to the front\n\nhotels |> select(hotel, market_segment, children, everything())\n\n\n\n# A tibble: 119,390 × 32\n   hotel        market_segment children is_canceled lead_time arrival_date_year\n   <chr>        <chr>             <int> <lgl>           <int>             <int>\n 1 Resort Hotel Direct                0 FALSE             342              2015\n 2 Resort Hotel Direct                0 FALSE             737              2015\n 3 Resort Hotel Direct                0 FALSE               7              2015\n 4 Resort Hotel Corporate             0 FALSE              13              2015\n 5 Resort Hotel Online TA             0 FALSE              14              2015\n 6 Resort Hotel Online TA             0 FALSE              14              2015\n 7 Resort Hotel Direct                0 FALSE               0              2015\n 8 Resort Hotel Direct                0 FALSE               9              2015\n 9 Resort Hotel Online TA             0 TRUE               85              2015\n10 Resort Hotel Offline TA/TO         0 TRUE               75              2015\n# ℹ 119,380 more rows\n# ℹ 26 more variables: arrival_date_month <chr>,\n#   arrival_date_week_number <int>, arrival_date_day_of_month <int>,\n#   stays_in_weekend_nights <int>, stays_in_week_nights <int>, adults <int>,\n#   babies <int>, meal <chr>, country <chr>, distribution_channel <chr>,\n#   is_repeated_guest <lgl>, previous_cancellations <int>,\n#   previous_bookings_not_canceled <int>, reserved_room_type <chr>, …"
  },
  {
    "objectID": "W03.html#more-select-helpers",
    "href": "W03.html#more-select-helpers",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "More select helpers",
    "text": "More select helpers\n\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\ncontains(): Contains a literal string\nnum_range(): Matches a numerical range like x01, x02, x03\none_of(): Matches variable names in a character vector\neverything(): Matches all variables\nlast_col(): Select last variable, possibly with an offset\nmatches(): Matches a regular expression (a sequence of symbols/characters expressing a string/pattern to be searched for within text)\n\n\n\nCheck details with ?one_of"
  },
  {
    "objectID": "W03.html#slice-for-certain-rows",
    "href": "W03.html#slice-for-certain-rows",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "slice for certain rows",
    "text": "slice for certain rows\n\nhotels |> slice(2:4)\n\n\n\n# A tibble: 3 × 32\n  hotel        is_canceled lead_time arrival_date_year arrival_date_month\n  <chr>        <lgl>           <int>             <int> <chr>             \n1 Resort Hotel FALSE             737              2015 July              \n2 Resort Hotel FALSE               7              2015 July              \n3 Resort Hotel FALSE              13              2015 July              \n# ℹ 27 more variables: arrival_date_week_number <int>,\n#   arrival_date_day_of_month <int>, stays_in_weekend_nights <int>,\n#   stays_in_week_nights <int>, adults <int>, children <int>, babies <int>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <lgl>,\n#   previous_cancellations <int>, previous_bookings_not_canceled <int>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, …"
  },
  {
    "objectID": "W03.html#filter-for-rows-with-certain-criteria",
    "href": "W03.html#filter-for-rows-with-certain-criteria",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "filter for rows with certain criteria",
    "text": "filter for rows with certain criteria\n\nhotels |> filter(hotel == \"City Hotel\")\n\n\n\n# A tibble: 79,330 × 32\n   hotel      is_canceled lead_time arrival_date_year arrival_date_month\n   <chr>      <lgl>           <int>             <int> <chr>             \n 1 City Hotel FALSE               6              2015 July              \n 2 City Hotel TRUE               88              2015 July              \n 3 City Hotel TRUE               65              2015 July              \n 4 City Hotel TRUE               92              2015 July              \n 5 City Hotel TRUE              100              2015 July              \n 6 City Hotel TRUE               79              2015 July              \n 7 City Hotel FALSE               3              2015 July              \n 8 City Hotel TRUE               63              2015 July              \n 9 City Hotel TRUE               62              2015 July              \n10 City Hotel TRUE               62              2015 July              \n# ℹ 79,320 more rows\n# ℹ 27 more variables: arrival_date_week_number <int>,\n#   arrival_date_day_of_month <int>, stays_in_weekend_nights <int>,\n#   stays_in_week_nights <int>, adults <int>, children <int>, babies <int>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <lgl>,\n#   previous_cancellations <int>, previous_bookings_not_canceled <int>, …"
  },
  {
    "objectID": "W03.html#filter-for-multiple-criteria",
    "href": "W03.html#filter-for-multiple-criteria",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "filter for multiple criteria",
    "text": "filter for multiple criteria\n\nhotels |> filter(\n  babies >= 1,\n  children >= 1, \n  ) |> \n  select(hotel, adults, babies, children)\n\n\n\n# A tibble: 175 × 4\n   hotel        adults babies children\n   <chr>         <int>  <int>    <int>\n 1 Resort Hotel      2      1        1\n 2 Resort Hotel      2      1        1\n 3 Resort Hotel      2      1        1\n 4 Resort Hotel      2      1        1\n 5 Resort Hotel      2      1        1\n 6 Resort Hotel      2      1        1\n 7 Resort Hotel      2      1        1\n 8 Resort Hotel      2      1        2\n 9 Resort Hotel      2      1        2\n10 Resort Hotel      1      1        2\n# ℹ 165 more rows\n\n\nComma-separated conditions are interpreted as all these should be fulfilled.\nThis is identical to the logical AND &.\nhotels |> filter(babies >= 1 & children >= 1)"
  },
  {
    "objectID": "W03.html#filter-for-complexer-criteria",
    "href": "W03.html#filter-for-complexer-criteria",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "filter for complexer criteria",
    "text": "filter for complexer criteria\n\nhotels |> filter(\n  babies >= 1 | children >= 1\n  ) |> \n  select(hotel, adults, babies, children)\n\n\n\n# A tibble: 9,332 × 4\n   hotel        adults babies children\n   <chr>         <int>  <int>    <int>\n 1 Resort Hotel      2      0        1\n 2 Resort Hotel      2      0        2\n 3 Resort Hotel      2      0        2\n 4 Resort Hotel      2      0        2\n 5 Resort Hotel      2      0        1\n 6 Resort Hotel      2      0        1\n 7 Resort Hotel      1      0        2\n 8 Resort Hotel      2      0        2\n 9 Resort Hotel      2      1        0\n10 Resort Hotel      2      1        0\n# ℹ 9,322 more rows\n\n\n| is the logical OR. Only one criterion needs to be fulfilled."
  },
  {
    "objectID": "W03.html#logical-operators",
    "href": "W03.html#logical-operators",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Logical operators1",
    "text": "Logical operators1\n\n\n\noperator\ndefinition\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\nx & y\nx AND y\n\n\nx | y\nx OR y\n\n\nis.na(x)\ntest if x is NA (missing data)\n\n\n!is.na(x)\ntest if x is not NA (not missing data)\n\n\nx %in% y\ntest if x is in y (often used for strings)\n\n\n!(x %in% y)\ntest if x is not in y\n\n\n!x\nnot x\n\n\n\nLogical is sometimes called Boolean"
  },
  {
    "objectID": "W03.html#excursions-the-concept-of-indexing",
    "href": "W03.html#excursions-the-concept-of-indexing",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Excursions: The Concept of Indexing",
    "text": "Excursions: The Concept of Indexing\nSelect and filter can also be achieved by indexing.\nIn (base) R as well as in python.\nSelect ranges of rows and columns\n\nhotels[1:3,5:7]\n\n\n\n# A tibble: 3 × 3\n  arrival_date_month arrival_date_week_number arrival_date_day_of_month\n  <chr>                                 <int>                     <int>\n1 July                                     27                         1\n2 July                                     27                         1\n3 July                                     27                         1\n\n\nYou can use any vector (with non-overshooting indexes)\n\nhotels[c(1:3,100232),c(5:7,1)]\n\n\n\n# A tibble: 4 × 4\n  arrival_date_month arrival_date_week_number arrival_date_day_of_month hotel   \n  <chr>                                 <int>                     <int> <chr>   \n1 July                                     27                         1 Resort …\n2 July                                     27                         1 Resort …\n3 July                                     27                         1 Resort …\n4 October                                  44                        23 City Ho…"
  },
  {
    "objectID": "W03.html#python-is-0-indexed-r-is-1-indexed",
    "href": "W03.html#python-is-0-indexed-r-is-1-indexed",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "python is 0-indexed, R is 1-indexed!",
    "text": "python is 0-indexed, R is 1-indexed!\npython: indexes go from 0 to n-1\nR: indexes go from 1 to n\nBe aware!\nNote: There is no correct way. For some use cases one is more natural for others the other.\nAnalogy: In mathematics there is an unsettled debate if \\(0 \\in \\mathbb{N}\\) or \\(0 \\notin \\mathbb{N}\\)"
  },
  {
    "objectID": "W03.html#excursion-the-concept-of-logical-indexing",
    "href": "W03.html#excursion-the-concept-of-logical-indexing",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Excursion: The Concept of Logical Indexing",
    "text": "Excursion: The Concept of Logical Indexing\nWith logical vectors you can select rows and columns\n\n\n\n\n\ndata <- tibble(x = LETTERS[1:5], y = letters[6:10])\ndata\n\n\n# A tibble: 5 × 2\n  x     y    \n  <chr> <chr>\n1 A     f    \n2 B     g    \n3 C     h    \n4 D     i    \n5 E     j    \n\n\n\n\n\n\ndata[c(TRUE,FALSE,TRUE,FALSE,TRUE),c(TRUE,FALSE)]\n\n\n# A tibble: 3 × 1\n  x    \n  <chr>\n1 A    \n2 C    \n3 E"
  },
  {
    "objectID": "W03.html#logical-vectors-from-conditional-statements",
    "href": "W03.html#logical-vectors-from-conditional-statements",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Logical vectors from conditional statements",
    "text": "Logical vectors from conditional statements\n\n\ndata$x\n\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\n\n\n\n\n\ndata$x %in% c(\"C\",\"E\")\n\n\n[1] FALSE FALSE  TRUE FALSE  TRUE\n\n\n\n\n\n\n\ndata[data$x %in% c(\"C\",\"E\"),]\n\n\n# A tibble: 2 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 E     j    \n\n\n\n\n\n\n\ndata[data$x %in% c(\"C\",\"E\") | \n       data$y %in% c(\"h\",\"i\"),]\n\n\n# A tibble: 3 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 D     i    \n3 E     j    \n\n\n\n\n\n\n\ndata |> \n  filter(\n    x %in% c(\"C\",\"E\") | y %in% c(\"h\",\"i\")\n    )\n\n\n# A tibble: 3 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 D     i    \n3 E     j"
  },
  {
    "objectID": "W03.html#unique-combinations-arranging",
    "href": "W03.html#unique-combinations-arranging",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Unique combinations, arranging",
    "text": "Unique combinations, arranging\ndistinct and arrange\n\nhotels |> \n  distinct(hotel, market_segment) |> \n  arrange(hotel, market_segment)\n\n\n\n# A tibble: 14 × 2\n   hotel        market_segment\n   <chr>        <chr>         \n 1 City Hotel   Aviation      \n 2 City Hotel   Complementary \n 3 City Hotel   Corporate     \n 4 City Hotel   Direct        \n 5 City Hotel   Groups        \n 6 City Hotel   Offline TA/TO \n 7 City Hotel   Online TA     \n 8 City Hotel   Undefined     \n 9 Resort Hotel Complementary \n10 Resort Hotel Corporate     \n11 Resort Hotel Direct        \n12 Resort Hotel Groups        \n13 Resort Hotel Offline TA/TO \n14 Resort Hotel Online TA"
  },
  {
    "objectID": "W03.html#counting",
    "href": "W03.html#counting",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Counting",
    "text": "Counting\ncount\n\nhotels |> \n  count(hotel, market_segment) |>      # This produces a new variable n\n  arrange(n)\n\n\n\n# A tibble: 14 × 3\n   hotel        market_segment     n\n   <chr>        <chr>          <int>\n 1 City Hotel   Undefined          2\n 2 Resort Hotel Complementary    201\n 3 City Hotel   Aviation         237\n 4 City Hotel   Complementary    542\n 5 Resort Hotel Corporate       2309\n 6 City Hotel   Corporate       2986\n 7 Resort Hotel Groups          5836\n 8 City Hotel   Direct          6093\n 9 Resort Hotel Direct          6513\n10 Resort Hotel Offline TA/TO   7472\n11 City Hotel   Groups         13975\n12 City Hotel   Offline TA/TO  16747\n13 Resort Hotel Online TA      17729\n14 City Hotel   Online TA      38748"
  },
  {
    "objectID": "W03.html#create-a-new-variable-with-mutate",
    "href": "W03.html#create-a-new-variable-with-mutate",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Create a new variable with mutate",
    "text": "Create a new variable with mutate\n\nhotels |>\n  mutate(little_ones = children + babies) |>\n  select(children, babies, little_ones) |>\n  arrange(desc(little_ones)) # This sorts in descending order. See the big things!\n\n\n\n# A tibble: 119,390 × 3\n   children babies little_ones\n      <int>  <int>       <int>\n 1       10      0          10\n 2        0     10          10\n 3        0      9           9\n 4        2      1           3\n 5        2      1           3\n 6        2      1           3\n 7        3      0           3\n 8        2      1           3\n 9        2      1           3\n10        3      0           3\n# ℹ 119,380 more rows"
  },
  {
    "objectID": "W03.html#more-mutating",
    "href": "W03.html#more-mutating",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "More mutating",
    "text": "More mutating\n\nhotels |>\n  mutate(little_ones = children + babies) |>\n  count(hotel, little_ones) |>\n  mutate(prop = n / sum(n))\n\n\n\n# A tibble: 12 × 4\n   hotel        little_ones     n       prop\n   <chr>              <int> <int>      <dbl>\n 1 City Hotel             0 73923 0.619     \n 2 City Hotel             1  3263 0.0273    \n 3 City Hotel             2  2056 0.0172    \n 4 City Hotel             3    82 0.000687  \n 5 City Hotel             9     1 0.00000838\n 6 City Hotel            10     1 0.00000838\n 7 City Hotel            NA     4 0.0000335 \n 8 Resort Hotel           0 36131 0.303     \n 9 Resort Hotel           1  2183 0.0183    \n10 Resort Hotel           2  1716 0.0144    \n11 Resort Hotel           3    29 0.000243  \n12 Resort Hotel          10     1 0.00000838"
  },
  {
    "objectID": "W03.html#summarizing",
    "href": "W03.html#summarizing",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Summarizing",
    "text": "Summarizing\n\nhotels |>\n  summarize(mean_adr = mean(adr))\n\n\n\n# A tibble: 1 × 1\n  mean_adr\n     <dbl>\n1     102.\n\n\n\nThat shrinks the data frame to one row!\nDon’t forget to name the new variable (here mean_adr)\nYou can use any function you can apply to a vector!\n(Sometimes you may need to write your own one.)\n\n\n\nIn hoteling, ADR is the average daily rate, the average daily rental income per paid occupied room. A performce indicator."
  },
  {
    "objectID": "W03.html#grouped-operations",
    "href": "W03.html#grouped-operations",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Grouped operations",
    "text": "Grouped operations\n\nhotels |>\n  group_by(hotel) |>\n  summarise(mean_adr = mean(adr))\n\n\n\n# A tibble: 2 × 2\n  hotel        mean_adr\n  <chr>           <dbl>\n1 City Hotel      105. \n2 Resort Hotel     95.0\n\n\nLook at the grouping attributes:\n\nhotels |>\n  group_by(hotel)\n\n\n\n# A tibble: 119,390 × 32\n# Groups:   hotel [2]\n   hotel        is_canceled lead_time arrival_date_year arrival_date_month\n   <chr>        <lgl>           <int>             <int> <chr>             \n 1 Resort Hotel FALSE             342              2015 July              \n 2 Resort Hotel FALSE             737              2015 July              \n 3 Resort Hotel FALSE               7              2015 July              \n 4 Resort Hotel FALSE              13              2015 July              \n 5 Resort Hotel FALSE              14              2015 July              \n 6 Resort Hotel FALSE              14              2015 July              \n 7 Resort Hotel FALSE               0              2015 July              \n 8 Resort Hotel FALSE               9              2015 July              \n 9 Resort Hotel TRUE               85              2015 July              \n10 Resort Hotel TRUE               75              2015 July              \n# ℹ 119,380 more rows\n# ℹ 27 more variables: arrival_date_week_number <int>,\n#   arrival_date_day_of_month <int>, stays_in_weekend_nights <int>,\n#   stays_in_week_nights <int>, adults <int>, children <int>, babies <int>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <lgl>,\n#   previous_cancellations <int>, previous_bookings_not_canceled <int>, …"
  },
  {
    "objectID": "W03.html#grouping-summarizing-visualizing",
    "href": "W03.html#grouping-summarizing-visualizing",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Grouping, summarizing, visualizing",
    "text": "Grouping, summarizing, visualizing\n\nhotels |>\n  group_by(hotel, arrival_date_week_number) |>\n  summarise(mean_adr = mean(adr)) |> \n  ggplot(aes(x = arrival_date_week_number, y = mean_adr, color = hotel)) +\n  geom_line()"
  },
  {
    "objectID": "W03.html#resources",
    "href": "W03.html#resources",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Resources",
    "text": "Resources\n\nFor systemic understanding: Learning resources linked in the syllabus\n\nR for Data Science\nPython Data Science Handbook\n\nFor quick overview to get inspiration\n\nCheatsheets (find some in RStudio -> Help, others by google)\n\nggplot2 Cheatsheet\ndplyr Cheatsheet\n\n\nFor detailed help with a function\n\nHelp file of the function ?FUNCTION-NAME, or search box in Help tab\nReference page on the package webpage\n\nTalk to ChatGPT? Does it work?"
  },
  {
    "objectID": "W03.html#named-vectors",
    "href": "W03.html#named-vectors",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Named vectors",
    "text": "Named vectors\nAll types of vectors can be named upon creation\n\nc(Num1 = 4, Second = 7, Last = 8)\n\n\n\n  Num1 Second   Last \n     4      7      8 \n\n\n\nor names can be set afterward.\n\nx <- 1:4\ny <- set_names(x, c(\"a\",\"b\",\"c\",\"d\"))\ny\n\n\n\na b c d \n1 2 3 4 \n\n\n\n\nNamed vectors can be used for subsetting.\n\ny[c(\"b\",\"d\")]\n\n\n\nb d \n2 4"
  },
  {
    "objectID": "W03.html#reminder-indexing-and-vectorized-thinking",
    "href": "W03.html#reminder-indexing-and-vectorized-thinking",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Reminder: Indexing and vectorized thinking",
    "text": "Reminder: Indexing and vectorized thinking\n\nx <- set_names(1:10,LETTERS[1:10])\nx\n\n\n\n A  B  C  D  E  F  G  H  I  J \n 1  2  3  4  5  6  7  8  9 10 \n\n\n\n\nx[c(4,2,1,1,1,1,4,1,5)]\n\n\n\nD B A A A A D A E \n4 2 1 1 1 1 4 1 5 \n\n\n\n\nRemoving with negative index numbers.\n\nx[c(-3,-5,-2)]\n\n\n\n A  D  F  G  H  I  J \n 1  4  6  7  8  9 10 \n\n\n\n\nMixing does not work.\nx[c(-3,1)]  # Will throw an error"
  },
  {
    "objectID": "W03.html#r-objects-can-have-attributes",
    "href": "W03.html#r-objects-can-have-attributes",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "R objects can have attributes",
    "text": "R objects can have attributes\nIn a named vector, the names are an attribute.\n\nx\n\n A  B  C  D  E  F  G  H  I  J \n 1  2  3  4  5  6  7  8  9 10 \n\nattributes(x)\n\n$names\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\n\n\nAttributes can be assigned freely.\n\nattr(x, \"SayHi\") <- \"Hi\"\nattr(x, \"SayBye\") <- \"Bye\"\nattributes(x)\n\n\n\n$names\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\n$SayHi\n[1] \"Hi\"\n\n$SayBye\n[1] \"Bye\""
  },
  {
    "objectID": "W03.html#attributes-in-data-structures",
    "href": "W03.html#attributes-in-data-structures",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Attributes in data structures",
    "text": "Attributes in data structures\n\nlibrary(nycflights13)\nattributes(airports)\n\n\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n   [1]    1    2    3    4    5    6    7    8    9   10   11   12   13   14\n  [15]   15   16   17   18   19   20   21   22   23   24   25   26   27   28\n  [29]   29   30   31   32   33   34   35   36   37   38   39   40   41   42\n  [43]   43   44   45   46   47   48   49   50   51   52   53   54   55   56\n  [57]   57   58   59   60   61   62   63   64   65   66   67   68   69   70\n  [71]   71   72   73   74   75   76   77   78   79   80   81   82   83   84\n  [85]   85   86   87   88   89   90   91   92   93   94   95   96   97   98\n  [99]   99  100  101  102  103  104  105  106  107  108  109  110  111  112\n [113]  113  114  115  116  117  118  119  120  121  122  123  124  125  126\n [127]  127  128  129  130  131  132  133  134  135  136  137  138  139  140\n [141]  141  142  143  144  145  146  147  148  149  150  151  152  153  154\n [155]  155  156  157  158  159  160  161  162  163  164  165  166  167  168\n [169]  169  170  171  172  173  174  175  176  177  178  179  180  181  182\n [183]  183  184  185  186  187  188  189  190  191  192  193  194  195  196\n [197]  197  198  199  200  201  202  203  204  205  206  207  208  209  210\n [211]  211  212  213  214  215  216  217  218  219  220  221  222  223  224\n [225]  225  226  227  228  229  230  231  232  233  234  235  236  237  238\n [239]  239  240  241  242  243  244  245  246  247  248  249  250  251  252\n [253]  253  254  255  256  257  258  259  260  261  262  263  264  265  266\n [267]  267  268  269  270  271  272  273  274  275  276  277  278  279  280\n [281]  281  282  283  284  285  286  287  288  289  290  291  292  293  294\n [295]  295  296  297  298  299  300  301  302  303  304  305  306  307  308\n [309]  309  310  311  312  313  314  315  316  317  318  319  320  321  322\n [323]  323  324  325  326  327  328  329  330  331  332  333  334  335  336\n [337]  337  338  339  340  341  342  343  344  345  346  347  348  349  350\n [351]  351  352  353  354  355  356  357  358  359  360  361  362  363  364\n [365]  365  366  367  368  369  370  371  372  373  374  375  376  377  378\n [379]  379  380  381  382  383  384  385  386  387  388  389  390  391  392\n [393]  393  394  395  396  397  398  399  400  401  402  403  404  405  406\n [407]  407  408  409  410  411  412  413  414  415  416  417  418  419  420\n [421]  421  422  423  424  425  426  427  428  429  430  431  432  433  434\n [435]  435  436  437  438  439  440  441  442  443  444  445  446  447  448\n [449]  449  450  451  452  453  454  455  456  457  458  459  460  461  462\n [463]  463  464  465  466  467  468  469  470  471  472  473  474  475  476\n [477]  477  478  479  480  481  482  483  484  485  486  487  488  489  490\n [491]  491  492  493  494  495  496  497  498  499  500  501  502  503  504\n [505]  505  506  507  508  509  510  511  512  513  514  515  516  517  518\n [519]  519  520  521  522  523  524  525  526  527  528  529  530  531  532\n [533]  533  534  535  536  537  538  539  540  541  542  543  544  545  546\n [547]  547  548  549  550  551  552  553  554  555  556  557  558  559  560\n [561]  561  562  563  564  565  566  567  568  569  570  571  572  573  574\n [575]  575  576  577  578  579  580  581  582  583  584  585  586  587  588\n [589]  589  590  591  592  593  594  595  596  597  598  599  600  601  602\n [603]  603  604  605  606  607  608  609  610  611  612  613  614  615  616\n [617]  617  618  619  620  621  622  623  624  625  626  627  628  629  630\n [631]  631  632  633  634  635  636  637  638  639  640  641  642  643  644\n [645]  645  646  647  648  649  650  651  652  653  654  655  656  657  658\n [659]  659  660  661  662  663  664  665  666  667  668  669  670  671  672\n [673]  673  674  675  676  677  678  679  680  681  682  683  684  685  686\n [687]  687  688  689  690  691  692  693  694  695  696  697  698  699  700\n [701]  701  702  703  704  705  706  707  708  709  710  711  712  713  714\n [715]  715  716  717  718  719  720  721  722  723  724  725  726  727  728\n [729]  729  730  731  732  733  734  735  736  737  738  739  740  741  742\n [743]  743  744  745  746  747  748  749  750  751  752  753  754  755  756\n [757]  757  758  759  760  761  762  763  764  765  766  767  768  769  770\n [771]  771  772  773  774  775  776  777  778  779  780  781  782  783  784\n [785]  785  786  787  788  789  790  791  792  793  794  795  796  797  798\n [799]  799  800  801  802  803  804  805  806  807  808  809  810  811  812\n [813]  813  814  815  816  817  818  819  820  821  822  823  824  825  826\n [827]  827  828  829  830  831  832  833  834  835  836  837  838  839  840\n [841]  841  842  843  844  845  846  847  848  849  850  851  852  853  854\n [855]  855  856  857  858  859  860  861  862  863  864  865  866  867  868\n [869]  869  870  871  872  873  874  875  876  877  878  879  880  881  882\n [883]  883  884  885  886  887  888  889  890  891  892  893  894  895  896\n [897]  897  898  899  900  901  902  903  904  905  906  907  908  909  910\n [911]  911  912  913  914  915  916  917  918  919  920  921  922  923  924\n [925]  925  926  927  928  929  930  931  932  933  934  935  936  937  938\n [939]  939  940  941  942  943  944  945  946  947  948  949  950  951  952\n [953]  953  954  955  956  957  958  959  960  961  962  963  964  965  966\n [967]  967  968  969  970  971  972  973  974  975  976  977  978  979  980\n [981]  981  982  983  984  985  986  987  988  989  990  991  992  993  994\n [995]  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008\n[1009] 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022\n[1023] 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036\n[1037] 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050\n[1051] 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064\n[1065] 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078\n[1079] 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092\n[1093] 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106\n[1107] 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120\n[1121] 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134\n[1135] 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148\n[1149] 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162\n[1163] 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176\n[1177] 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190\n[1191] 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204\n[1205] 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218\n[1219] 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232\n[1233] 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246\n[1247] 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260\n[1261] 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274\n[1275] 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288\n[1289] 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302\n[1303] 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316\n[1317] 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330\n[1331] 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344\n[1345] 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358\n[1359] 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372\n[1373] 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386\n[1387] 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400\n[1401] 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414\n[1415] 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428\n[1429] 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442\n[1443] 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456\n[1457] 1457 1458\n\n$spec\ncols(\n  id = col_double(),\n  name = col_character(),\n  city = col_character(),\n  country = col_character(),\n  faa = col_character(),\n  icao = col_character(),\n  lat = col_double(),\n  lon = col_double(),\n  alt = col_double(),\n  tz = col_double(),\n  dst = col_character(),\n  tzone = col_character()\n)\n\n$names\n[1] \"faa\"   \"name\"  \"lat\"   \"lon\"   \"alt\"   \"tz\"    \"dst\"   \"tzone\""
  },
  {
    "objectID": "W03.html#three-important-attributes",
    "href": "W03.html#three-important-attributes",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Three important attributes",
    "text": "Three important attributes\n\nNames are used to name element of a vector, also works for lists and therefore also data frames (lists of atomic vectors of the same length)\nDimensions (dim()) is a short numeric vector making a vector behave as a matrix or a higher dimensional array. A vector 1:6 together with dim being c(2,3) is a matrix with 2 rows and 3 columns\n\\(\\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{bmatrix}\\)\nClass is used to implement the S3 object oriented system. We don’t need to know the details here. The class system makes it for example possible that the same function, e.g. print() behaves differently for objects of a different class.\n\nClass plays a role in specifying augmented vectors like factors, dates, date-times, or tibbles."
  },
  {
    "objectID": "W03.html#factors",
    "href": "W03.html#factors",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Factors",
    "text": "Factors\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values\n\nx <- factor(c(\"BS\", \"MS\", \"PhD\", \"MS\", \"BS\", \"BS\"))\nx\n\n\n\n[1] BS  MS  PhD MS  BS  BS \nLevels: BS MS PhD\n\n\n\nTechnically, a factor is vector of integers with a levels attribute which specifies the categories for the integers.\n\ntypeof(x)\n\n[1] \"integer\"\n\nas.integer(x)\n\n[1] 1 2 3 2 1 1\n\nattributes(x)\n\n$levels\n[1] \"BS\"  \"MS\"  \"PhD\"\n\n$class\n[1] \"factor\"\n\n\n\n\nThe class factor makes R print the level of each element of the vector instead of the underlying integer."
  },
  {
    "objectID": "W03.html#factors-for-data-visualization",
    "href": "W03.html#factors-for-data-visualization",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Factors for data visualization",
    "text": "Factors for data visualization\nWe manipulate factors with functions from the forcats package of the tidyverse core.\n\nPlotReverseOrder by frequencyRegroup\n\n\n\nmpg |> ggplot(aes(y = manufacturer)) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_rev(manufacturer))) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_rev(fct_infreq(manufacturer)))) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_other(manufacturer, keep = c(\"dodge\", \"toyota\", \"volkswagen\")))) + geom_bar()"
  },
  {
    "objectID": "W03.html#dates",
    "href": "W03.html#dates",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Dates",
    "text": "Dates\n\n\n\nISO 8601 standard for dates: YYYY-MM-DD. Today: 2023-10-30.\nDates in R are numeric vectors that represent the number of days since 1 January 1970.\n\n\ny <- as.Date(\"2020-01-01\"); y\n\n[1] \"2020-01-01\"\n\ntypeof(y)\n\n[1] \"double\"\n\nattributes(y)\n\n$class\n[1] \"Date\"\n\nas.double(y)\n\n[1] 18262\n\nas.double(as.Date(\"1970-01-01\"))\n\n[1] 0\n\nas.double(as.Date(\"1969-01-01\"))\n\n[1] -365\n\n\n\n\nhttps://social.coop/@mattl/110509557203534941"
  },
  {
    "objectID": "W03.html#how-many-days-are-you-old",
    "href": "W03.html#how-many-days-are-you-old",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "How many days are you old?",
    "text": "How many days are you old?\n\n\nSys.Date() - as.Date(\"1976-01-16\") \n\nTime difference of 17454 days\n\n# Sys.Date() gives as the current day your computer is set to"
  },
  {
    "objectID": "W03.html#date-times",
    "href": "W03.html#date-times",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Date-times",
    "text": "Date-times\nFor date-time manipulation use lubridate form the tidyverse.\n\nx <- lubridate::ymd_hm(\"1970-01-01 01:00\")\n# Note: Instead of loading package `pack` to use its function `func` you can also write `pack::func`\n# This works when the package is installed even when not loaded.\nx\n\n[1] \"1970-01-01 01:00:00 UTC\"\n\nattributes(x)\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"UTC\"\n\nas.double(x)\n\n[1] 3600\n\n\nUTC: Coordinated Universal Time. We are in the UTC+1 timezone.\nPOSIXct: Portable Operating System Interface, calendar time. Stores date and time in seconds with the number of seconds beginning at 1 January 1970."
  },
  {
    "objectID": "W03.html#how-many-seconds-are-you-old",
    "href": "W03.html#how-many-seconds-are-you-old",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "How many seconds are you old?",
    "text": "How many seconds are you old?\n\nas.double(lubridate::now()) - \n as.double(lubridate::ymd_hm(\"1976-01-16_12:04\"))\n\n[1] 1508013674"
  },
  {
    "objectID": "W03.html#summary-on-factors-and-dates",
    "href": "W03.html#summary-on-factors-and-dates",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Summary on Factors and Dates",
    "text": "Summary on Factors and Dates\n\nFactors\n\nCan be used to create categorical variables specified by the levels-attribute\nOften used to specify the order of categories. Particularly useful for graphics!\nCan be manipulated with functions from the forcats package\nOften it is sufficient to work with character vectors.\n\nDates and times\n\nDo not shy away from learning to work with dates and times properly!\nTedious to get right when the date format from the data is messy, but it is worth it!\nUse the lubridate package. Usually you just need one command to convert a character vector to a date or date-time vector, but you have to customize correctly.\n\n\nRead the chapter of factors and dates in R for Data Science"
  },
  {
    "objectID": "W03.html#string-modification",
    "href": "W03.html#string-modification",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "String modification",
    "text": "String modification\nWe modify strings with the stringr package from the tidyverse core. All functions from stringr start with str_.\nVery few examples:\n\nc(\"x\",\"y\")\n\n[1] \"x\" \"y\"\n\nstr_c(\"x\",\"y\")\n\n[1] \"xy\"\n\nstr_c(\"x\",\"y\",\"z\", sep=\",\")\n\n[1] \"x,y,z\"\n\nlength(c(\"x\",\"y\",\"z\"))\n\n[1] 3\n\nstr_length(c(\"x\",\"y\",\"z\"))\n\n[1] 1 1 1\n\nstr_length(c(\"This is a string.\",\"z\"))\n\n[1] 17  1"
  },
  {
    "objectID": "W03.html#string-wrangling-with-variable-names",
    "href": "W03.html#string-wrangling-with-variable-names",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "String wrangling with variable names",
    "text": "String wrangling with variable names\n\ndata <- tibble(Name = c(\"A\",\"B\",\"C\"), Age_2020 = c(20,30,40), Age_2021 = c(21,31,41), Age_2022 = c(22,32,42))\ndata\n\n# A tibble: 3 × 4\n  Name  Age_2020 Age_2021 Age_2022\n  <chr>    <dbl>    <dbl>    <dbl>\n1 A           20       21       22\n2 B           30       31       32\n3 C           40       41       42\n\n\nWe tidy that data set by creating a year variable.\n\n\ndata |> pivot_longer(c(\"Age_2020\", \"Age_2021\", \"Age_2022\"), names_to = \"Year\", values_to=\"Age\")\n\n\n\n# A tibble: 9 × 3\n  Name  Year       Age\n  <chr> <chr>    <dbl>\n1 A     Age_2020    20\n2 A     Age_2021    21\n3 A     Age_2022    22\n4 B     Age_2020    30\n5 B     Age_2021    31\n6 B     Age_2022    32\n7 C     Age_2020    40\n8 C     Age_2021    41\n9 C     Age_2022    42\n\n\n\n\nOK, but the year variable is a string but we want numbers."
  },
  {
    "objectID": "W03.html#use-word",
    "href": "W03.html#use-word",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Use word",
    "text": "Use word\nword extracts words from a sentence. However, the separator need not be \" \" but can be any character.\n\nword(\"This is a string.\", start=2, end=-2) \n\n[1] \"is a\"\n\n#Selects from the second to the second last word.\nword(\"Age_2022\", start=2, sep = \"_\")\n\n[1] \"2022\"\n\n\n\nIt also works vectorized.\n\ndata |> pivot_longer(c(\"Age_2020\", \"Age_2021\", \"Age_2022\"), names_to = \"Year\", values_to=\"Age\") |> \n  mutate(Year = word(Year, start = 2, sep = \"_\") |> as.numeric())\n\n\n\n# A tibble: 9 × 3\n  Name   Year   Age\n  <chr> <dbl> <dbl>\n1 A      2020    20\n2 A      2021    21\n3 A      2022    22\n4 B      2020    30\n5 B      2021    31\n6 B      2022    32\n7 C      2020    40\n8 C      2021    41\n9 C      2022    42"
  },
  {
    "objectID": "W03.html#string-detection-and-regular-expressions",
    "href": "W03.html#string-detection-and-regular-expressions",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "String detection and regular expressions",
    "text": "String detection and regular expressions\n\nfruits <- c(\"apple\", \"pineapple\", \"Pear\", \"orange\", \"peach\", \"banana\")\nstr_detect(fruits,\"apple\")\n\n[1]  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nstr_extract(fruits,\"apple\")\n\n[1] \"apple\" \"apple\" NA      NA      NA      NA     \n\n\nRegular expressions are useful because strings usually contain unstructured or semi-structured data, and regexps are a concise language for describing patterns in strings. When you first look at a regexp, you’ll think a cat walked across your keyboard, but as your understanding improves they will start to make sense.\n\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n\"^[\\w-\\.]+@([\\w-]+\\.)+[\\w-]{2,4}$\"\n\"\"^[[:alnum:].-_]+@[[:alnum:].-]+$\"\"\n\nThese are all regular expressions for email addresses."
  },
  {
    "objectID": "W03.html#special-values",
    "href": "W03.html#special-values",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Special values",
    "text": "Special values\n\nNA: Not available\nNaN: Not a number\nInf: Positive infinity\n-Inf: Negative infinity\n\n\n\n1/0\n\n\n[1] Inf\n\n\n\n\n\n-1/0\n\n\n[1] -Inf\n\n\n\n\n\n0/0\n\n\n[1] NaN\n\n\n\n\n\n1/0 + 1/0\n\n\n[1] Inf\n\n\n\n\n\n1/0 - 1/0\n\n\n[1] NaN"
  },
  {
    "objectID": "W03.html#nas",
    "href": "W03.html#nas",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "NAs",
    "text": "NAs\nInstead of NaN, NA stands for genuinely unknown values.\nIt can also be in a character of logical vector.\n\nx = c(1, 2, 3, 4, NA)\nmean(x)\n\n[1] NA\n\nmean(x, na.rm = TRUE)\n\n[1] 2.5\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    1.75    2.50    2.50    3.25    4.00       1 \n\n\nThe type of NA is logical.\n\ntypeof(NA)\n\n[1] \"logical\"\n\ntypeof(NaN)\n\n[1] \"double\"\n\n\nDoes it make sense?"
  },
  {
    "objectID": "W03.html#nas-in-logical-operations",
    "href": "W03.html#nas-in-logical-operations",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "NAs in logical operations",
    "text": "NAs in logical operations\nNA can be TRUE or FALSE.\nUsually operations including NA results again in NA, but some not!\n\n\nNA & TRUE\n\n\n[1] NA\n\n\n\n\n\nNA | TRUE\n\n\n[1] TRUE\n\n\n\n\n\nNA & FALSE\n\n\n[1] FALSE\n\n\n\n\n\nNA | FALSE\n\n\n[1] NA\n\n\n\nUnderstanding logical operations is important!"
  },
  {
    "objectID": "W03.html#null-is-the-null-object",
    "href": "W03.html#null-is-the-null-object",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "NULL is the null object",
    "text": "NULL is the null object\n\nused to represent lists with zero length\n\n\nx <- 1:10\nattributes(x)\n\nNULL\n\n\n\nused as a placeholder for missing values in lists and data frames\n\n\nL <- list(a = 1)\nL[[3]] <- 5\nL\n\n$a\n[1] 1\n\n[[2]]\nNULL\n\n[[3]]\n[1] 5"
  },
  {
    "objectID": "W03.html#working-with-more-data-frames",
    "href": "W03.html#working-with-more-data-frames",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Working with more data frames",
    "text": "Working with more data frames\n\nData can be distributed in several data frames which have relations which each other.\nFor example, they share variables as the five data frames in nycflights13.\n\n\n\n\nOften variables in different data frame have the same name, but that need not be the case! See the variable faa in airports matches origin and dest in flights."
  },
  {
    "objectID": "W03.html#data-women-in-science",
    "href": "W03.html#data-women-in-science",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Data: Women in science",
    "text": "Data: Women in science\n10 women in science who changed the world: Ada Lovelace, Marie Curie, Janaki Ammal, Chien-Shiung Wu, Katherine Johnson, Rosalind Franklin, Vera Rubin, Gladys West, Flossie Wong-Staal, Jennifer Doudna\n\n\n\n\nProfessionsDatesWorks\n\n\n\nprofessions <- read_csv(\"data/scientists/professions.csv\")\nprofessions\n\n# A tibble: 10 × 2\n   name               profession                        \n   <chr>              <chr>                             \n 1 Ada Lovelace       Mathematician                     \n 2 Marie Curie        Physicist and Chemist             \n 3 Janaki Ammal       Botanist                          \n 4 Chien-Shiung Wu    Physicist                         \n 5 Katherine Johnson  Mathematician                     \n 6 Rosalind Franklin  Chemist                           \n 7 Vera Rubin         Astronomer                        \n 8 Gladys West        Mathematician                     \n 9 Flossie Wong-Staal Virologist and Molecular Biologist\n10 Jennifer Doudna    Biochemist                        \n\n\n\n\n\ndates <- read_csv(\"data/scientists/dates.csv\")\ndates\n\n# A tibble: 8 × 3\n  name               birth_year death_year\n  <chr>                   <dbl>      <dbl>\n1 Janaki Ammal             1897       1984\n2 Chien-Shiung Wu          1912       1997\n3 Katherine Johnson        1918       2020\n4 Rosalind Franklin        1920       1958\n5 Vera Rubin               1928       2016\n6 Gladys West              1930         NA\n7 Flossie Wong-Staal       1947         NA\n8 Jennifer Doudna          1964         NA\n\n\n\n\n\nworks <- read_csv(\"data/scientists/works.csv\")\nworks\n\n# A tibble: 9 × 2\n  name               known_for                                                  \n  <chr>              <chr>                                                      \n1 Ada Lovelace       first computer algorithm                                   \n2 Marie Curie        theory of radioactivity,  discovery of elements polonium a…\n3 Janaki Ammal       hybrid species, biodiversity protection                    \n4 Chien-Shiung Wu    confim and refine theory of radioactive beta decy, Wu expe…\n5 Katherine Johnson  calculations of orbital mechanics critical to sending the …\n6 Vera Rubin         existence of dark matter                                   \n7 Gladys West        mathematical modeling of the shape of the Earth which serv…\n8 Flossie Wong-Staal first scientist to clone HIV and create a map of its genes…\n9 Jennifer Doudna    one of the primary developers of CRISPR, a ground-breaking…\n\n\n\n\n\n\n\nSource: Discover Magazine\nThe data can be downloaded: professions.csv, dates.csv, works.csv"
  },
  {
    "objectID": "W03.html#we-want-this-data-frame",
    "href": "W03.html#we-want-this-data-frame",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "We want this data frame",
    "text": "We want this data frame\n\n\n# A tibble: 10 × 5\n   name               profession                 birth_year death_year known_for\n   <chr>              <chr>                           <dbl>      <dbl> <chr>    \n 1 Ada Lovelace       Mathematician                      NA         NA first co…\n 2 Marie Curie        Physicist and Chemist              NA         NA theory o…\n 3 Janaki Ammal       Botanist                         1897       1984 hybrid s…\n 4 Chien-Shiung Wu    Physicist                        1912       1997 confim a…\n 5 Katherine Johnson  Mathematician                    1918       2020 calculat…\n 6 Rosalind Franklin  Chemist                          1920       1958 <NA>     \n 7 Vera Rubin         Astronomer                       1928       2016 existenc…\n 8 Gladys West        Mathematician                    1930         NA mathemat…\n 9 Flossie Wong-Staal Virologist and Molecular …       1947         NA first sc…\n10 Jennifer Doudna    Biochemist                       1964         NA one of t…"
  },
  {
    "objectID": "W03.html#joining-data-frames",
    "href": "W03.html#joining-data-frames",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Joining data frames",
    "text": "Joining data frames\nsomething_join(x, y)1 for data frames x and y which have a relation\n\nleft_join(): all rows from x\nright_join(): all rows from y\nfull_join(): all rows from both x and y\ninner_join(): all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches\n…\n\nThe notion join comes from SQL database. In other data manipulation frameworks joining is called merging."
  },
  {
    "objectID": "W03.html#simple-setup-for-x-and-y",
    "href": "W03.html#simple-setup-for-x-and-y",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Simple setup for x and y",
    "text": "Simple setup for x and y\n\nx <- tibble(\n  id = c(1, 2, 3),\n  value_x = c(\"x1\", \"x2\", \"x3\")\n  )\ny <- tibble(\n  id = c(1, 2, 4),\n  value_y = c(\"y1\", \"y2\", \"y4\")\n  )\nx\n\n# A tibble: 3 × 2\n     id value_x\n  <dbl> <chr>  \n1     1 x1     \n2     2 x2     \n3     3 x3     \n\ny\n\n# A tibble: 3 × 2\n     id value_y\n  <dbl> <chr>  \n1     1 y1     \n2     2 y2     \n3     4 y4"
  },
  {
    "objectID": "W03.html#left_join",
    "href": "W03.html#left_join",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "left_join()",
    "text": "left_join()\n\n\n\n\n\nleft_join(x, y)\n\n# A tibble: 3 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     3 x3      <NA>"
  },
  {
    "objectID": "W03.html#right_join",
    "href": "W03.html#right_join",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "right_join()",
    "text": "right_join()\n\n\n\n\n\nright_join(x, y)\n\n# A tibble: 3 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     4 <NA>    y4"
  },
  {
    "objectID": "W03.html#full_join",
    "href": "W03.html#full_join",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "full_join()",
    "text": "full_join()\n\n\n\n\n\nfull_join(x, y)\n\n# A tibble: 4 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     3 x3      <NA>   \n4     4 <NA>    y4"
  },
  {
    "objectID": "W03.html#inner_join",
    "href": "W03.html#inner_join",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "inner_join()",
    "text": "inner_join()\n\n\n\n\n\ninner_join(x, y)\n\n# A tibble: 2 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2"
  },
  {
    "objectID": "W03.html#women-in-science",
    "href": "W03.html#women-in-science",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Women in science",
    "text": "Women in science\n\nleft_joinright_joinfull_joininner_joinFinal\n\n\n\nprofessions |> left_join(works)\n\n# A tibble: 10 × 3\n   name               profession                         known_for              \n   <chr>              <chr>                              <chr>                  \n 1 Ada Lovelace       Mathematician                      first computer algorit…\n 2 Marie Curie        Physicist and Chemist              theory of radioactivit…\n 3 Janaki Ammal       Botanist                           hybrid species, biodiv…\n 4 Chien-Shiung Wu    Physicist                          confim and refine theo…\n 5 Katherine Johnson  Mathematician                      calculations of orbita…\n 6 Rosalind Franklin  Chemist                            <NA>                   \n 7 Vera Rubin         Astronomer                         existence of dark matt…\n 8 Gladys West        Mathematician                      mathematical modeling …\n 9 Flossie Wong-Staal Virologist and Molecular Biologist first scientist to clo…\n10 Jennifer Doudna    Biochemist                         one of the primary dev…\n\n\n\n\n\nprofessions |> right_join(works)\n\n# A tibble: 9 × 3\n  name               profession                         known_for               \n  <chr>              <chr>                              <chr>                   \n1 Ada Lovelace       Mathematician                      first computer algorithm\n2 Marie Curie        Physicist and Chemist              theory of radioactivity…\n3 Janaki Ammal       Botanist                           hybrid species, biodive…\n4 Chien-Shiung Wu    Physicist                          confim and refine theor…\n5 Katherine Johnson  Mathematician                      calculations of orbital…\n6 Vera Rubin         Astronomer                         existence of dark matter\n7 Gladys West        Mathematician                      mathematical modeling o…\n8 Flossie Wong-Staal Virologist and Molecular Biologist first scientist to clon…\n9 Jennifer Doudna    Biochemist                         one of the primary deve…\n\n\n\n\n\ndates |> full_join(works)\n\n# A tibble: 10 × 4\n   name               birth_year death_year known_for                           \n   <chr>                   <dbl>      <dbl> <chr>                               \n 1 Janaki Ammal             1897       1984 hybrid species, biodiversity protec…\n 2 Chien-Shiung Wu          1912       1997 confim and refine theory of radioac…\n 3 Katherine Johnson        1918       2020 calculations of orbital mechanics c…\n 4 Rosalind Franklin        1920       1958 <NA>                                \n 5 Vera Rubin               1928       2016 existence of dark matter            \n 6 Gladys West              1930         NA mathematical modeling of the shape …\n 7 Flossie Wong-Staal       1947         NA first scientist to clone HIV and cr…\n 8 Jennifer Doudna          1964         NA one of the primary developers of CR…\n 9 Ada Lovelace               NA         NA first computer algorithm            \n10 Marie Curie                NA         NA theory of radioactivity,  discovery…\n\n\n\n\n\ndates |> inner_join(works)\n\n# A tibble: 7 × 4\n  name               birth_year death_year known_for                            \n  <chr>                   <dbl>      <dbl> <chr>                                \n1 Janaki Ammal             1897       1984 hybrid species, biodiversity protect…\n2 Chien-Shiung Wu          1912       1997 confim and refine theory of radioact…\n3 Katherine Johnson        1918       2020 calculations of orbital mechanics cr…\n4 Vera Rubin               1928       2016 existence of dark matter             \n5 Gladys West              1930         NA mathematical modeling of the shape o…\n6 Flossie Wong-Staal       1947         NA first scientist to clone HIV and cre…\n7 Jennifer Doudna          1964         NA one of the primary developers of CRI…\n\n\n\n\n\nprofessions |> left_join(dates) |> left_join(works)\n\n# A tibble: 10 × 5\n   name               profession                 birth_year death_year known_for\n   <chr>              <chr>                           <dbl>      <dbl> <chr>    \n 1 Ada Lovelace       Mathematician                      NA         NA first co…\n 2 Marie Curie        Physicist and Chemist              NA         NA theory o…\n 3 Janaki Ammal       Botanist                         1897       1984 hybrid s…\n 4 Chien-Shiung Wu    Physicist                        1912       1997 confim a…\n 5 Katherine Johnson  Mathematician                    1918       2020 calculat…\n 6 Rosalind Franklin  Chemist                          1920       1958 <NA>     \n 7 Vera Rubin         Astronomer                       1928       2016 existenc…\n 8 Gladys West        Mathematician                    1930         NA mathemat…\n 9 Flossie Wong-Staal Virologist and Molecular …       1947         NA first sc…\n10 Jennifer Doudna    Biochemist                       1964         NA one of t…"
  },
  {
    "objectID": "W03.html#keys",
    "href": "W03.html#keys",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Keys",
    "text": "Keys\n\nA key is a variable or a set of variables which uniquely identifies observations\nWhat was the key in the data frame of women in science?\n\n\n\nSwitching back to nycflights13 as example\nIn simple cases, a single variable is sufficient to identify an observation, e.g. each plane in planes is identified by tailnum.\nSometimes, multiple variables are needed; e.g. to identify an observation in weather you need five variables: year, month, day, hour, and origin"
  },
  {
    "objectID": "W03.html#how-can-we-check",
    "href": "W03.html#how-can-we-check",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "How can we check?",
    "text": "How can we check?\nCounting observation and filter those more than one\n\nlibrary(nycflights13)\nplanes |> count(tailnum) |> filter(n > 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tailnum <chr>, n <int>\n\nweather |> count(year, month, day, hour, origin) |> filter(n > 1) \n\n# A tibble: 3 × 6\n   year month   day  hour origin     n\n  <int> <int> <int> <int> <chr>  <int>\n1  2013    11     3     1 EWR        2\n2  2013    11     3     1 JFK        2\n3  2013    11     3     1 LGA        2\n\n# OK, here 3 observations are twice. Probably a data error.\n# Example: Without hour it is not a key\nweather |> count(year, month, day, origin) |> filter(n > 1) \n\n# A tibble: 1,092 × 5\n    year month   day origin     n\n   <int> <int> <int> <chr>  <int>\n 1  2013     1     1 EWR       22\n 2  2013     1     1 JFK       22\n 3  2013     1     1 LGA       23\n 4  2013     1     2 EWR       24\n 5  2013     1     2 JFK       24\n 6  2013     1     2 LGA       24\n 7  2013     1     3 EWR       24\n 8  2013     1     3 JFK       24\n 9  2013     1     3 LGA       24\n10  2013     1     4 EWR       24\n# ℹ 1,082 more rows"
  },
  {
    "objectID": "W03.html#terminology-primary-and-foreign-keys",
    "href": "W03.html#terminology-primary-and-foreign-keys",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Terminology: Primary and foreign keys",
    "text": "Terminology: Primary and foreign keys\n\nA primary key uniquely identifies an observation in its own table. E.g, planes$tailnum in planes.\nA foreign key uniquely identifies an observation in another data frame E.g. flights$tailnum is a foreign key in flights because it matches each flight to a unique plane in planes.\nData frames need not have a key and the joins will still do their work.\nA primary key and a foreign key form a relation.\nRelations are typically 1-to-many. Each plane has many flights\nRelations can also be many-to-many. Airlines can fly to many airports; airport can host many airplanes."
  },
  {
    "objectID": "W03.html#joining-when-key-names-differ",
    "href": "W03.html#joining-when-key-names-differ",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Joining when key names differ?",
    "text": "Joining when key names differ?\nWe have to specify the key relation with a named vector in the by argument.\n\ndim(flights)\n\n[1] 336776     19\n\nflights |> left_join(airports, by = c(\"dest\" = \"faa\"))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 18 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>, name <chr>, lat <dbl>,\n#   lon <dbl>, alt <dbl>, tz <dbl>, dst <chr>, tzone <chr>\n\n# New version\nflights |> left_join(airports, join_by(\"dest\" == \"faa\"))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 18 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>, name <chr>, lat <dbl>,\n#   lon <dbl>, alt <dbl>, tz <dbl>, dst <chr>, tzone <chr>\n\n\nWhy does the number of rows stays the same after joining?\n\nfaa is a primary key in airports."
  },
  {
    "objectID": "W03.html#left_join-essentially-right_join-with-switched-data-frames",
    "href": "W03.html#left_join-essentially-right_join-with-switched-data-frames",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "left_join essentially right_join with switched data frames",
    "text": "left_join essentially right_join with switched data frames\n\nairports_right_flights <- airports |> right_join(flights, by = c(\"faa\" = \"dest\"))\nairports_right_flights \n\n# A tibble: 336,776 × 26\n   faa   name       lat   lon   alt    tz dst   tzone  year month   day dep_time\n   <chr> <chr>    <dbl> <dbl> <dbl> <dbl> <chr> <chr> <int> <int> <int>    <int>\n 1 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     1     1955\n 2 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     2     2010\n 3 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     3     1955\n 4 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     4     2017\n 5 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     5     1959\n 6 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     6     1959\n 7 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     7     2002\n 8 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     8     1957\n 9 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     9     1957\n10 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10    10     2011\n# ℹ 336,766 more rows\n# ℹ 14 more variables: sched_dep_time <int>, dep_delay <dbl>, arr_time <int>,\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\nDifferences\n\nIn a join where keys have different column names the name of the first data frame survives (unless you use keep = TRUE). Here, faa instead of dest\nThe columns from the first data frame come first\nThe order of rows is taken from the first data frame, while duplication and dropping of variables is determined by the second data frame (because it is a right_join)\n\nUsing the fact that flights seem to be ordered by year, month, day, dep_time we can re-arrange:\n\nairports_right_flights |> \n  rename(dest = faa) |> \n  select(names(flights)) |> # Use order of flights\n  arrange(year, month, day, dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      924            917\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n\nNote of caution: A deeper analysis shows that the order is still not exactly the same."
  },
  {
    "objectID": "W03.html#left_join-with-reversed-data-frames",
    "href": "W03.html#left_join-with-reversed-data-frames",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "left_join with reversed data frames",
    "text": "left_join with reversed data frames\n\ndim(airports)\n\n[1] 1458    8\n\ndim(flights)\n\n[1] 336776     19\n\nairports |> \n  left_join(flights, by = c(\"faa\" = \"dest\"))\n\n# A tibble: 330,531 × 26\n   faa   name      lat    lon   alt    tz dst   tzone  year month   day dep_time\n   <chr> <chr>   <dbl>  <dbl> <dbl> <dbl> <chr> <chr> <int> <int> <int>    <int>\n 1 04G   Lansdo…  41.1  -80.6  1044    -5 A     Amer…    NA    NA    NA       NA\n 2 06A   Moton …  32.5  -85.7   264    -6 A     Amer…    NA    NA    NA       NA\n 3 06C   Schaum…  42.0  -88.1   801    -6 A     Amer…    NA    NA    NA       NA\n 4 06N   Randal…  41.4  -74.4   523    -5 A     Amer…    NA    NA    NA       NA\n 5 09J   Jekyll…  31.1  -81.4    11    -5 A     Amer…    NA    NA    NA       NA\n 6 0A9   Elizab…  36.4  -82.2  1593    -5 A     Amer…    NA    NA    NA       NA\n 7 0G6   Willia…  41.5  -84.5   730    -5 A     Amer…    NA    NA    NA       NA\n 8 0G7   Finger…  42.9  -76.8   492    -5 A     Amer…    NA    NA    NA       NA\n 9 0P2   Shoest…  39.8  -76.6  1000    -5 U     Amer…    NA    NA    NA       NA\n10 0S9   Jeffer…  48.1 -123.    108    -8 A     Amer…    NA    NA    NA       NA\n# ℹ 330,521 more rows\n# ℹ 14 more variables: sched_dep_time <int>, dep_delay <dbl>, arr_time <int>,\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\nWhy does the number of rows changes after joining?\ndest is not a primary key in flights. There are more flights with the same destination so rows of airports get duplicated.\nWhy is the number of rows then less than the number of rows in flights?\nLet us do some checks:\n\nlength(unique(airports$faa)) # Unique turns out to be redundant because faa is a primary key\n\n[1] 1458\n\nlength(unique(flights$dest))\n\n[1] 105\n\n# There are much more airports then destinations in flights!\n# ... but the rows of airports prevail when it is the first in a left_join.\n# So, the data frame should even increase because \n# we get several rows of airports without flights\n# Let us dig deeper.\n\nsetdiff( unique(airports$faa), unique(flights$dest)) |> length()\n\n[1] 1357\n\n# 1,357 airports have no flights. But also:\nsetdiff( unique(flights$dest), unique(airports$faa)) |> length()\n\n[1] 4\n\n# There are four destinations in flights, which are not in the airports list!\n\n# How many flights are to these?\nflights |> \n  filter(dest %in% setdiff( unique(flights$dest), unique(airports$faa))) |> \n  nrow()\n\n[1] 7602\n\n# 7,602 flights go to destinations not listed as airport\n\n# Check\nnrow(airports |> left_join(flights, by = c(\"faa\" = \"dest\"))) == nrow(flights) - 7602 + 1357\n\n[1] TRUE\n\n# OK, now we have a clear picture\n# airport with left_joined flights duplicates the rows an airports for each flight flying to it\n# So the total number of rows is the number of flights plus the number of airport which do not \n# appear as a destination minus the flights which go to destinations which are not listed in airports\n\nLearning: The new number of observation after a join can be a complex combination of duplication and dropping. It is your responsibility to understand what is happening."
  },
  {
    "objectID": "W11_projects.html#hw-04-task-1-form-project-teams",
    "href": "W11_projects.html#hw-04-task-1-form-project-teams",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "HW 04 Task 1: Form Project Teams",
    "text": "HW 04 Task 1: Form Project Teams\n✔️ Team formation is mostly complete\n8 repositories are created in\nhttps://github.com/orgs/JU-F22-MDSSB-MET-01/.\nYou are to deliver your project reports in your repository."
  },
  {
    "objectID": "W11_projects.html#hw-04-task-2",
    "href": "W11_projects.html#hw-04-task-2",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "HW 04 Task 2:",
    "text": "HW 04 Task 2:\nStart a document and list data source/topic and draft questions\n\nAn initial document in the repository which lists either\n\na link and brief description to a data source you want to build your project on, or\na topic and three potential questions on that topic you like to answer within your project report.\n\nIdeally, you provide both.\nNone, of these binds you. It can be changed.\nIt is possible to build on either ESS or Corona data. If you want to do this, make sure to find new questions not those covered in Homework or Lectures.\n\n❌ Only two teams pushed a document to the repository. No quarto draft yet."
  },
  {
    "objectID": "W11_projects.html#ok-lets-do-it-together-now",
    "href": "W11_projects.html#ok-lets-do-it-together-now",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "OK, let’s do it together now",
    "text": "OK, let’s do it together now\nThere is a template quarto file here: https://github.com/JU-F22-MDSSB-MET-01/project-TEMPLATE/blob/main/report.qmd\nPlan:\n\nWe clone project repositories now and add the template\nWe go through a push and pull cycle\nWe create and solve a merge conflict\n\nLearning goal:\n\nFirst experiences with collaborative data science work with git"
  },
  {
    "objectID": "W11_projects.html#step-1-git-clone-project-teamname",
    "href": "W11_projects.html#step-1-git-clone-project-teamname",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 1: git clone project-TEAMname",
    "text": "Step 1: git clone project-TEAMname\n\nGo to: https://github.com/orgs/JU-F22-MDSSB-MET-01/\nFind your project repository project-TEAMname\nCopy the URL\n\nGo to RStudio\n\nNew Project > Form Version Control > Git > Paste your URL\nThe project is created, but it is empty"
  },
  {
    "objectID": "W11_projects.html#step-2-download-the-template-file",
    "href": "W11_projects.html#step-2-download-the-template-file",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 2: Download the template file",
    "text": "Step 2: Download the template file\nOnly the first team member needs to do the following: \n\nWho is the first team member? The first one mentioned in the TEAMname.\nExample: In the project project-PoroJan Poro is the first team member\n\nThe first team member copies the template file to project folder locally.\n\nGo to: https://github.com/JU-F22-MDSSB-MET-01/project-TEMPLATE\nClick on the file report.qmd\nClick on “Raw” to see the file content only. Usually, this happens in the browser.\nSelect “Save as” from your browser’s menu and save the file in the project folder on your computer: YOURPATH/project-TEAMname/\ncopy a template quarto file there\none team member commits and pushes the file\nthe other pulls the latest commits"
  },
  {
    "objectID": "W11_projects.html#step-3-first-team-member-commits-and-pushes",
    "href": "W11_projects.html#step-3-first-team-member-commits-and-pushes",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 3: First Team member commits and pushes",
    "text": "Step 3: First Team member commits and pushes\nNow the first team member propagates the template file to the others.\n\nAs the first team member in RStudio:\n\nSelect the document report.qmd in the Git pane to perform git add of the file\nDo a git commit in the RStudio interface. Enter “Add report.qmd” as commit message.\nDo git push in the RStudio interface."
  },
  {
    "objectID": "W11_projects.html#step-4-second-and-third-team-member-pulls",
    "href": "W11_projects.html#step-4-second-and-third-team-member-pulls",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 4: Second (and third) team member pulls",
    "text": "Step 4: Second (and third) team member pulls\n\nWho is the second team member? The first one mentioned in the TEAMname.\nExample: In the project project-PoroJan Jan is the second team member\nAs the second team member in RStudio:\n\nDo git pull in the RStudio interface."
  },
  {
    "objectID": "W11_projects.html#what-does-git-pull-do",
    "href": "W11_projects.html#what-does-git-pull-do",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "What does git pull do?",
    "text": "What does git pull do?\n\nIt first does git fetch which gets the commit from the remote repository (GitHub) to the local machine.\n\nThen it git merges the commit with the latest commit on your local machine.\nWhen we are lucky this works with no problems. (Should be the case with new files.)\n\n\nSource: https://mastodon.social/@allison_horst/109303149552034159"
  },
  {
    "objectID": "W11_projects.html#step-5-merge-independent-changes",
    "href": "W11_projects.html#step-5-merge-independent-changes",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 5: Merge independent changes",
    "text": "Step 5: Merge independent changes\nGit can merge changes in the same file when there are no conflicts. Let’s try.\n\nThe second team member:\n\nChange the title in the YAML to something meaningful.\nSave, add the file in the Git pane, commit with message “Title”, push.\n\nThe first team member:\n\nAdd your name in the author section of the YAML, save the file, add the file in the Git pane and make a commit.\nTry to push. You should receive an error. Read it carefully, often it tells you what to do. Here: Do git pull first. You cannot push because remotely there is a newer commit (the one your colleague just made).\nPull.\nThis should result in message about a successfull auto-merge. Check that both changes are there. If you receive several hints instead, first read the next slide!"
  },
  {
    "objectID": "W11_projects.html#git-configuration-for-divergent-branches",
    "href": "W11_projects.html#git-configuration-for-divergent-branches",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "??? git configuration for divergent branches",
    "text": "??? git configuration for divergent branches\n\n\nIf you pull for the first time in a local git repository, git may complain like this:\n\nRead that carefully. It advises to configure with git config pull.rebase false as the default version.\n\nHow to do the configuration?\n\nCopy the line git config pull.rebase false and close the window.\nGo to the Terminal pane (not the console, the one besides that). This is a terminal not for R but to speak with the computer in general. Paste the command and press enter. Now you are done and your next git pull should work.\n\n\n\n\n\nWhat is a branch and what a rebase? These are features of git well worth to learn but not now. Learn at http://happygitwithr.com"
  },
  {
    "objectID": "W11_projects.html#step-6-push-and-pull-the-other-way-round",
    "href": "W11_projects.html#step-6-push-and-pull-the-other-way-round",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 6: Push and pull the other way round",
    "text": "Step 6: Push and pull the other way round\n\nThe first member:\n\nThe successful merge creates a new commit, which you can directly push.\nPush.\n\nThe second team member:\n\nPull the changes of your colleague.\n\n\nPractice a bit more pulling and pushing commits and check the merging."
  },
  {
    "objectID": "W11_projects.html#step-7-create-a-merge-conflict",
    "href": "W11_projects.html#step-7-create-a-merge-conflict",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 7: Create a merge conflict",
    "text": "Step 7: Create a merge conflict\n\nBoth team members:\n\nChange a word in the title (but different ones)\ngit add and git commit on your local machine.\n\nFirst member: git push\nSecond member:\n\ngit pull. That should result in a conflict. If you receive several hints instead, first read the slide two slides before!\nThe conflict should show directly in the file with markings like this\n\n\n>>>>>>>>\none option of text,\n======== a separator,\nthe other option, and\n<<<<<<<."
  },
  {
    "objectID": "W11_projects.html#step-8-solve-the-conflict",
    "href": "W11_projects.html#step-8-solve-the-conflict",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 8: Solve the conflict",
    "text": "Step 8: Solve the conflict\n\nThe second member\n\nYou have to solve this conflict now!\nSolve is by editing the text\nDecide for an option or make a new text\nThereby, remove the >>>>>,=====,<<<<<<\nWhen you are done: git add, git commit, and git push.\n\n\nNow you know how to solve merge conflicts. Practice a bit in your team.\nWorking in VSCode: The workflow is very similar because it essentially relies on git not on the editor of choice."
  },
  {
    "objectID": "W11_projects.html#advice-collaborative-work-with-git",
    "href": "W11_projects.html#advice-collaborative-work-with-git",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Advice: Collaborative work with git",
    "text": "Advice: Collaborative work with git\n\nWhenever you start a work session: First pull to see if there is anything new. That way you reduce the need for merges.\nInform your colleagues when you pushed new commits.\nCoordinate the work, e.g. discuss who works on what part and maybe when. However, git allows to also work without fully coordination and in parallel.\nWhen you finish your work session, end with a pushing a nice commit. That means. The file should render. You made comments when there are loose ends.\nYou can also use the issues section of the GitHub repository for things to do.\nNote: When you work on different parts of the file, be aware that also a successful merge can create problems. Example: Your colleague changed the data import, while you worked on graphics. Maybe after the merge the imported data is not what you need for your chunk. Then coordinate.\n\nCommit and push often. This avoids that potential merge conflicts become large."
  },
  {
    "objectID": "W11_projects.html#a-project-report-in-a-nutshell",
    "href": "W11_projects.html#a-project-report-in-a-nutshell",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "A project report in a nutshell",
    "text": "A project report in a nutshell\n\nYou pick a dataset,\ndo some interesting question-driven data analysis with it,\nwrite up a well structured and nicely formatted report about the analysis, and\npresent it at the end of the semester."
  },
  {
    "objectID": "W11_projects.html#what-are-good-questions",
    "href": "W11_projects.html#what-are-good-questions",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "What are good questions?",
    "text": "What are good questions?\n\nIs the question specific or too broad?\nCan it be answered (or at least approached) with data?\nDoes the question clearly call for one of the different types of data analysis: descriptive, explanatory, inferential, predictive, causal, or mechanistic? (Note: Questions for causal and mechanistic explanations are not the typical choices for a short term data analysis projects. We have not discussed much on these. They should not be central but may be touched.)"
  },
  {
    "objectID": "W11_projects.html#six-types-of-questions",
    "href": "W11_projects.html#six-types-of-questions",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Six types of questions",
    "text": "Six types of questions\n\n\n\nDescriptive: summarize a characteristic of data\nExploratory: analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\nInferential: analyze patterns, trends, or relationships in representative data from a population\nPredictive: make predictions for individuals or groups of individuals\nCausal: whether changing one factor will change another factor, on average, in a population\nMechanistic: explore “how” as opposed to whether\n\n\n\n\n\n\n\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146."
  },
  {
    "objectID": "W11_projects.html#project-idea-of-one-team",
    "href": "W11_projects.html#project-idea-of-one-team",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Project idea of one team",
    "text": "Project idea of one team\nTitle: Marijuana related Crime\nData: https://www.kaggle.com/datasets/jinbonnie/crime-and-weed\nTopic: Causes and correlations of crimes in the capital of Colorado\n\nGeographical issues. Does location of the district contribute to crime features (like offense_type and offense_category by neighbourhoods)? We may put it onto the city map and understand: Does airport nearby help planting weed, do people do it in their flats or rather in countryhouses etc.\nCriminology issues. Relation between MJ and other types of crimes (e.g. against property or rather violent).\nMachine learning problems: classify whether (i) certrain crime is industry or non-industry type and (ii) certain crime is more likely to be a larceny or a burglary.\n\nBONUS: Temporal issues of the crimes by type. E.g. do some kinds of crimes tend to be discovered later or to be proved as lasting one with more difficulty? Days of week?"
  },
  {
    "objectID": "W11_projects.html#another-teams-project-idea",
    "href": "W11_projects.html#another-teams-project-idea",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Another team’s project idea",
    "text": "Another team’s project idea\nTitle: The climate crisis as seen in temperature rise across the world\nData: https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data\nQuestions:\n\nHow have temperatures risen across the world since 1750?\nWhich regions, countries and cities have seen the most extreme changes in temperature? (E.g. Which cities have registered the hottest temperatures, what is the impact of temperature rising in the Arctic region?)\nHow are fossil fuel consumption and CO2 emissions correlated to temperature changes in different regions? (Note: We may add an additional dataset on fossil fuel and CO2 data per country for this)\nHow closely does a predictive model based on historical data match the temperatures in subsequent years (2016 onwards)"
  },
  {
    "objectID": "W11_projects.html#my-project",
    "href": "W11_projects.html#my-project",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "My project",
    "text": "My project\nQuestion: Do summer school vacations (in Germany) spur or mitigate a pandemic like corona?\nIdea: Summer School vacations differ substantially in time between the 16 Germany federal states. Maybe this helps to isolate a vacation effect from other things like general seasonality.\nDiscuss:\nHypothesis?\nWhat data do I need?\nWhich visualizations would help?\nWhat analysis could I try?"
  },
  {
    "objectID": "W11_projects.html#other-teams",
    "href": "W11_projects.html#other-teams",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Other teams",
    "text": "Other teams\n\nLet us know your first thought\nWhat were your obstacles?"
  },
  {
    "objectID": "W11_projects.html#different-routes-to-setup-your-project",
    "href": "W11_projects.html#different-routes-to-setup-your-project",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Different routes to setup your project",
    "text": "Different routes to setup your project\n\nRandom datasets from the sources mentioned in the homework.\n\n\nPick a data source (at random) and investigate deeper: What are the cases? What are the variables?\nStart to think about potential exploratory and predictive questions on this. If you feel some curiosity – take it. If not, repeat. Try with at least three datasets.\n\n\nTopical data search\n\n\nSelect a topic you are interested in and search for datasets in the sources\nContinue with investigating deeper and thinking about questions as above"
  },
  {
    "objectID": "W11_projects.html#different-routes-to-setup-your-project-1",
    "href": "W11_projects.html#different-routes-to-setup-your-project-1",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Different routes to setup your project",
    "text": "Different routes to setup your project\n\nQuestion driven\n\n\nMaybe you have a fairly precise question (Like vacation and corona.)\nThink about out what data you would need\nFind that data (investigation!). Adjust the question when seeing what is available.\n\n\nDepart from Homework on Corona or ESS (you may download new variables yourself) and develop your own question"
  },
  {
    "objectID": "W11_projects.html#advice",
    "href": "W11_projects.html#advice",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Advice",
    "text": "Advice\n\nA manageable dataset: at least 50 cases, good is 10-20 variables with a mix of categorical and numeric\nGo through the visualizations, statistics, and models we had in lectures and homework and think if similar things would be interesting.\nThe goal is not an exhaustive data analysis. Do not calculate every statistic and procedure you have learned for every variable, but rather show that you can ask meaningful questions and answer them with results of data analysis and proficient interpretation and presentation of the results.\nDo NOT blindly do all visualization and all statistic on all variables in the data set! You do that in your data exploration but that should not go into your report as is.\nA single high quality visualization which shows a point clearly will receive a much higher appreciation than a large number of poor quality visualizations without an explanation what they should communicate."
  },
  {
    "objectID": "W11_projects.html#potential-questions",
    "href": "W11_projects.html#potential-questions",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Potential questions",
    "text": "Potential questions\n\nDo we need to stick to methods and visualizations treated in lectures?\n\nNo, you are invited to use other data analysis and visualization methods (from other courses or packages which you self-learn)! We are happy to give advice if we can.\n\nAre we only allowed to use one dataset?\n\nNo, you can also merge data from different sources. This is a more challenging project because the data wrangling work would be a bit more. If your question calls for it we encourage you to use another data source. The additional effort will be recognized.\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W02.html#git-and-github",
    "href": "W02.html#git-and-github",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Git and GitHub",
    "text": "Git and GitHub\n\n\n\n\nGit the version control system\n\nsomehow like Track Changes in Microsoft Word\nsomehow like “Save as …” for multiple files in a folder (with old versions saved)\n\nDeveloped 2005 by Linus Torvalds to maintain the Linux kernel\n\n\n\n\nGitHub our home for Git-based projects on the internet – a bit like DropBox but for code files\nThe platform for web hosting, collaboration, and as our course management system in this course"
  },
  {
    "objectID": "W02.html#our-git-github-dance",
    "href": "W02.html#our-git-github-dance",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Our git-GitHub dance",
    "text": "Our git-GitHub dance\nHappens with thw GitHub-Organization CU-F23-MDSSB-MET-03-VisCommDataStory\n\n\n\nFrom http://datasciencebox.org/"
  },
  {
    "objectID": "W02.html#our-git-github-dance-1",
    "href": "W02.html#our-git-github-dance-1",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Our git-GitHub dance",
    "text": "Our git-GitHub dance\nWe only use only a few of the various git commands.1\n\n\n\nFrom http://datasciencebox.org/\nThat is why the resource http://happygitwithr.com is more helpful to us compared to the git-documentation."
  },
  {
    "objectID": "W02.html#our-git-github-dance-2",
    "href": "W02.html#our-git-github-dance-2",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Our git-GitHub dance",
    "text": "Our git-GitHub dance\nYou git clone your private repositories to your computer.\n\n\n\nFrom http://datasciencebox.org/"
  },
  {
    "objectID": "W02.html#our-git-github-dance-3",
    "href": "W02.html#our-git-github-dance-3",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Our git-GitHub dance",
    "text": "Our git-GitHub dance\ngit add, git commit, git push your work, git pull other’s work\n\n\n\nFrom http://datasciencebox.org/"
  },
  {
    "objectID": "W02.html#programming-languages",
    "href": "W02.html#programming-languages",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Programming languages",
    "text": "Programming languages\nDefinition: Systems of rules which can process instructions to be executed by the computer.\n\nOur two programming languages are:\n   \n\n\n\nIn R:\ndo_this(to_this)\ndo_that(to_this, with_those)\nto_this |> do_this() |> do_that(with_those) \nstore <- do_that(to_this)\n\nIn python:\nto_this.do_this()\nto_this.do_this(with_those)\nto_this.do_this().do_that(with_those)\nstore = do_that(to_this)\n\n\n\nThe role of brackets, dots, spaces, special words, and so on is the syntax of a language. Wrong syntax is a common cause of error. Learning syntax slows you down, but only initially!\nEssential part of the game: Installing and using libraries/packages. In R:\ninstall.packages(\"tidyverse\") (called once to install) and\nlibrary(tidyverse) (in the code before using its commands)"
  },
  {
    "objectID": "W02.html#integrated-development-environment",
    "href": "W02.html#integrated-development-environment",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Integrated development environment",
    "text": "Integrated development environment\nIDEs provide terminals, a source code editor, an object browser (the environment), output and help view, tools for rendering and version control, and more to help in the workflow. Our IDEs are:\n    VS Code\n\nEditors delight us with\n\nsyntax highlighting Then we see if code looks good\n\nc(1O, Text, true, 10,\"Text\",TRUE)\n\ncode completion Start writing, and press Tab to see options\nautomatic indentation, brace matching, keyboard shortcuts, …\n\nLearn to customize and use IDEs ever better and better!"
  },
  {
    "objectID": "W02.html#publishing-system",
    "href": "W02.html#publishing-system",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Publishing system",
    "text": "Publishing system\nWeaves together text and code to produces good-looking formatted scientific or technical output.\n   \n\nA YAML header and Markdown text with code chunks is rendered to a document in several formats.\n\n\n\nNotebooks are a similar concept: text and executable code mixed together in a browser tab. Notebooks (e.g. .ipynb-files) Can be rendered by quarto. Popular in the python world.\nMain difference between notebook and quarto-output: Code in the notebook can be executed. The page is not static."
  },
  {
    "objectID": "W02.html#quarto-documents",
    "href": "W02.html#quarto-documents",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "quarto Documents",
    "text": "quarto Documents\nHeader: YAML with title, author, document-wide specifications\nBody:\n\n\nParts of plain text with formatting syntax like **bold** for bold, and other for headers, lists, and so on. Good idea: Occassionally spend a few minutes to look up and learn some formatting rules for markdown.\nCode chunks where the programming happens!\n\nBeginning with ```{r}\nThen optional chunk specifications. For example\n\n#| label: NAME to cross-reference to it in the text\n#| echo: false to not show (or show if true) code in output\n#| warning: false to not show (or show if true) the the warnings you would see in the Console\n\nThen comes the code itself. By default, every command which creates some print output or shows a graphic will make this appear in your output-document.\nEnding with ```"
  },
  {
    "objectID": "W02.html#what-is-an-enviroment-and-why-you-should-know-it",
    "href": "W02.html#what-is-an-enviroment-and-why-you-should-know-it",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "What is an Enviroment, and why you should know it",
    "text": "What is an Enviroment, and why you should know it\n\nEnvironment can be understood as the surrounding conditions in which code is executed.\n\nIn RStudio: There is an Environment-Tab.\nMost important: Shows the variables currently accessible from the Console.\n\nThe environment of your quarto document is different for that of the Console! Remember this! It will be a source of confusion.\n\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W07.html#recap-linear-model",
    "href": "W07.html#recap-linear-model",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Recap: Linear model",
    "text": "Recap: Linear model\nFlipper length as a function of body mass.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE) + \n theme_classic(base_size = 24)\n\n\n\n\n\n\n\nThe fitting question: How can we get this line?"
  },
  {
    "objectID": "W07.html#recap-a-line",
    "href": "W07.html#recap-a-line",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Recap: A line",
    "text": "Recap: A line\nA line is a shift-scale transformation of the identity function usually written in the form\n\\[f(x) = a\\cdot x + b\\]\nwhere \\(a\\) is the slope, \\(b\\) is the intercept."
  },
  {
    "objectID": "W07.html#recap-terminology",
    "href": "W07.html#recap-terminology",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Recap: Terminology",
    "text": "Recap: Terminology\n\nResponse variable:1 Variable whose behavior or variation you are trying to understand, on the y-axis\nExplanatory variable(s):2 Other variable(s) that you want to use to explain the variation in the response, on the x-axis\nPredicted value: Output of the model function.\n\nThe model function gives the (expected) average value of the response variable conditioning on the explanatory variables\nResidual(s): A measure of how far away a case is from its predicted value (based on the particular model)\nResidual = Observed value - Predicted value\nThe residual tells how far above/below the expected value each case is\n\n\nAlso dependent variable in statistics or empirical social sciences.Also independent variable(s) in statistics or empirical social sciences."
  },
  {
    "objectID": "W07.html#our-goal",
    "href": "W07.html#our-goal",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Our goal",
    "text": "Our goal\nPredict flipper length from body mass\naverage flipper_length_mm \\(= \\beta_0 + \\beta_1\\cdot\\) body_mass_g\n\n\\(\\beta_0\\) is the intercet of the line\n\\(\\beta_1\\) is the slope of the line\n\n\nLater we will have \\(\\beta_2, \\dots, \\beta_m\\) as coefficients for more variables."
  },
  {
    "objectID": "W07.html#fitting-the-model-in-r",
    "href": "W07.html#fitting-the-model-in-r",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Fitting the model in R",
    "text": "Fitting the model in R\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\naverage flipper_length_mm \\(= 136.72956 + 0.01528\\cdot\\) body_mass_g\nInterpretation:\nThe penguins have a flipper length of 138 mm plus 0.01528 mm for each gram of body mass (that is 15.28 mm per kg). Penguins with zero mass have a flipper length of 138 mm. However, this is not in the range where the model was fitted."
  },
  {
    "objectID": "W07.html#parsnip-model-objects",
    "href": "W07.html#parsnip-model-objects",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "parsnip model objects",
    "text": "parsnip model objects\n\npengmod <- linear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\nclass(pengmod) # attributes\n\n[1] \"_lm\"       \"model_fit\"\n\ntypeof(pengmod) \n\n[1] \"list\"\n\nnames(pengmod)\n\n[1] \"lvl\"          \"spec\"         \"fit\"          \"preproc\"      \"elapsed\"     \n[6] \"censor_probs\"\n\n\n\nMost interesting for us for now: $fit\n\npengmod$fit\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\n\n\nNotice: parsnip model object is now missing in the output."
  },
  {
    "objectID": "W07.html#fit-is-the-object-created-by-lm-base-r",
    "href": "W07.html#fit-is-the-object-created-by-lm-base-r",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "$fit is the object created by lm (base R)",
    "text": "$fit is the object created by lm (base R)\n\nnames(pengmod$fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n\npengmod$fit$call\n\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\npengmod$fit$coefficients\n\n (Intercept)  body_mass_g \n136.72955927   0.01527592 \n\npengmod$fit$fitted.values |> head()\n\n       1        2        3        5        6        7 \n194.0142 194.7780 186.3763 189.4315 192.4867 192.1048 \n\npengmod$fit$residuals |> head()\n\n         1          2          3          5          6          7 \n-13.014243  -8.778039   8.623715   3.568532  -2.486651 -11.104753"
  },
  {
    "objectID": "W07.html#fitting-method-least-squares-regression",
    "href": "W07.html#fitting-method-least-squares-regression",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Fitting method: Least squares regression",
    "text": "Fitting method: Least squares regression\n\nThe regression line shall minimize the sum of the squared residuals\n(or, identically, their mean).\nMathematically: The residual for case \\(i\\) is \\(e_i = \\hat y_i - y_i\\).\nNow we want to minimize \\(\\sum_{i=1}^n e_i^2\\)\n(or equivalently \\(\\frac{1}{n}\\sum_{i=1}^n e_i^2\\) the the mean of squared errors, which we will look at later)."
  },
  {
    "objectID": "W07.html#visualization-of-residuals",
    "href": "W07.html#visualization-of-residuals",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Visualization of residuals",
    "text": "Visualization of residuals\nThe residuals are the gray lines between predictid values on the regression line and the actual values."
  },
  {
    "objectID": "W07.html#check-fitted-values-and-residuals",
    "href": "W07.html#check-fitted-values-and-residuals",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Check: Fitted values and Residuals",
    "text": "Check: Fitted values and Residuals\nRecall: Residual = Observed value - Predicted value\nThe Predicted values are also called Fitted values. Hence:\nResiduals + Fitted values = Observed values\n\n(pengmod$fit$residuals + pengmod$fit$fitted.values) |> \nhead()\n\n  1   2   3   5   6   7 \n181 186 195 193 190 181 \n\n\n\npenguins$flipper_length_mm |> head()\n\n[1] 181 186 195  NA 193 190"
  },
  {
    "objectID": "W07.html#proporties-of-least-squares-regression",
    "href": "W07.html#proporties-of-least-squares-regression",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Proporties of least squares regression",
    "text": "Proporties of least squares regression\nThe regression lines goes through the point (mean(x), mean(y)).\n\nmean(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4201.754\n\nmean(penguins$flipper_length_mm, na.rm = TRUE)\n\n[1] 200.9152"
  },
  {
    "objectID": "W07.html#proporties-of-least-squares-regression-1",
    "href": "W07.html#proporties-of-least-squares-regression-1",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Proporties of least squares regression",
    "text": "Proporties of least squares regression\nResiduals sum up to zero\n\npengmod <- linear_reg() |>  set_engine(\"lm\") |> fit(flipper_length_mm ~ body_mass_g, data = penguins)\npengmod$fit$residuals |> sum()\n\n[1] -3.765044e-14\n\n\n\nThere is no correlation between residuals and the explanatory variable\n\ncor(pengmod$fit$residuals, na.omit(penguins$body_mass_g))\n\n[1] -1.353445e-16\n\n\n\n\nThe correlation of \\(x\\) and \\(y\\) is the slope \\(b_1\\) corrected by their standard deviations.\n\ncorrelation <- cor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")\nsd_flipper <- sd(penguins$flipper_length_mm, na.rm = T)\nsd_mass <- sd(penguins$body_mass_g, na.rm = T)\nc(correlation, sd_flipper, sd_mass)\n\n[1]   0.8712018  14.0617137 801.9545357\n\ncorrelation * sd_flipper / sd_mass\n\n[1] 0.01527592\n\npengmod$fit$coefficients\n\n (Intercept)  body_mass_g \n136.72955927   0.01527592"
  },
  {
    "objectID": "W07.html#correlation-and-linear-regression",
    "href": "W07.html#correlation-and-linear-regression",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Correlation and linear regression",
    "text": "Correlation and linear regression\nWhen the two variables in the linear regression are standardized (standard scores)\n\nthe intercept is zero\nthe coefficient coincides with the correlation"
  },
  {
    "objectID": "W07.html#explanatory-variables-are-categorical",
    "href": "W07.html#explanatory-variables-are-categorical",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Explanatory variables are categorical",
    "text": "Explanatory variables are categorical\nLet’s just try what happens with species as explanatory variable. Remember, we have three species: Adelie, Chinstrap, Gentoo.\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ species, data = penguins) |> \n    tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\nWhat happened?\n\nTwo of the three species categories appear as variables now.\n\nCategorical variables are automatically encoded to dummy variables\nEach coefficient describes the expected difference between flipper length of that particular species compared to the baseline level\nWhat is the baseline level? The first category! (Here alphabetically \"Adelie\")"
  },
  {
    "objectID": "W07.html#how-do-dummy-variables-look",
    "href": "W07.html#how-do-dummy-variables-look",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "How do dummy variables look?",
    "text": "How do dummy variables look?\n\n\n\nspecies\nspeciesChinstrap\nspeciesGentoo\n\n\n\n\nAdelie\n0\n0\n\n\nChinstrap\n1\n0\n\n\nGentoo\n0\n1\n\n\n\nThen the fitting of the linear model is as before using the zero-one variables."
  },
  {
    "objectID": "W07.html#interpretation",
    "href": "W07.html#interpretation",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ species, data = penguins) |> \n    tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\n\nFlipper length of the baseline species is the intercept.\n\nAverage flipper length of Adelie is 190 mm\n\nFlipper length of the two other species add their coefficient\n\nAverage flipper length of Chinstrap is 190 + 5.87 mm\nAverage flipper length of Gentoo is 190 + 27.2 mm"
  },
  {
    "objectID": "W07.html#compare-to-a-visualization",
    "href": "W07.html#compare-to-a-visualization",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Compare to a visualization",
    "text": "Compare to a visualization\n\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\n\nThe red dots are the average values for species."
  },
  {
    "objectID": "W07.html#linear-model",
    "href": "W07.html#linear-model",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Linear model",
    "text": "Linear model\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107\n\n\naverage flipper_length_mm \\(= 137 + 0.0153\\cdot\\) body_mass_g\n\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(bill_depth_mm ~ bill_length_mm, data = penguins) |> \n tidy()\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic  p.value\n  <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     20.9       0.844      24.7  4.72e-78\n2 bill_length_mm  -0.0850    0.0191     -4.46 1.12e- 5\n\n\naverage bill_depth_mm \\(= 20.9 -0.085\\cdot\\) bill_length_mm\n\n\nTechnical: The idea of the tidy() function is to turn an object into a tidy tibble. Here, it extracts the coefficients of the linear model (and more statistical information)."
  },
  {
    "objectID": "W07.html#r-squared-of-a-fitted-model",
    "href": "W07.html#r-squared-of-a-fitted-model",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "R-squared of a fitted model",
    "text": "R-squared of a fitted model\n\\(R^2\\) is the percentage of variability in the response explained by the regression model.\nR-squared is also called coefficient of determination.\nDefinition:\n\\(R^2 = 1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}\\)\nwhere \\(SS_\\text{res} = \\sum_i(y_i - f_i)^2 = \\sum_i e_i^2\\) is the sum of the squared residuals, and\n\\(SS_\\text{tot} = \\sum_i(y_i - \\bar y)^2\\) the total sum of squares which is proportional to the variance of \\(y\\). (\\(\\bar y\\) is the mean of \\(y\\).)"
  },
  {
    "objectID": "W07.html#linear-model-r-squared",
    "href": "W07.html#linear-model-r-squared",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Linear model R-squared",
    "text": "Linear model R-squared\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g, data = penguins) |>\n glance()  # glance shows summary statistics of model fit\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.759         0.758  6.91     1071. 4.37e-107     1 -1146. 2297. 2309.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nInterpretation R-square? 75.8% of the variance of flipper length can be explained by a linear relation with body mass.\n\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(bill_depth_mm ~ bill_length_mm, data = penguins) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0552        0.0525  1.92      19.9 0.0000112     1  -708. 1422. 1433.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n5.25% of the variance of bill depth can be explained by a linear relation with bill length.\n\n\nTechnical: The idea of the glance() function is to construct a single row summary “glance” of a model, fit, or other object."
  },
  {
    "objectID": "W07.html#r-squared-and-correlation",
    "href": "W07.html#r-squared-and-correlation",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "R-squared and correlation",
    "text": "R-squared and correlation\nFor a linear model with one predictor, the square of the correlation coefficient is the same as the R-squared of the model.\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.759         0.758  6.91     1071. 4.37e-107     1 -1146. 2297. 2309.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\ncor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")\n\n[1] 0.8712018\n\n\n\ncor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")^2\n\n[1] 0.7589925\n\n\n\nHence, the name \\(R^2\\)!"
  },
  {
    "objectID": "W07.html#more-predictors-coefficients",
    "href": "W07.html#more-predictors-coefficients",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "More predictors: Coefficients",
    "text": "More predictors: Coefficients\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107\n\n\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + bill_length_mm, data = penguins) |> \n tidy()\n\n# A tibble: 3 × 5\n  term           estimate std.error statistic   p.value\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    122.      2.86         42.7  1.70e-138\n2 body_mass_g      0.0131  0.000545     23.9  7.56e- 75\n3 bill_length_mm   0.549   0.0801        6.86 3.31e- 11\n\n\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + bill_length_mm + bill_depth_mm, data = penguins) |> \n tidy()\n\n# A tibble: 4 × 5\n  term           estimate std.error statistic   p.value\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    158.      4.63         34.1  1.80e-111\n2 body_mass_g      0.0109  0.000538     20.3  1.93e- 60\n3 bill_length_mm   0.592   0.0717        8.26 3.42e- 15\n4 bill_depth_mm   -1.68    0.181        -9.29 1.99e- 18"
  },
  {
    "objectID": "W07.html#more-predictors-glance-r-squared",
    "href": "W07.html#more-predictors-glance-r-squared",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "More predictors: glance R-squared",
    "text": "More predictors: glance R-squared\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.759         0.758  6.91     1071. 4.37e-107     1 -1146. 2297. 2309.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + bill_length_mm, data = penguins) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.788         0.787  6.49      631. 4.88e-115     2 -1123. 2255. 2270.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + bill_length_mm + bill_depth_mm, data = penguins) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.831         0.830  5.80      556. 2.99e-130     3 -1084. 2179. 2198.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"
  },
  {
    "objectID": "W07.html#more-predictors-equations",
    "href": "W07.html#more-predictors-equations",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "More predictors: Equations",
    "text": "More predictors: Equations\naverage flipper_length_mm \\(= 137 + 0.0153\\cdot\\) body_mass_g\n75.8% explained variance\naverage flipper_length_mm \\(= 122 + 0.0131\\cdot\\) body_mass_g \\(+ 0.549\\cdot\\) bill_length_mm\n78.7% explained variance\naverage flipper_length_mm \\(= 158 + 0.0109\\cdot\\) body_mass_g \\(+ 0.592\\cdot\\) bill_length_mm \\(- 1.68\\cdot\\) bill_length_mm\n83.0% explained variance"
  },
  {
    "objectID": "W07.html#adding-a-categorical-variable-as-main-effect",
    "href": "W07.html#adding-a-categorical-variable-as-main-effect",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Adding a categorical variable as main effect",
    "text": "Adding a categorical variable as main effect\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> \n tidy()\n\n# A tibble: 4 × 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      159.       2.39         66.6  2.45e-196\n2 body_mass_g        0.00840  0.000634     13.3  1.40e- 32\n3 speciesChinstrap   5.60     0.788         7.10 7.33e- 12\n4 speciesGentoo     15.7      1.09         14.4  6.80e- 37\n\n\nA main effect by categorical dummy variables allows for different intercepts per species. (However, the slopes are the same.)\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.854         0.853  5.40      659. 7.42e-141     3 -1060. 2129. 2149.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nAdding species increases R-squared better than adding bill length and bill depth together!"
  },
  {
    "objectID": "W07.html#adding-as-interaction-effect",
    "href": "W07.html#adding-as-interaction-effect",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Adding as interaction effect",
    "text": "Adding as interaction effect\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> \n tidy()\n\n# A tibble: 6 × 5\n  term                          estimate std.error statistic   p.value\n  <chr>                            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)                  165.       3.55         46.5  1.56e-148\n2 body_mass_g                    0.00668  0.000952      7.01 1.30e- 11\n3 speciesChinstrap             -13.9      7.30         -1.90 5.84e-  2\n4 speciesGentoo                  6.06     6.05          1.00 3.17e-  1\n5 body_mass_g:speciesChinstrap   0.00523  0.00195       2.68 7.66e-  3\n6 body_mass_g:speciesGentoo      0.00236  0.00135       1.75 8.16e-  2\n\n\n\nNote the * for interaction effect!\nAlso main effects for both variables are in as coefficients.\nAdelie is the baseline species (because it is first in the alphabet).\nAn interaction effect allows for different slopes for each species!"
  },
  {
    "objectID": "W07.html#improvement-through-the-interaction-effects-is-small-here",
    "href": "W07.html#improvement-through-the-interaction-effects-is-small-here",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Improvement through the interaction effects is small here",
    "text": "Improvement through the interaction effects is small here\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.854         0.853  5.40      659. 7.42e-141     3 -1060. 2129. 2149.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.857         0.855  5.35      404. 9.60e-140     5 -1056. 2125. 2152.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"
  },
  {
    "objectID": "W07.html#regression-lines-by-species",
    "href": "W07.html#regression-lines-by-species",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Regression lines by species",
    "text": "Regression lines by species\n\nCompare the slopes to the regression output on the slides before!"
  },
  {
    "objectID": "W07.html#different-equations-for-each-species",
    "href": "W07.html#different-equations-for-each-species",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Different equations for each species!",
    "text": "Different equations for each species!\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> \n tidy()\n\n# A tibble: 6 × 5\n  term                          estimate std.error statistic   p.value\n  <chr>                            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)                  165.       3.55         46.5  1.56e-148\n2 body_mass_g                    0.00668  0.000952      7.01 1.30e- 11\n3 speciesChinstrap             -13.9      7.30         -1.90 5.84e-  2\n4 speciesGentoo                  6.06     6.05          1.00 3.17e-  1\n5 body_mass_g:speciesChinstrap   0.00523  0.00195       2.68 7.66e-  3\n6 body_mass_g:speciesGentoo      0.00236  0.00135       1.75 8.16e-  2\n\n\nAdelie:\naverage flipper_length_mm \\(= 165 + 0.00668\\cdot\\) body_mass_g\nChinstrap:\naverage flipper_length_mm \\(= 165 - 13.6 + (0.00668 + 0.00523)\\cdot\\) body_mass_g\nGentoo:\naverage flipper_length_mm \\(= 165 + 6.06 + (0.00668 + 0.00236)\\cdot\\) body_mass_g"
  },
  {
    "objectID": "W07.html#interaction-effects-more-categoricals",
    "href": "W07.html#interaction-effects-more-categoricals",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Interaction effects more categoricals",
    "text": "Interaction effects more categoricals\nAdding products of variables in the linear model \\(y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{3}x_1x_2 + \\dots\\).\nFor \\(x_1\\) and \\(x_2\\) being dummy variables for being female and having kids this is for example\n\n\n\n\n\ngndr_f\nhas_kids\ngndr_f_x_has_kids\n\n\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n1\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\n\nWhat is the baseline? Being male without kids.\n\nThought experiment:\n\nWhen we estimate a model explaining life satisfaction with these. How would we see if being a mother increases life satisfaction more than being a father? positiv coefficient for gndr_f_x_has_kids"
  },
  {
    "objectID": "W07.html#when-a-linear-model-is-bad",
    "href": "W07.html#when-a-linear-model-is-bad",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "When a linear model is bad",
    "text": "When a linear model is bad\nExample: Total corona cases in Germany in the first wave 2020."
  },
  {
    "objectID": "W07.html#log-transformation",
    "href": "W07.html#log-transformation",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "\\(\\log\\) transformation",
    "text": "\\(\\log\\) transformation\nInstead of Cumulative_cases we look at \\(\\log(\\)Cumulative_cases\\()\\)\n\nAlmost perfect fit of the linear model: \\(\\log(y)=\\log(\\beta_0) + \\beta_1\\cdot x\\)\n(\\(y=\\) Cumulative cases, \\(x=\\) Days)\n\nExponention gives the model: \\(y=\\beta_0 e^{\\beta_1\\cdot x}\\)"
  },
  {
    "objectID": "W07.html#exponential-growth",
    "href": "W07.html#exponential-growth",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Exponential growth!",
    "text": "Exponential growth!\n\\(y=\\beta_0 e^{\\beta_1\\cdot x}\\)\nFor comparison: Logistic function \\(y = \\frac{N \\beta_0 e^{\\beta_1\\cdot x}}{N + \\beta_0 e^{\\beta_1\\cdot x}}\\) for \\(N=200000\\)\n\nLogistic growth (as in the SI model) mimicks exponential growth initially."
  },
  {
    "objectID": "W07.html#log-transformation-1",
    "href": "W07.html#log-transformation-1",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "\\(\\log\\) transformation",
    "text": "\\(\\log\\) transformation\n\nWhat is the difference to the penguin model?\n\n\n\\(x\\) has an ordered structure and no duplicates\n\nThe fit looks so good. Why?\n\n\nBecause there is a mechanistic explanation behind: The SI model."
  },
  {
    "objectID": "W07.html#however-it-works-only-in-a-certain-range",
    "href": "W07.html#however-it-works-only-in-a-certain-range",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "However, it works only in a certain range",
    "text": "However, it works only in a certain range"
  },
  {
    "objectID": "W07.html#however-it-works-only-in-a-certain-range-log-scale-on-y",
    "href": "W07.html#however-it-works-only-in-a-certain-range-log-scale-on-y",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "However, it works only in a certain range (log scale on y)",
    "text": "However, it works only in a certain range (log scale on y)"
  },
  {
    "objectID": "W07.html#what-if-response-is-binary",
    "href": "W07.html#what-if-response-is-binary",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "What if response is binary?",
    "text": "What if response is binary?\n\nExample: Spam filter for emails\n\n\nlibrary(openintro)\nlibrary(tidyverse)\nglimpse(email)\n\nRows: 3,921\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         <dttm> 2012-01-01 07:16:41, 2012-01-01 08:03:59, 2012-01-01 17:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     <dbl> 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  <int> 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       <fct> 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       <fct> big, small, small, small, none, none, big, small, small, …"
  },
  {
    "objectID": "W07.html#multinomial-response-variable",
    "href": "W07.html#multinomial-response-variable",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Multinomial response variable?",
    "text": "Multinomial response variable?\n\nWe will not cover other categorical variables than binary ones here.\nHowever, many of the probabilistic concepts transfer."
  },
  {
    "objectID": "W07.html#variables",
    "href": "W07.html#variables",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Variables",
    "text": "Variables\n?email shows all variable descriptions. For example:\n\nspam Indicator for whether the email was spam.\nfrom Whether the message was listed as from anyone (this is usually set by default for regular outgoing email).\ncc Number of people cc’ed.\ntime Time at which email was sent.\nattach The number of attached files.\ndollar The number of times a dollar sign or the word “dollar” appeared in the email.\nnum_char The number of characters in the email, in thousands.\nre_subj Whether the subject started with “Re:”, “RE:”, “re:”, or “rE:”\n\n\n\nThe development, extraction, or discovery of such variables is called feature engineering, feature extraction or feature discovery. Usually, a combination of domain knowledge and data science skill is needed to do this."
  },
  {
    "objectID": "W07.html#data-exploration",
    "href": "W07.html#data-exploration",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Data exploration",
    "text": "Data exploration\nWould you expect spam to be longer or shorter?\n\n\nemail |> ggplot(aes(x = num_char, y = spam)) + geom_boxplot()\n\n\n\n\n\n\nWould you expect spam subject to start with “Re:” or the like?\n\n\n\nemail |> ggplot(aes(y = re_subj, fill = spam)) + geom_bar()"
  },
  {
    "objectID": "W07.html#linear-models",
    "href": "W07.html#linear-models",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Linear models?",
    "text": "Linear models?\nBoth seem to give some signal. How can we model the relationship?\nWe focus first on just num_char:\n\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + \n geom_point(alpha = 0.2) + geom_smooth(method = \"lm\")\n\n\n\nlinear_reg() |> fit(as.numeric(spam)-1 ~ num_char, data = email)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = as.numeric(spam) - 1 ~ num_char, data = data)\n\nCoefficients:\n(Intercept)     num_char  \n   0.118214    -0.002299  \n\n\nWe would like to have a better concept!"
  },
  {
    "objectID": "W07.html#a-penguins-example",
    "href": "W07.html#a-penguins-example",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "A penguins example",
    "text": "A penguins example\n\npenguins |> na.omit() |> \n ggplot(aes(x = body_mass_g, y = as.numeric(sex)-1)) + \n geom_point(alpha = 0.2) + geom_smooth(method = \"lm\")\n\n\nIt does not make much sense to predict 0-1-values with a linear model."
  },
  {
    "objectID": "W07.html#a-probabilistic-concept",
    "href": "W07.html#a-probabilistic-concept",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "A probabilistic concept",
    "text": "A probabilistic concept\n\nWe treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials\n\nBernoulli trial: a random experiment with exactly two possible outcomes, success and failure, in which the probability of success is the same every time the experiment is conducted\n\n\n\n\nEach email is treated as Bernoulli trial with separate probability of success\n\n\\[ y_i ∼ \\text{Bernoulli}(p_i) \\]\n\n\n\nWe use the predictor variables to model the Bernoulli parameter \\(p_i\\)\n\n\n\n\nNow we conceptualized a continuous response, but still a linear model does not fit perfectly for \\(p_i\\) (since a probability is between 0 and 1).\nHowever, we can transform the linear model to have the appropriate range."
  },
  {
    "objectID": "W07.html#characterising-glms",
    "href": "W07.html#characterising-glms",
    "title": "W#07: Fitting Linear Models, Interaction Effects, Nonlinear Models, Predicting Categorical Variables",
    "section": "Characterising GLMs",
    "text": "Characterising GLMs\n\nGeneralized linear models (GLMs) are a way of addressing many problems in regression\nLogistic regression is one example\n\nAll GLMs have the following three characteristics:\n\nA probability distribution as a generative model for the outcome variable \\(y_i \\sim \\text{Distribution}(\\text{parameter})\\)\n\n\n\nA linear model \\(\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\)\nwhere \\(\\eta\\) is related to a mean parameter of the distribution by the …\n\n\n\n\nLink function that relates the linear model to the parameter of the outcome distribution.\n\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "socialmedia.html",
    "href": "socialmedia.html",
    "title": "Data Science on the Internet and in Social Media",
    "section": "",
    "text": "Many people talk data science and related things on social media. Some people even make a business publishing short snippets because they love communicating about data science but also trying to attract you to their newsletters or courses. Also many developers of R or python packages are on social media occasionally posting new releases, ideas, or reacting."
  },
  {
    "objectID": "socialmedia.html#should-you-follow-data-science-on-social-media",
    "href": "socialmedia.html#should-you-follow-data-science-on-social-media",
    "title": "Data Science on the Internet and in Social Media",
    "section": "2 Should you follow data science on social media?",
    "text": "2 Should you follow data science on social media?\nYes, you should! Because\n\nyou can get a feeling of how the field develops.\nyou can find nice snippets to learn things, when lucky they are exactly on your level.\nmany people learn data science things on social media, you react with a questions and may be lucky to get a response.\nthere may sometimes be release for your confused mind through data science memes, cartoons, or jokes.\n\nNo, you should not! Because\n\nall social media is disctraction from focused learning and concentrated working.\nyou may develop imposter syndrome symptoms. There is so much stuff out and you may think everybody else knows so much more and you will never catch up. (Hint, the reality is more like this: No one knows everything, you already have your own perspective and own unique set of skills, every good data scientist is constantly learning and is constantly able to rethink and also sometimes has similar feelings, what gets pushed on social media tends to be more opinionated and overstated.)\nyou may fall into hype topics where people overstate the importance, risks, dangers, or opportunities a lot.\n\nFinal answer: You decide. If you do, learn to stop when it feels bad, learn to detect hype, and do more other data science learning: real exercises, tutorials or your projects. Just assume you are in the blue region on the graph in this cartoon 😉."
  },
  {
    "objectID": "socialmedia.html#which-social-media-and-how-to-follow-data-science",
    "href": "socialmedia.html#which-social-media-and-how-to-follow-data-science",
    "title": "Data Science on the Internet and in Social Media",
    "section": "3 Which social media and how to follow data science?",
    "text": "3 Which social media and how to follow data science?\nThe following is not intended to be complete or balanced in anyway. If you find something interesting let me know and I may add it to the lists.\nFor following Data Science in social media you often need platform accounts, otherwise most platform do not even let you see things. Anyway, accounts help you by selecting who to follow. You do not need to post or like things, you can start just looking around and following.\nThe general ways to access content is by following interesting people, searching for words or hashtags, or use the explore options the platforms offer.\nFor sure data science is a common topic on 𝕏 (which I still call Twitter which is still the URL), Reddit , and Mastodon .\nOn Mastodon: It is not a propriatary service, it is offering a joint open source protocol which different independent servers can use to run something like a decentralized twitter. Some people call it Twitter with an open protocol like email. I think, this is a very good development! When you want to have a mastodon account, you need to decide for a server where to live. In the end it is not so important because you can follow people on all servers and you can also move with your account later. I am in the community https://datasci.social but you could take anythings.\nAfter Elon Musk’s twitter take over microblogging people are a but puzzled where to go and many maintain more than one plattform like twitter/X, bluesky, threads, or mastodon (which I support most). So, you may find the same people at different places."
  },
  {
    "objectID": "socialmedia.html#some-entry-points",
    "href": "socialmedia.html#some-entry-points",
    "title": "Data Science on the Internet and in Social Media",
    "section": "4 Some “entry points”",
    "text": "4 Some “entry points”\n\n4.1 People\nA cool statistician: https://twitter.com/kareem_carr\nhttps://fosstodon.org/@juliasilge\nAllison Horst Chelsea\nChristoph Molnar https://twitter.com/ChristophMolnar Albert Rapp https://twitter.com/rappa753 Daniël Lakens https://twitter.com/lakens\nHadley wickham\n\n\n4.2 Other things\nSubreddits and the like\nYoutube, and podcasts\nA reddit thread https://www.reddit.com/r/datascience/comments/16dk5b6/r_vs_python_detailed_examples_from_proficient/"
  },
  {
    "objectID": "benford .html",
    "href": "benford .html",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "",
    "text": "This is an excersise combining knowledge about logarithms and probability distributions and a statistical test.\nExercise: Analyzing Data with Benford’s Law in R\nBackground: Benford’s Law, also known as the First-Digit Law, is a phenomenon where in many naturally occurring datasets, the leading digit (i.e., the first digit) is more likely to be small, such as 1, 2, or 3, than it is to be large, such as 8 or 9. This law has applications in fraud detection, financial auditing, and data integrity verification.\nTask: You are given a dataset in a CSV file named data.csv. Your task is to perform the following steps using R and data analytics libraries to determine if the dataset follows Benford’s Law:\nData Loading: Read the data from data.csv into a data frame using read.csv().\n\nData Exploration: Explore the dataset to understand its structure and contents. Display summary statistics, and visualize the distribution of the first digits in the dataset using a histogram.\n\nCalculate Expected Benford's Law Frequencies: Calculate the expected frequency of each first digit (1 through 9) according to Benford's Law. You can use the formula:\n\nP(d)=log⁡10(d+1)−log⁡10(d)P(d)=log10​(d+1)−log10​(d)\n\nWhere P(d)P(d) is the expected proportion of numbers starting with digit dd.\n\nCompare Observed vs. Expected Frequencies: Calculate the observed frequency of each first digit in the dataset. Compare the observed frequencies with the expected frequencies from Benford's Law. Create a bar chart or similar visualization to illustrate the comparison.\n\nHypothesis Testing: Perform a statistical hypothesis test to determine if the dataset significantly deviates from Benford's Law. You can use a chi-squared goodness-of-fit test to assess the deviation.\n\nConclusion: Based on the results of your analysis, draw conclusions about whether the dataset follows or deviates from Benford's Law. Explain your findings and their implications.\nNote:\nYou may assume that the dataset contains numerical values.\nYou can use R packages like readr for data reading, and ggplot2 for data visualization.\nInclude comments in your code to explain your approach and calculations.\nProvide clear visualizations and explanations in your final report.\nBonus (Optional): If you’d like to extend this exercise, you can consider applying Benford’s Law to more complex datasets or exploring other variations of the law, such as the Second-Digit Law or the Benford’s Law for Two-Digit Numbers using R."
  }
]