[
  {
    "objectID": "W05.html#data-sets-1a-and-1b-widsom-of-crowd",
    "href": "W05.html#data-sets-1a-and-1b-widsom-of-crowd",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Data Sets 1a and 1b: Widsom of Crowd",
    "text": "Data Sets 1a and 1b: Widsom of Crowd\n1a: Ox weigh guessing competition 1907 (collected by Galton)\n\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5)\n\n\n\n\n1b: Viertelfest “guess the number of sold lots”-competition 2009\n\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500)"
  },
  {
    "objectID": "W05.html#data-set-2-palmer-penguins",
    "href": "W05.html#data-set-2-palmer-penguins",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Data Set 2: Palmer Penguins",
    "text": "Data Set 2: Palmer Penguins\nPalmer Penguins\nChinstrap, Gentoo, and Adélie Penguins\n   \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "W05.html#summary-from-base-r",
    "href": "W05.html#summary-from-base-r",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "summary from base R",
    "text": "summary from base R\n\n\nShows summary statistics for the values in a vector\n\nsummary(galton$Estimate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    896    1162    1208    1197    1236    1516 \n\n\n\nsummary(viertel$Schätzung)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n     120     5000     9843    53164    20000 29530000 \n\n\nOr for all columns in a data frame\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\nQuestion\nWhat does\n1st Qu. and\n3rd Qu. mean?"
  },
  {
    "objectID": "W05.html#quantiles",
    "href": "W05.html#quantiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Quantiles",
    "text": "Quantiles\nCut points specifying intervals which contain equal amounts of values of the distribution.\n\\(q\\)-quantiles divide numbers into \\(q\\) intervals covering all values.\nThe quantiles are the cut points: For \\(q\\) intervals there are \\(q-1\\) cut points of interest.\n\nThe one 2-quantile is the median\nThe three 4-quantiles are called quartiles\n\n1st Qu. is the first quartile\nThe second quartile is the median\n3rd Qu. is the third quartile\n\n100-quantiles are called percentiles\n\n\n\nWe omit problems of estimating quantiles from a sample where the number of estimates does not fit to a desired partition of equal size here."
  },
  {
    "objectID": "W05.html#a-galton-quartiles",
    "href": "W05.html#a-galton-quartiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "1a Galton: Quartiles",
    "text": "1a Galton: Quartiles\n\n# Min, 3 Quartiles, Max\nquantile(galton$Estimate, prob = seq(0, 1, by = 0.25))\n\n    0%    25%    50%    75%   100% \n 896.0 1162.5 1208.0 1236.0 1516.0 \n\n\nInterpretation: What does the value at 25% mean?\n\nThe 25% of all values are lower than the value. 75% are larger.\n\n\n\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + \n geom_vline(xintercept = quantile(galton$Estimate, prob = seq(0, 1, by = 0.25)), color = \"red\")"
  },
  {
    "objectID": "W05.html#a-galton-20-quantiles",
    "href": "W05.html#a-galton-20-quantiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "1a Galton: 20-quantiles",
    "text": "1a Galton: 20-quantiles\n\n# Min, 3 Quartiles, Max\nquantile(galton$Estimate, prob = seq(0, 1, by = 0.05))\n\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n 896.0 1078.3 1109.0 1126.9 1150.0 1162.5 1174.0 1181.0 1189.0 1199.0 1208.0 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%   100% \n1214.0 1219.0 1225.0 1231.0 1236.0 1243.8 1255.1 1270.0 1293.0 1516.0 \n\n\n\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + \n geom_vline(xintercept = quantile(galton$Estimate, prob = seq(0, 1, by = 0.05)), color = \"red\")"
  },
  {
    "objectID": "W05.html#b-viertelfest-quartiles",
    "href": "W05.html#b-viertelfest-quartiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "1b Viertelfest: Quartiles",
    "text": "1b Viertelfest: Quartiles\n\nquantile(viertel$Schätzung, prob = seq(0, 1, by = 0.25))\n\n      0%      25%      50%      75%     100% \n     120     5000     9843    20000 29530000 \n\n\n\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + \n geom_vline(xintercept = quantile(viertel$`Schätzung`, prob = seq(0, 1, by = 0.25))[1:4], color = \"red\")"
  },
  {
    "objectID": "W05.html#b-viertelfest-20-quantiles",
    "href": "W05.html#b-viertelfest-20-quantiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "1b Viertelfest: 20-quantiles",
    "text": "1b Viertelfest: 20-quantiles\n\nquantile(viertel$Schätzung, prob = seq(0, 1, by = 0.05))\n\n         0%          5%         10%         15%         20%         25% \n     120.00     1213.25     2000.00     3115.00     4012.00     5000.00 \n        30%         35%         40%         45%         50%         55% \n    5853.50     7000.00     7821.00     8705.25     9843.00    10967.50 \n        60%         65%         70%         75%         80%         85% \n   12374.00    14444.00    16186.00    20000.00    27500.00    38000.00 \n        90%         95%        100% \n   63649.50    99773.50 29530000.00 \n\n\n\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + \n geom_vline(xintercept = quantile(viertel$`Schätzung`, prob = seq(0, 1, by = 0.05))[1:19], color = \"red\")"
  },
  {
    "objectID": "W05.html#palmer-penguins-flipper-length-quartiles",
    "href": "W05.html#palmer-penguins-flipper-length-quartiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "2 Palmer Penguins Flipper Length: Quartiles",
    "text": "2 Palmer Penguins Flipper Length: Quartiles\n\nquantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.25), na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n 172  190  197  213  231 \n\n\n\npenguins |> ggplot(aes(flipper_length_mm)) + geom_histogram(binwidth = 1) + \n geom_vline(xintercept = quantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.25), na.rm = TRUE), color = \"red\")"
  },
  {
    "objectID": "W05.html#palmer-penguins-flipper-length-20-quantiles",
    "href": "W05.html#palmer-penguins-flipper-length-20-quantiles",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "2 Palmer Penguins Flipper Length: 20-quantiles",
    "text": "2 Palmer Penguins Flipper Length: 20-quantiles\n\nquantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.05), na.rm = TRUE)\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n172.0 181.0 185.0 187.0 188.0 190.0 191.0 193.0 194.0 195.0 197.0 199.0 203.0 \n  65%   70%   75%   80%   85%   90%   95%  100% \n208.0 210.0 213.0 215.0 218.0 220.9 225.0 231.0 \n\n\n\npenguins |> ggplot(aes(flipper_length_mm)) + geom_histogram(binwidth = 1) + \n geom_vline(xintercept = quantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.05), na.rm = TRUE), color = \"red\")"
  },
  {
    "objectID": "W05.html#interquartile-range-iqr",
    "href": "W05.html#interquartile-range-iqr",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\nThe difference between the 1st and the 3rd quartile. Alternative dispersion measure.\nThe range in which the middle 50% of the values are located.\nExamples:\n\n\n\n# Min, 3 Quartiles, Max\nIQR(galton$Estimate)\n\n[1] 73.5\n\nsd(galton$Estimate) # for comparison\n\n[1] 73.58677\n\nIQR(viertel$Schätzung)\n\n[1] 15000\n\nsd(viertel$Schätzung) # for comparison\n\n[1] 848395.5\n\nIQR(penguins$flipper_length_mm, na.rm = TRUE)\n\n[1] 23\n\nsd(penguins$flipper_length_mm, na.rm = TRUE) # for comparison\n\n[1] 14.06171"
  },
  {
    "objectID": "W05.html#boxplots",
    "href": "W05.html#boxplots",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Boxplots",
    "text": "Boxplots\nA condensed visualization of a distribution showing location, spread, skewness and outliers.\n\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n\n\n\nThe box shows the median in the middle and the other two quartiles as their borders.\nWhiskers: From above the upper quartile, a distance of 1.5 times the IQR is measured out and a whisker is drawn up to the largest observed data point from the dataset that falls within this distance. Similarly, for the lower quartile.\nWhiskers must end at an observed data point! (So lengths can differ.)\nAll other values outside of box and whiskers are shown as points and often called outliers. (There may be none.)"
  },
  {
    "objectID": "W05.html#boxplots-vs.-histograms",
    "href": "W05.html#boxplots-vs.-histograms",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\n\nHistograms can show the shape of the distribution well, but not the summary statistics like the median.\n\n\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n\n\n\n\n\ngalton |> ggplot(aes(x = Estimate)) + geom_histogram(binwidth = 5)"
  },
  {
    "objectID": "W05.html#boxplots-vs.-histograms-1",
    "href": "W05.html#boxplots-vs.-histograms-1",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\n\nBoxplots can not show the patterns of bimodal or multimodal distributions.\n\n\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_boxplot()\n\n\n\n\n\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_histogram(binwidth = 1)"
  },
  {
    "objectID": "W05.html#minimizing-proporties-of-mean-and-median",
    "href": "W05.html#minimizing-proporties-of-mean-and-median",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Minimizing proporties of Mean and Median",
    "text": "Minimizing proporties of Mean and Median\nMean minimizes the mean of squared deviations from it. No other value \\(a\\) has a lower mean of square distances from the data points. \\(\\frac{1}{n}\\sum_{i=1}^n(x_i - a)^2\\).\n\nMedian minimizes the sum of the absolute deviation. No other value \\(a\\) has a lower mean of absolute distances from the data points. \\(\\frac{1}{n}\\sum_{i=1}^n|x_i - a|\\)."
  },
  {
    "objectID": "W05.html#two-families-of-summary-statistics",
    "href": "W05.html#two-families-of-summary-statistics",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Two families of summary statistics",
    "text": "Two families of summary statistics\n\nMeasures based on sums (related to mathematical moments)\n\nMean\nStandard deviation\n\nMeasures based on the ordered sequence of these observations (order statistics)\n\nMedian (and all quantiles)\nInterquartile range"
  },
  {
    "objectID": "W05.html#covariance",
    "href": "W05.html#covariance",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Covariance",
    "text": "Covariance\nGoal: We want to measure the joint variation in numerical observations of two variables \\(x_1,\\dots,x_n\\) and \\(y_1, \\dots, y_n\\).\nCovariance\n\\(\\text{cov}(x,y) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)\\)\nwhere \\(\\mu_x\\) and \\(\\mu_y\\) are the arithmetic means of \\(x\\) and \\(y\\).\n\nNote: \\(\\text{cov}(x,x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)(x_i - \\mu_x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)^2 = \\text{Var}(x)\\)"
  },
  {
    "objectID": "W05.html#correlation",
    "href": "W05.html#correlation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Correlation",
    "text": "Correlation\nGoal: We want to measure the linear correlation in numerical observations of two variables \\(x_1,\\dots,x_n\\) and \\(y_1, \\dots, y_n\\).\nPearson correlation coefficient \\(r_{xy} = \\frac{\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^n(x_i - \\mu_x)^2}\\sqrt{\\sum_{i=1}^n(y_i - \\mu_y)^2}}\\)\n\nRelation to covariance: \\(r_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}\\)\nwhere \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of \\(x\\) and \\(y\\).\nRelation to standard scores:\nWhen \\(x\\) and \\(y\\) are standard scores (each with mean zero and standard deviation one), then \\(\\text{cov}(x,y) = r_{xy}\\).\n\n\n\nThere are other correlation coefficients which we omit here."
  },
  {
    "objectID": "W05.html#interpretation-of-correlation",
    "href": "W05.html#interpretation-of-correlation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretation of correlation",
    "text": "Interpretation of correlation\nCorrelation between two vectors \\(x\\) and \\(y\\) is “normalized”.\n\nThe maximal possible values is \\(r_{xy} = 1\\)\n\n\\(x\\) and \\(y\\) are fully correlated\n\nThe minimal values is \\(r_{xy} = -1\\)\n\n\\(x\\) and \\(y\\) are anticorrelated\n\n\\(r_{xy} \\approx 0\\) mean\n\nthe variables are uncorrelated\n\n\\(r_{xy} = r_{yx}\\)"
  },
  {
    "objectID": "W05.html#correlation-matrix",
    "href": "W05.html#correlation-matrix",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nUsing corrr from the packagestidymodels\n\nlibrary(corrr)\npenguins |> select(-species, -island, -sex) |> \n correlate()\n\n# A tibble: 5 × 6\n  term        bill_length_mm bill_depth_mm flipper_length_mm body_mass_g    year\n  <chr>                <dbl>         <dbl>             <dbl>       <dbl>   <dbl>\n1 bill_lengt…        NA            -0.235              0.656      0.595   0.0545\n2 bill_depth…        -0.235        NA                 -0.584     -0.472  -0.0604\n3 flipper_le…         0.656        -0.584             NA          0.871   0.170 \n4 body_mass_g         0.595        -0.472              0.871     NA       0.0422\n5 year                0.0545       -0.0604             0.170      0.0422 NA"
  },
  {
    "objectID": "W05.html#correlation-table",
    "href": "W05.html#correlation-table",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Correlation table",
    "text": "Correlation table\nUsing correlation from the packagescorrelation\n\nlibrary(correlation)\nresults <- palmerpenguins::penguins |> \n select(-species, -island, -sex) |> \n correlation()\nresults\n\n# Correlation Matrix (pearson-method)\n\nParameter1        |        Parameter2 |     r |         95% CI | t(340) |         p\n-----------------------------------------------------------------------------------\nbill_length_mm    |     bill_depth_mm | -0.24 | [-0.33, -0.13] |  -4.46 | < .001***\nbill_length_mm    | flipper_length_mm |  0.66 | [ 0.59,  0.71] |  16.03 | < .001***\nbill_length_mm    |       body_mass_g |  0.60 | [ 0.52,  0.66] |  13.65 | < .001***\nbill_length_mm    |              year |  0.05 | [-0.05,  0.16] |   1.01 | 0.797    \nbill_depth_mm     | flipper_length_mm | -0.58 | [-0.65, -0.51] | -13.26 | < .001***\nbill_depth_mm     |       body_mass_g | -0.47 | [-0.55, -0.39] |  -9.87 | < .001***\nbill_depth_mm     |              year | -0.06 | [-0.17,  0.05] |  -1.11 | 0.797    \nflipper_length_mm |       body_mass_g |  0.87 | [ 0.84,  0.89] |  32.72 | < .001***\nflipper_length_mm |              year |  0.17 | [ 0.06,  0.27] |   3.17 | 0.007**  \nbody_mass_g       |              year |  0.04 | [-0.06,  0.15] |   0.78 | 0.797    \n\np-value adjustment method: Holm (1979)\nObservations: 342\n\n\n\n\nWhat do the stars mean? Statistical significance automatically added by the . We treat that later."
  },
  {
    "objectID": "W05.html#correlation-visualization",
    "href": "W05.html#correlation-visualization",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Correlation visualization",
    "text": "Correlation visualization\n\nresults %>%\n  summary(redundant = TRUE) %>%\n  plot()"
  },
  {
    "objectID": "W05.html#exploratory-data-analysis-1",
    "href": "W05.html#exploratory-data-analysis-1",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nEDA is the systematic exploration of data using\n\nvisualization\ntransformation\ncomputation of characteristic values\nmodeling\n\n\n\nComputation of characteristic values: Functions like mean, median, mode, standard deviation, or interquartile range\nModeling: Operations like linear regression or dimensionality reduction. We haven’t talked about it, but will do soon."
  },
  {
    "objectID": "W05.html#systematic-but-no-standard-routine",
    "href": "W05.html#systematic-but-no-standard-routine",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Systematic but no standard routine",
    "text": "Systematic but no standard routine\n\n“There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox\n\n\n“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey"
  },
  {
    "objectID": "W05.html#systematic-but-no-standard-routine-1",
    "href": "W05.html#systematic-but-no-standard-routine-1",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Systematic but no standard routine",
    "text": "Systematic but no standard routine\n\nGoal of EDA: Develop understanding of your data.\nEDA’s iterative cycle\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\n\nEDA is fundamentally a creative process."
  },
  {
    "objectID": "W05.html#questions",
    "href": "W05.html#questions",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Questions",
    "text": "Questions\n\nThe way to ask quality questions:\n\nGenerate many questions!\nYou cannot come up with most interesting questions when you start.\n\nThere is no rule which questions to ask. These are useful\n\nWhat type of variation occurs within my variables?\n(Barplots, Histograms,…)\nWhat type of covariation occurs between my variables?\n(Scatterplots, Timelines,…)"
  },
  {
    "objectID": "W05.html#eda-embedded-in-a-statistical-data-science-project",
    "href": "W05.html#eda-embedded-in-a-statistical-data-science-project",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "EDA embedded in a statistical data science project",
    "text": "EDA embedded in a statistical data science project\n\nStating and refining the question\nExploring the data\nBuilding formal statistical models\nInterpreting the results\nCommunicating the results\n\n\n\nRoger D. Peng and Elizabeth Matsui. “The Art of Data Science.” A Guide for Anyone Who Works with Data. Skybrude Consulting, LLC (2015)."
  },
  {
    "objectID": "W05.html#example-eda-strange-airports",
    "href": "W05.html#example-eda-strange-airports",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Example EDA: Strange Airports",
    "text": "Example EDA: Strange Airports\nFrom Homework 02:\n\nlibrary(nycflights13)\nggplot(data = airports, mapping = aes(x = lon, y = lat)) + geom_point(aes(color = tzone)) \n\n\n\nairports %>% filter(lon >= 0) \n\n# A tibble: 4 × 8\n  faa   name                            lat   lon   alt    tz dst   tzone       \n  <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr> <chr>       \n1 DVT   Deer Valley Municipal Airport  33.4 112.   1478     8 A     Asia/Chongq…\n2 EEN   Dillant Hopkins Airport        72.3  42.9   149    -5 A     <NA>        \n3 MYF   Montgomery Field               32.5 118.     17     8 A     Asia/Chongq…\n4 SYA   Eareckson As                   52.7 174.     98    -9 A     America/Anc…"
  },
  {
    "objectID": "W05.html#airport-errors",
    "href": "W05.html#airport-errors",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Airport errors",
    "text": "Airport errors\n\n\n# A tibble: 4 × 8\n  faa   name                            lat   lon   alt    tz dst   tzone       \n  <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr> <chr>       \n1 DVT   Deer Valley Municipal Airport  33.4 112.   1478     8 A     Asia/Chongq…\n2 EEN   Dillant Hopkins Airport        72.3  42.9   149    -5 A     <NA>        \n3 MYF   Montgomery Field               32.5 118.     17     8 A     Asia/Chongq…\n4 SYA   Eareckson As                   52.7 174.     98    -9 A     America/Anc…\n\n\n\nCorrect locations (internet research and location of maps):\n\n\n\nDeer Valley Municipal Airport: Phoenix\n33°41′N 112°05′W Missing minus for lon (W)\nDillant Hopkins Airport: New Hampshire\n42°54′N 72°16′W lon-lat switched, minus (W)\nMontgomery Field: San Diego\n32°44′N 117°11″W Missing minus for lon (W)\nEareckson As: Alaska\n52°42′N 174°06′E No error: Too west,it’s east!"
  },
  {
    "objectID": "W05.html#conclusions-on-data-errors",
    "href": "W05.html#conclusions-on-data-errors",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Conclusions on data errors",
    "text": "Conclusions on data errors\n\nIn real-world datasets errors like the 3 airport are quite common.\nErrors of this type are often hard to detect and remain unnoticed.\n\nThis can (but need not) change results drastically!\n\n\n\nConclusions\n\nAlways remain alert for inconsistencies and be ready to check the plausibility of results.\nSkills in exploratory data analysis (EDA) are essential to find errors and explore their nature and implication\nErrors are unpredictable, of diverse types, and are often deeply related to the reality the data presents.\n\nThis is one reason why EDA can not be a fully formalized and automatized process."
  },
  {
    "objectID": "W05.html#six-types-of-questions",
    "href": "W05.html#six-types-of-questions",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Six types of questions",
    "text": "Six types of questions\n\nDescriptive: summarize a characteristic of a set of data\nExploratory: analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\nInferential: analyze patterns, trends, or relationships in representative data from a population\nPredictive: make predictions for individuals or groups of individuals\nCausal: whether changing one factor will change another factor, on average, in a population\nMechanistic: explore “how” as opposed to whether\n\n\nWe only did 1 and 2, so far.\n\n\n\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146."
  },
  {
    "objectID": "W05.html#descriptive-projects",
    "href": "W05.html#descriptive-projects",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Descriptive Projects",
    "text": "Descriptive Projects\n\n\n\nDubin (1969). Theory Building - A Practical Guide to the Construction and Testing of Theoretical Models"
  },
  {
    "objectID": "W05.html#data-analysis-flowchart",
    "href": "W05.html#data-analysis-flowchart",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Data Analysis Flowchart",
    "text": "Data Analysis Flowchart"
  },
  {
    "objectID": "W05.html#example-covid-19-and-vitamin-d",
    "href": "W05.html#example-covid-19-and-vitamin-d",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Example: COVID-19 and Vitamin D",
    "text": "Example: COVID-19 and Vitamin D\n\n\nDescriptive: frequency of hospitalisations due to COVID-19 in a set of data collected from a group of individuals\nExploratory: examine relationships between a range of dietary factors and COVID-19 hospitalisations\nInferential: examine whether any relationship between taking Vitamin D supplements and COVID-19 hospitalisations found in the sample hold for the population at large\nPredictive: what types of people will take Vitamin D supplements during the next year\nCausal: whether people with COVID-19 who were randomly assigned to take Vitamin D supplements or those who were not are hospitalised\nMechanistic: how increased vitamin D intake leads to a reduction in the number of viral illnesses"
  },
  {
    "objectID": "W05.html#questions-to-questions",
    "href": "W05.html#questions-to-questions",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Questions to questions",
    "text": "Questions to questions\n\nDo you have appropriate data to answer your question?\nDo you have information on confounding variables?\nWas the data you’re working with collected in a way that introduces bias?\n\n\n\nExample\nI want to estimate the average number of children in households in Bremen. I conduct a survey at an elementary school and ask pupils how many children, including themselves, live in their house. Then, I take the average of the responses.\n\nIs this a biased or an unbiased estimate of the number of children in households in Bremen?\nIf biased, will the value be an overestimate or underestimate?"
  },
  {
    "objectID": "W05.html#context-information-is-important",
    "href": "W05.html#context-information-is-important",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Context Information is important!",
    "text": "Context Information is important!\n\nNot all information is in the data!\nPotential confounding variables you infer from general knowledge\nInformation about data collection you may receive from an accompanying report\nInformation about computed variables you may need to look up in accompanying documentation\nInformation about certain variables you may find in an accompanying codebook. For example the exact wording of questions in survey data."
  },
  {
    "objectID": "W05.html#data-science-projects-in-the-course",
    "href": "W05.html#data-science-projects-in-the-course",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Data Science Projects in the Course",
    "text": "Data Science Projects in the Course\n\n\nA project report is the main assessment for the Data Science Tools module.\nThis week more homework repositories will be released.\nThese homework repositories may be updated with new tasks once new concepts have been treated.\nTopics will for example be\n\nCOVID-19\nGermany’s income distribution and income tax\nFuel prices in Western Australia\nPolitical Attitudes in Germany\n…\n\nThe repositories mimick data science projects. Later you will choose a data science project on your own. It can build on these topics or on different datasets which you find interesting."
  },
  {
    "objectID": "W05.html#pca-description",
    "href": "W05.html#pca-description",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "PCA Description",
    "text": "PCA Description\nPrinciple component analysis\n\nis a dimensionality-reduction technique, that means it can be used to reduce the number of variables\ncomputes new variables which represent the data in a different way\ntransforms the data linearly to a new coordinate system where most of the variation in the data can be described with fewer variables than the original data\n\nToday: Quick walk through how to use and interpret it."
  },
  {
    "objectID": "W05.html#example-numerical-variables-of-penguins",
    "href": "W05.html#example-numerical-variables-of-penguins",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Example: Numerical variables of penguins",
    "text": "Example: Numerical variables of penguins\n\npeng <- \n penguins |> \n select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |> \n na.omit()\npeng |> count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      151\n2 Chinstrap    68\n3 Gentoo      123\n\n\nWe have 342 penguins and 4 numeric variables."
  },
  {
    "objectID": "W05.html#two-variables",
    "href": "W05.html#two-variables",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Two Variables",
    "text": "Two Variables\nExample for the new axes.\n\n\n\n\n\n\n\nThe two arrows show the two eigenvectors of the covariance matrix of the two variables scaled by the square root of the corresponding eigenvalues, and shifted so their origins are at the means of both variables."
  },
  {
    "objectID": "W05.html#computation-in-r",
    "href": "W05.html#computation-in-r",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Computation in R",
    "text": "Computation in R\nThe basic function is base-R’s prcomp (there is an older princomp which is not advisable to use).\n\n# prcomp can take a data frame with all numerical vectors as 1st argument\nP <- peng |> select(flipper_length_mm, bill_length_mm) |> prcomp()\n\n\n\nThe base output\n\nP\n\nStandard deviations (1, .., p=2):\n[1] 14.549388  3.981729\n\nRotation (n x k) = (2 x 2):\n                        PC1        PC2\nflipper_length_mm 0.9637169 -0.2669266\nbill_length_mm    0.2669266  0.9637169\n\n\n\nThe summary output\n\nsummary(P)\n\nImportance of components:\n                           PC1     PC2\nStandard deviation     14.5494 3.98173\nProportion of Variance  0.9303 0.06968\nCumulative Proportion   0.9303 1.00000"
  },
  {
    "objectID": "W05.html#the-prcomp-object",
    "href": "W05.html#the-prcomp-object",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "The prcomp object",
    "text": "The prcomp object\nIncludes 4 different related entities.\n\n\nThe standard deviations related to each principal component.\n\nP$sdev\n\n[1] 14.549388  3.981729\n\n\nThe matrix of variable loadings. (It is also the matrix which rotates the original data vectors.)\n\nP$rotation\n\n                        PC1        PC2\nflipper_length_mm 0.9637169 -0.2669266\nbill_length_mm    0.2669266  0.9637169\n\n\n\nThe means for each original variable.\n\nP$center\n\nflipper_length_mm    bill_length_mm \n        200.91520          43.92193 \n\n\nNote, there are also standard deviations of original variables in $scale when this is set to be used.\nThe centered (scaled, if set) and rotated data.\n\nP$x\n\n               PC1         PC2\n  [1,] -20.4797199  0.66892267\n  [2,] -15.5543649 -0.28022355\n  [3,]  -6.6673719 -1.91158941\n  [4,]  -9.5557414 -4.84711693\n  [5,] -11.7528828 -1.54067330\n  [6,] -20.5331052  0.47617930\n  [7,]  -6.9609911 -2.97167796\n  [8,] -10.2497505 -7.35278078\n  [9,] -11.0321810  1.06136223\n [10,] -16.0081402 -1.91854222\n [11,] -21.7904413 -0.31698266\n [12,] -18.9821498  2.32942980\n [13,] -10.9760146 -2.48220170\n [14,]  -5.2977029 -8.20555532\n [15,] -17.2921689 -2.80807586\n [16,]  -7.0944544 -3.45353639\n [17,]  -4.1526997 -0.32526550\n [18,] -18.8431243 -4.66132637\n [19,]  -6.1096072  3.84852331\n [20,] -27.5727425  1.28457691\n [21,] -21.8171340 -0.41335434\n [22,] -13.6241501 -4.55038405\n [23,] -16.8650864 -1.26612888\n [24,] -21.5235147  0.64673421\n [25,] -15.7117398 -4.59476098\n [26,] -18.1518963  1.58064478\n [27,] -14.3237215  0.41656672\n [28,] -29.4734836  1.91480178\n [29,] -21.0697395  2.28505287\n [30,] -23.2640999  1.85518920\n [31,] -23.8780310 -0.36135959\n [32,] -13.6269312 -0.81407674\n [33,] -17.1081014  1.60283324\n [34,]  -7.7083856 -5.67008518\n [35,]  -5.9972743 -3.23860456\n [36,] -11.8863461 -2.02253174\n [37,] -20.6159643  3.92337154\n [38,] -20.8801098 -0.77665262\n [39,] -17.4017207  0.54274469\n [40,] -20.2100122 -2.10366777\n [41,]  -6.5339086 -1.42973098\n [42,] -16.4886080 -3.65323258\n [43,]  -4.6893340  1.48360808\n [44,] -17.1853983 -2.42258912\n [45,] -11.6728048 -1.25155824\n [46,] -18.9821498  2.32942980\n [47,] -22.8342362 -0.33917112\n [48,] -12.6337406 -4.72093895\n [49,]  -9.9883862  1.08355069\n [50,] -15.5276723 -0.18385187\n [51,] -13.4667753 -0.23584662\n [52,] -12.9006672 -5.68465582\n [53,]  -1.3950124 -1.60790371\n [54,] -15.9252810 -5.36573447\n [55,] -10.2286201  0.21620552\n [56,] -15.6878282 -0.76208199\n [57,]  -8.5147276 -1.08862116\n [58,] -21.1737290 -1.83674117\n [59,]  -8.3517906 -4.24669835\n [60,] -17.5324029 -3.67542104\n [61,]  -6.4004453 -0.94787255\n [62,] -17.0252423 -1.84435900\n [63,]  -9.3449812 -0.33983614\n [64,] -18.3092711 -2.73389264\n [65,]  -9.2115179  0.14202229\n [66,]  -7.9486195 -6.53743036\n [67,] -13.1998487  0.72787024\n [68,] -12.6604332 -4.81731064\n [69,]  -3.3758314 -1.26679390\n [70,] -13.3010571 -7.13023111\n [71,] -11.6461122 -1.15518656\n [72,]  -5.8905036 -2.85311781\n [73,]  -3.2718419  2.85500015\n [74,] -12.7672039 -5.20279739\n [75,]  -6.0000554  0.49770275\n [76,] -10.3620834 -0.26565292\n [77,] -18.0957298 -1.96291915\n [78,] -15.4715058 -3.72741580\n [79,]  -6.1869040 -0.17689906\n [80,] -13.9711547 -5.80321597\n [81,]  -5.0096459  0.32714784\n [82,] -15.3380425 -3.24555737\n [83,]  -9.9828239 -6.38906391\n [84,] -11.3230191 -3.73503363\n [85,]  -7.3641622 -0.68094595\n [86,] -12.5536626 -4.43182390\n [87,] -13.3572235 -3.58666718\n [88,] -12.9835263 -2.23746357\n [89,] -11.8596534 -1.92616005\n [90,]  -1.1492162 -8.21317314\n [91,]   3.1833380 -3.80988186\n [92,] -17.9861781 -5.31373971\n [93,] -15.5276723 -0.18385187\n [94,] -15.4715058 -3.72741580\n [95,]   5.9944106 -4.89977671\n [96,] -12.0731947 -2.69713354\n [97,]  -5.7036550 -2.17851601\n [98,] -24.9724301 -4.31259873\n [99,]  -8.7844354  1.68396928\n[100,] -10.9732334 -6.21850901\n[101,]   1.2292116 -3.37240036\n[102,] -18.9259834 -1.21413413\n[103,] -12.1532727 -2.98624860\n[104,]  -9.2354294 -3.69065670\n[105,] -17.4284133  0.44637301\n[106,]  -3.2662796 -4.61761446\n[107,] -12.0465021 -2.60076185\n[108,] -20.7466465 -0.29479419\n[109,]  -3.9658510  0.34933630\n[110,]  -4.3634598 -4.83254629\n[111,]  -9.1075284  4.26381634\n[112,]  -8.7549616 -1.95596634\n[113,]  -4.2327776 -0.61438056\n[114,] -10.7090880 -1.51848484\n[115,]  -5.0630312  0.13440447\n[116,] -13.8671651 -1.68142192\n[117,]  -3.6132842 -5.87044638\n[118,] -13.6775354 -4.74312742\n[119,] -12.2361318  0.46094364\n[120,] -15.4715058 -3.72741580\n[121,]  -4.4702304 -5.21803304\n[122,] -25.0046850  3.06364419\n[123,]   0.3722654 -2.71998702\n[124,] -16.7021493 -4.42420607\n[125,]  -2.7324265 -2.69018073\n[126,] -10.9226292 -2.28945833\n[127,]  -6.3470600 -0.75512918\n[128,] -10.8692439 -2.09671496\n[129,]   8.8027021 -2.25336424\n[130,] -11.9664241 -2.31164679\n[131,]  -3.9925437  0.25296462\n[132,]  -9.5290487 -4.75074525\n[133,]  -3.5598989 -5.67770301\n[134,] -14.9643453 -1.89635376\n[135,] -11.2724149  0.19401705\n[136,] -11.7767943 -5.37335229\n[137,]  -1.8754802 -3.34259407\n[138,] -17.1853983 -2.42258912\n[139,]  -8.7549616 -1.95596634\n[140,]  -8.6214983 -1.47410791\n[141,] -14.2970288  0.51293840\n[142,] -15.6021880 -7.94558153\n[143,] -11.3791856 -0.19146969\n[144,] -10.3593023 -4.00196022\n[145,] -16.6515451 -0.49515539\n[146,] -11.7795755 -1.63704499\n[147,] -18.2558858 -2.54114927\n[148,]  -7.8151562 -6.05557193\n[149,]  -9.2621221 -3.78702838\n[150,] -15.5248912 -3.92015917\n[151,]  -0.5647588 -2.35668874\n[152,]  10.3002722 -0.59285711\n[153,]  29.6519063 -1.90596663\n[154,]  10.0305645  2.17973333\n[155,]  18.0873039  1.29715250\n[156,]  14.5555295 -0.21498819\n[157,]   9.4433259  0.05955623\n[158,]  10.1134236 -1.26745892\n[159,]  18.1701630 -2.15003975\n[160,]   7.6254440 -2.75741114\n[161,]  14.3419882 -0.98596168\n[162,]  11.8034045 -6.40496458\n[163,]  15.8929436  0.86728882\n[164,]  13.0312668 -1.97186701\n[165,]  12.8416371  1.08983849\n[166,]   9.2564773 -0.61504558\n[167,]  16.9367385  0.88947729\n[168,]   8.2421563 -4.27716966\n[169,]  20.7649133 -0.27460078\n[170,]   8.3995311  0.03736776\n[171,]  21.5951668 -1.02338580\n[172,]  18.1406893  1.48989587\n[173,]  13.8882130 -2.62428035\n[174,]  12.3344765 -0.74122355\n[175,]  14.2085249 -1.46782012\n[176,]  13.3009745 -4.74445745\n[177,]  14.1551396 -1.66056349\n[178,]  14.6917739 -3.46943706\n[179,]  14.6089148 -0.02224482\n[180,]   9.8971012  1.69787490\n[181,]  20.0147377  0.76329931\n[182,]  21.2214696 -2.37258941\n[183,]   7.4919807 -3.23926957\n[184,]   6.1784781 -0.48886760\n[185,]  32.2144016  7.34571526\n[186,]  19.7745037 -0.10404587\n[187,]  19.5876551 -0.77864767\n[188,]  11.2934628 -4.49971932\n[189,]  17.5562319 -4.36658853\n[190,]   6.8485757 -1.81588274\n[191,]   8.1031307  2.71358652\n[192,]   6.5015712 -3.06871466\n[193,]  24.7265513 -0.95682041\n[194,]   9.1230140 -1.09690401\n[195,]  16.0530996  1.44551894\n[196,]  22.0756347  0.71130455\n[197,]  15.4152569 -4.60370884\n[198,]   9.1763994 -0.90416063\n[199,]  24.9667853 -0.08947523\n[200,]  11.9073940 -2.28317054\n[201,]  13.9149057 -2.52790867\n[202,]   9.4700186  0.15592792\n[203,]  19.6143478 -0.68227599\n[204,]   9.0696287 -1.28964738\n[205,]  24.8600146 -0.47496198\n[206,]  16.1893440 -1.80892993\n[207,]  18.6801047 -4.05528501\n[208,]   6.7951904 -2.00862611\n[209,]  18.8135680 -3.57342658\n[210,]   6.6350345 -2.58685623\n[211,]  23.9763758  0.08107968\n[212,]   7.1955803 -0.56305082\n[213,]  19.9641335 -3.16575137\n[214,]  13.0846521 -1.77912364\n[215,]  31.7634075  1.97108929\n[216,]  17.9299291 -3.01738492\n[217,]  29.5985210 -2.09871001\n[218,]  13.2181154 -1.29726521\n[219,]  28.5547261 -2.12089847\n[220,]  18.2797148 -5.50086030\n[221,]  23.0927369  0.63712133\n[222,]  15.5459390 -0.38554310\n[223,]  20.0175188 -2.97300799\n[224,]  20.4979867 -1.23831764\n[225,]  16.1893440 -1.80892993\n[226,]  15.1989345 -1.63837502\n[227,]  29.2782091 -3.25517024\n[228,]   8.7465357  1.29019969\n[229,]  20.3083569  1.82338786\n[230,]  13.9149057 -2.52790867\n[231,]  21.6246406 -4.66332142\n[232,]  12.0647688  2.03136689\n[233,]  21.6457710  2.90566487\n[234,]  11.6109936  0.39304822\n[235,]  23.8696051 -0.30440707\n[236,]  10.9436771 -2.01624394\n[237,]  27.9380138 -0.60113995\n[238,]  16.3255884 -5.06337880\n[239,]  18.4343085  2.54998442\n[240,]  11.6376863  0.48941990\n[241,]  30.2124521  0.11783878\n[242,]  17.4199874 -1.11213966\n[243,]  28.3117111  0.74806366\n[244,]  11.1038331 -1.43801382\n[245,]  23.7361418 -0.78626550\n[246,]  12.7643402 -2.93558388\n[247,]  26.0105801 -0.06728677\n[248,]  15.9997143  1.25277557\n[249,]  21.1146989 -2.75807616\n[250,]   3.2044684  3.75910443\n[251,]  25.1269412  0.48875489\n[252,]  18.6506309 -0.41534939\n[253,]  29.2993395  4.31381605\n[254,]  14.4487589 -0.60047494\n[255,]  27.4842386 -2.23945862\n[256,]  15.4391684 -0.77102985\n[257,]  14.3419882 -0.98596168\n[258,]   8.1620783 -4.56628472\n[259,]  19.9585712  4.30686324\n[260,]   6.6617271 -2.49048455\n[261,]   8.9066916  1.86842981\n[262,]  16.2933335  2.31286412\n[263,]  28.6348041 -1.83178341\n[264,]  11.5336968 -3.63237414\n[265,]  30.0522962 -0.46039134\n[266,]  16.1092660 -2.09804499\n[267,]  31.0132319  3.00898937\n[268,]  15.6554908 -3.73636366\n[269,]  21.6218595 -0.92701412\n[270,]  13.4850420 -0.33354834\n[271,]  14.3419882 -0.98596168\n[272,]  22.0489420  0.61493287\n[273,]  11.0237551 -1.72712888\n[274,]  13.2420270  2.53541378\n[275,]  -7.9035776  4.86423493\n[276,]  -3.1144671  7.16953757\n[277,]  -5.6586131  9.22314928\n[278,] -12.0520643  4.87185275\n[279,]  -1.4300484  9.50464651\n[280,]  -2.4682810  2.00984344\n[281,] -21.5023843  8.21572050\n[282,]  -1.8037456  8.15544290\n[283,]  -5.1458903  3.58159671\n[284,]  -0.8400288  7.88851631\n[285,]  -6.9131681  4.69368002\n[286,]  -4.5881256  9.34170943\n[287,] -14.5161323  7.21457952\n[288,]   2.2379704  7.76233833\n[289,]  -9.9911673  4.81985800\n[290,]   1.8375806  6.31676303\n[291,]  -2.0706722  7.19172604\n[292,] -15.4348073 18.88317139\n[293,]  -9.8577040  5.30171643\n[294,]  -4.2917252  6.66549067\n[295,] -19.5988621  3.84918832\n[296,]  -8.3334413  7.05859525\n[297,] -13.6030197  3.01860225\n[298,]  -5.8454617  8.54854747\n[299,]  -4.9590417  4.25619852\n[300,]  -1.6168970  8.83004470\n[301,]   0.8738637  6.58368963\n[302,]   0.6069371  5.61997276\n[303,]  -8.8939871  5.03478983\n[304,]   6.3063792  7.46560544\n[305,] -14.2169508  0.80205346\n[306,]   2.8252089  9.88251543\n[307,] -13.7898683  2.34400044\n[308,]   3.8984776  6.26476828\n[309,]  -4.1582619  7.14734911\n[310,]  -0.8906330  3.95946563\n[311,]  -4.7188078  5.12354369\n[312,]  10.9114222  5.35999898\n[313,]  -7.7968070  5.24972167\n[314,]   6.4932278  8.14020725\n[315,]  10.1106424  2.46884839\n[316,] -12.8022399  5.90975284\n[317,]  -2.8742331  8.03688275\n[318,]  -4.3156367  2.83281168\n[319,]  -2.8742331  8.03688275\n[320,]   1.9176585  6.60587809\n[321,]  -8.8700756  8.86746882\n[322,]  12.0380762  1.93499520\n[323,] -11.3875289 11.01745222\n[324,]  -1.2404187  6.44294101\n[325,]  -0.7304770  4.53769575\n[326,]   2.0778145  7.18410821\n[327,]  -7.1534020  3.82633484\n[328,]   3.8183996  5.97565322\n[329,] -13.7898683  2.34400044\n[330,]  -1.5635117  9.02278808\n[331,]  -9.2142990  3.87832960\n[332,]   3.4447024  4.62644961\n[333,]   2.7212194  5.76072138\n[334,]  -6.2163778  3.46303656\n[335,]   7.0298621  6.33133367\n[336,] -10.7146502  5.95412977\n[337,]  -5.2259683  3.29248165\n[338,]   9.0345927  9.82290284\n[339,]   0.9328113 -0.69618161\n[340,]  -6.1123883  7.58483061\n[341,]  10.5911103  4.20353874\n[342,]  -1.1336480  6.82842776"
  },
  {
    "objectID": "W05.html#pca-as-exploratory-data-analysis",
    "href": "W05.html#pca-as-exploratory-data-analysis",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "PCA as Exploratory Data Analysis",
    "text": "PCA as Exploratory Data Analysis\nSuppose we do a PCA with all 342 penguins (rows) and all 4 numeric variables.\n\nHow long will the vector of standard deviations be? 4\n\nWhat dimensions will the rotation matrix have? 4 x 4\n\nWhat dimensions will the rotated data frame have? 342 x 4\n\n\nWhen we do a PCA for exploration there are 3 things to look at:\n\nThe data in PC coordinates - the centered (scaled, if set) and rotated data.\n\nThe rotation matrix - the variable loadings.\nThe variance explained by each PC - based on the standard deviations."
  },
  {
    "objectID": "W05.html#all-variables",
    "href": "W05.html#all-variables",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "All variables",
    "text": "All variables\nNow, with scale = TRUE (recommended). Data will be centered and scaled (a.k.a. standardized) first.\n\npeng_PCA <- peng |> select(-species) |> \n prcomp(scale = TRUE)\npeng_PCA\n\nStandard deviations (1, .., p=4):\n[1] 1.6594442 0.8789293 0.6043475 0.3293816\n\nRotation (n x k) = (4 x 4):\n                         PC1          PC2        PC3        PC4\nbill_length_mm     0.4552503 -0.597031143 -0.6443012  0.1455231\nbill_depth_mm     -0.4003347 -0.797766572  0.4184272 -0.1679860\nflipper_length_mm  0.5760133 -0.002282201  0.2320840 -0.7837987\nbody_mass_g        0.5483502 -0.084362920  0.5966001  0.5798821"
  },
  {
    "objectID": "W05.html#explore-data-in-pc-coordinates",
    "href": "W05.html#explore-data-in-pc-coordinates",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Explore data in PC coordinates",
    "text": "Explore data in PC coordinates\n\n\n\nStart plotting PC1 against PC2. By default these are the most important ones. Drill deeper later.\nAppend the original data. Here used to color by species.\n\n\n\nplotdata <- peng_PCA$x |> as_tibble() |> bind_cols(peng)\nplotdata |> ggplot(aes(PC1, PC2, color = species)) +\n geom_point() +\n coord_fixed() + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W05.html#variable-loadings",
    "href": "W05.html#variable-loadings",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Variable loadings",
    "text": "Variable loadings\n\nThe columns of the rotation matrix shows how the original variables load on the principle components.\nWe can try to interpret these loadings and give descriptive names to principal components.\ntidy extracts the rotation matrix in long format with a PC, a column (for the original variable name), and a value variable.\n\n\npeng_PCA$rotation |> as_tibble(rownames = \"variable\") |> \n pivot_longer(starts_with(\"PC\"), names_to = \"PC\", values_to = \"value\") |> \n ggplot(aes(value, variable)) + geom_col() + geom_vline(xintercept = 0, color = \"blue\") +\n facet_wrap(~PC, nrow = 1) + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W05.html#variance-explained",
    "href": "W05.html#variance-explained",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Variance explained",
    "text": "Variance explained\n\nPrinciple components are by default sorted by importance.\nThe squares of the standard deviation for each component gives its variances and variances have to sum up to the sum of the variances of the original variables.\n\nWhen original variables were standardized their original variances are all each one. Consequently, the variances of the principal components sum up to the number of original variables.\n\nA typical plot to visualize the importance of the components is to plot the percentage of the variance explained by each component.\n\n\ntibble(PC = 1:4, sdev = peng_PCA$sdev) |> \n mutate(percent = sdev^2/sum(sdev^2) * 100) |>\n ggplot(aes(PC, percent)) + geom_col() + theme_grey(base_size = 20)"
  },
  {
    "objectID": "W05.html#interpretations-1",
    "href": "W05.html#interpretations-1",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretations (1)",
    "text": "Interpretations (1)\n\n\nThe first component explains almost 70% of the variance. So most emphasize should be on this.\nThe first two explain about 88% of the total variance."
  },
  {
    "objectID": "W05.html#interpretations-2",
    "href": "W05.html#interpretations-2",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretations (2)",
    "text": "Interpretations (2)\n\n\nTo score high on PC1 a penguin needs to be generally large but with low bill depth.\nPenguins scoring high on PC2 are penguins with generally small bills."
  },
  {
    "objectID": "W05.html#interpretations-3",
    "href": "W05.html#interpretations-3",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretations (3)",
    "text": "Interpretations (3)"
  },
  {
    "objectID": "W05.html#apply-pca",
    "href": "W05.html#apply-pca",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Apply PCA",
    "text": "Apply PCA\n\nBesides standardization, PCA may benefit by preprocessing steps of data transformation with variables with skew distributions (log, square-root, or Box-Cox transformation). This may result in less outliers.\nPCA is a often a useful step of exploratory data analysis when you have a large number of numerical variables to show the empirical dimensionality of the data and its structure\nLimitation: PCA is only sensitive for linear relation ships (no U-shaped) or the like\nThe principal components can be used as predictors in a model instead of the raw variables."
  },
  {
    "objectID": "W05.html#properties-of-pca",
    "href": "W05.html#properties-of-pca",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Properties of PCA",
    "text": "Properties of PCA\n\n\nThe principal components (the columns of the rotation matrix) are maximally uncorrelated (actually they are even orthogonal).\nThis also holds for the columns of the rotated data.\nThe total variances of all prinicipal components sum up to the number of variables (when variables are standardized)\nThe PCA is unique. All principle components together are a complete representation of the data. (Unlike other technique of dimensionality reduction which may rely on starting values, random factors, or tuning parameters)"
  },
  {
    "objectID": "W05.html#relations-of-pca",
    "href": "W05.html#relations-of-pca",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Relations of PCA",
    "text": "Relations of PCA\n\n\nA technique similar in spirit is factor analysis (e.g. factanal). It is more theory based as it requires to specify to the theoriezed number of factors up front.\nPCA is an example of the importance of linear algebra (“matrixology”) in data science techniques.\n\nPCA is based on the eigenvalue decomposition of the covariance matrix (or correlation matrix in the standardized case) of the data."
  },
  {
    "objectID": "W05.html#epidemic-modeling",
    "href": "W05.html#epidemic-modeling",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Epidemic Modeling",
    "text": "Epidemic Modeling\n\nAssume a population of N individuals.\n\nIndividuals can have different states, e.g.: Susceptible, Infectious, Recovered, …\nThe population divides into compartments of these states which change over time, e.g.: \\(S(t), I(t), R(t)\\) number of susceptible, infectious, recovered individuals\n\nNow we define dynamics like\n\nwhere the numbers on the arrows represent transition probabilities."
  },
  {
    "objectID": "W05.html#si-model",
    "href": "W05.html#si-model",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "SI model",
    "text": "SI model\nToday we only treat the SI part of the model.\n\nPeople who are susceptible can become infected through contact with infectious.\nPeople who are infectious stay infectious\n\nThe parameter \\(\\beta\\) is the average number of contacts per unit time multiplied with the probability that an infection happens during such a contact.\nTwo compartments:\n\\(S(t)\\) is the number of susceptible people at time \\(t\\).\n\\(I(t)\\) is the number of infected people at time \\(t\\).\nIt always holds \\(S(t) + I(t) = N\\). (The total population is constant.)"
  },
  {
    "objectID": "W05.html#how-many-infections-per-time",
    "href": "W05.html#how-many-infections-per-time",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "How many infections per time?",
    "text": "How many infections per time?\nThe change of the number of infectious\n\\[\\frac{dI}{dt} = \\underbrace{\\beta}_\\text{infection prob.} \\cdot \\underbrace{\\frac{S}{N}}_\\text{frac. of $S$ still there} \\cdot \\underbrace{\\frac{I}{N}}_\\text{frac. $I$ to meet} \\cdot N = \\frac{\\beta\\cdot S\\cdot I}{N}\\]\nwhere \\(dI\\) is the change of \\(I\\) (the newly infected here) and \\(dt\\) the time interval.\nInterpretation: The newly infected are from the fraction of susceptible times the probability that they meet an infected times the infection probability times the total number of individuals.\nUsing \\(S = N - I\\) we rewrite\n\\[\\frac{dI}{dt} = \\frac{\\beta (N-I)I}{N}\\]"
  },
  {
    "objectID": "W05.html#ordinary-differential-equation",
    "href": "W05.html#ordinary-differential-equation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Ordinary differential equation",
    "text": "Ordinary differential equation\nWe interpret \\(I(t)\\) as a function of time which gives us the number of infectious at each point in time. The change function is now\n\\[\\frac{dI(t)}{dt} = \\frac{\\beta (N-I(t))I(t)}{N}\\]\nand \\(\\frac{dI(t)}{dt}\\) is also called the derivative of \\(I(t)\\)."
  },
  {
    "objectID": "W05.html#derivatives",
    "href": "W05.html#derivatives",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Derivatives",
    "text": "Derivatives\n\n\n\nThe derivative of a function is also a function with the same domain.\nMeasures the sensitivity to change of the function output when the input changes (a bit)\nExample from physics: The derivative of the position of a moving object is its speed. The derivative of its speed is its acceleration.\nGraphically: The derivative is the slope of a tangent line of the graph of a function."
  },
  {
    "objectID": "W05.html#differentiation",
    "href": "W05.html#differentiation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Differentiation",
    "text": "Differentiation\nis the process to compute the derivative. For parameters \\(a\\) and \\(b\\) and other functions \\(g\\) and \\(h\\), rules of differentiation are\n\n\nFunction \\(f(x)\\)\n\\(a\\cdot x\\)\n\\(b\\)\n\\(x^2,\\ x^{-1} = \\frac{1}{x},\\ x^k\\)\n\\(g(x) + h(x)\\)\n\\(g(x)\\cdot h(x)\\)\n\\(g(h(x))\\)\n\\(e^x,\\ 10^x = e^{\\log(10)x}\\)\n\\(\\log(x)\\)\n\nIts derivative \\(\\frac{df(x)}{dx}\\) or \\(\\frac{d}{dx}f(x)\\) or \\(f'(x)\\)\n\n\\(a\\)\n\n\n\\(0\\)\n\n\n\\(2\\cdot x,\\ -x^{-2} = -\\frac{1}{x^2},\\ k\\cdot x^{k-1}\\)\n\n\n\\(g'(x) + h'(x)\\)\n\n\n\\(g'(x)\\cdot h(x) + g(x)\\cdot h'(x)\\) (product rule)\n\n\n\\(g'(h(x))\\cdot h'(x)\\) (chain rule)\n\n\n\\(e^x,\\ 10^x = \\log(10)\\cdot10^x\\)\n\n\n\\(\\frac{1}{x}\\) (This is a “surprising” relation …)"
  },
  {
    "objectID": "W05.html#differential-equation",
    "href": "W05.html#differential-equation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Differential equation",
    "text": "Differential equation\nIn a differential equation the unknown is a function!\nWe are looking for a function which derivative is a function of the function itself.\nExample: SI-model\n\\[\\frac{dI(t)}{dt} = \\frac{\\beta (N-I(t))I(t)}{N}\\]\nWhich function \\(I(t)\\) fulfills this equation?\nThe analytical solution1 is\n\\(I(t) = \\frac{N}{1 + (\\frac{N}{I(0)} - 1)e^{-\\beta t}}\\)\nWhich is called the logistic equation.\nNote, we need to specify the initial number of infectious individuals \\(I(0)\\).\nCan you check that this is correct? If you want but can’t, don’t hesitate to ask later."
  },
  {
    "objectID": "W05.html#si-model-logistic-equation",
    "href": "W05.html#si-model-logistic-equation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "SI-model: Logistic Equation",
    "text": "SI-model: Logistic Equation\n\\(I(t) = \\frac{N}{1 + (\\frac{N}{I(0)} - 1)e^{-\\beta t}}\\)\nPlot the equation for \\(N = 10000\\), \\(I_0 = 1\\), and \\(\\beta = 0.3\\)\n\nN <- 10000\nI0 <- 1\nbeta <- 0.3\nggplot() + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ) + \n xlim(c(0,75))"
  },
  {
    "objectID": "W05.html#si-model-numerical-integration",
    "href": "W05.html#si-model-numerical-integration",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "SI-model: Numerical integration",
    "text": "SI-model: Numerical integration\nAnother way of solution using, e.g., using Euler’s method.\nWe compute the solution step-by-step using small increments of, e.g. \\(dt = 0.5\\).\n\n\nN <- 10000\nI0 <- 1\ndI <- function(I,N,b) b*I*(N - I)/N\nbeta <- 0.3\ndt <- 0.5 # time increment, supposed to be infinitesimally small\ntmax <- 75\nt <- seq(0,tmax,dt) # this is the vector of timesteps\nIt <- I0 # this will become the vector of the number infected I(t) over time\nfor (i in 2:length(t)) { # We iterate over the vector of time steps and incrementally compute It\n  It[i] = It[i-1] + dt * dI(It[i-1], N, beta) # This is called Euler's method\n}\ntibble(t, It) |> ggplot(aes(t,It)) + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ,\n                color = \"red\") + # Analytical solution for comparison\n geom_line() # The numerical solution in black"
  },
  {
    "objectID": "W05.html#numerical-integration-more-precise-with-small-dt",
    "href": "W05.html#numerical-integration-more-precise-with-small-dt",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Numerical integration more precise with small \\(dt\\)",
    "text": "Numerical integration more precise with small \\(dt\\)\nWe compute the solution step-by-step using small increments of, e.g. \\(dt = 0.01\\).\n\n\nN <- 10000\nI0 <- 1\ndI <- function(I,N,b) b*I*(N - I)/N\nbeta <- 0.3\ndt <- 0.01 # time increment, supposed to be infinitesimally small\ntmax <- 75\nt <- seq(0,tmax,dt) # this is the vector of timesteps\nIt <- I0 # this will become the vector of the number infected I(t) over time\nfor (i in 2:length(t)) { # We iterate over the vector of time steps and incrementally compute It\n  It[i] = It[i-1] + dt * dI(It[i-1], N, beta) # This is called Euler's method\n}\ntibble(t, It) |> ggplot(aes(t,It)) + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ,\n                color = \"red\") + # Analytical solution for comparison\n geom_line() # The numerical solution in black"
  },
  {
    "objectID": "W05.html#si-model-simulation",
    "href": "W05.html#si-model-simulation",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "SI-model: Simulation",
    "text": "SI-model: Simulation\nAnother more basic solution is direct individual-based simulation.\n\nWe produce a vector of length \\(N\\) with entries representing the state of each individual as \"S\" or \"I\".\nWe model the random infection process in each step of unit time\n\n\n\nN <- 10000\nbeta <- 0.3\nrandomly_infect <- function(N, prob) { runif(N) < prob }\n# Gives a logical vector of length N where TRUE appears with probability 0.3\ninit <- rep(\"S\",N) # All susceptible\ninit[sample.int(N, size=1)] <- \"I\" # Infect one individual\ntmax <- 75\nsim_run <- list(init)\nfor (i in 2:tmax) {\n contacts <- sample(sim_run[[i-1]], size = N)\n sim_run[[i]] <- if_else(contacts == \"I\" & randomly_infect(N, beta), \n                         true = \"I\", \n                         false = sim_run[[i-1]])\n}\nsim_output <- tibble(t = 0:(tmax-1), \n       # Compute a vector with length tmax with the count of \"I\" in sim_run list\n       infected = map_dbl(sim_run, function(x) sum(x == \"I\"))) \nsim_output |> ggplot(aes(t,infected)) + geom_line()"
  },
  {
    "objectID": "W05.html#new-programming-concepts-used-here",
    "href": "W05.html#new-programming-concepts-used-here",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "New programming concepts used here",
    "text": "New programming concepts used here\nFrom base R:\nrunif random numbers\nsample.int and sample\nfor loops\nif and else ifelse\nFrom purrr:\nmap apply function to lists and collect output"
  },
  {
    "objectID": "W05.html#three-ways-to-explore-mechanistic-models",
    "href": "W05.html#three-ways-to-explore-mechanistic-models",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Three ways to explore mechanistic models",
    "text": "Three ways to explore mechanistic models\n\nIndividual-based simulation\n\nWe model every individual explicitly\nSimulation involve random numbers! So simulation runs can be different!\n\nNumerical integration of differential equation\n\nNeeds a more abstract concept of compartments\n\nAnalytical solutions of differential equation\n\noften not possible or not in nice form\n\n\nSuch mechanistic models are typical for natural sciences, but they also make sense for many processes in societies and ecomonies."
  },
  {
    "objectID": "W05.html#differentiation-with-data",
    "href": "W05.html#differentiation-with-data",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Differentiation with data",
    "text": "Differentiation with data\nIn empirical data we can compute the increase in a vector with the function diff:\n\nx <- c(1,2,4,5,5,3,0)\ndiff(x)\n\n[1]  1  2  1  0 -2 -3\n\n\nMore convenient in a data frame is to use x - lag(x) because the vector has the same length.\n\nx - lag(x)\n\n[1] NA  1  2  1  0 -2 -3"
  },
  {
    "objectID": "W05.html#the-diff-of-our-simulation-output",
    "href": "W05.html#the-diff-of-our-simulation-output",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "The diff of our simulation output",
    "text": "The diff of our simulation output\n\ng2 <- sim_output |> \n mutate(derivative_infected = infected - lag(infected)) |> \n ggplot(aes(x = t)) + geom_line(aes(y = derivative_infected))\ng1 <- sim_output |> ggplot(aes(x = t)) + geom_line(aes(y = infected))\ng2\n\n\n\ng1"
  },
  {
    "objectID": "W05.html#nd-derivative-change-of-change",
    "href": "W05.html#nd-derivative-change-of-change",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "2nd derivative: Change of change",
    "text": "2nd derivative: Change of change\n\ng3 <- sim_output |> \n mutate(derivative_infected = infected - lag(infected),\n        derivative2_infected = derivative_infected - lag(derivative_infected)) |> \n ggplot(aes(x = t)) + geom_line(aes(y = derivative2_infected))\ng3\n\n\nIn empirical data: Derivatives of higher order tend to show fluctuation"
  },
  {
    "objectID": "W05.html#interpretation-in-si-model",
    "href": "W05.html#interpretation-in-si-model",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Interpretation in SI-model",
    "text": "Interpretation in SI-model\n\n\n\\(I(t)\\) total number of infected\n\\(I'(t)\\) number of new cases per day (time step)\n\\(I''(t)\\) how the number of new cases has changes compared to yesterday\n\nCan be a good early indicator for the end of a wave."
  },
  {
    "objectID": "W05.html#integration",
    "href": "W05.html#integration",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Integration",
    "text": "Integration\nThe integral of the daily new cases from the beginning to day \\(s\\) is \\(\\int_{-\\infty}^s f(t)dt\\) and represents the total cases at day \\(s\\).\n\nThe integral of a function \\(f\\) up to time \\(s\\) is also called the anti-derivative \\(F(s) = \\int_{-\\infty}^s f(t)dt\\).\nCompute the anti-derivative of data vector with cumsum.\n\n\nx <- c(1,2,4,5,5,3,0)\ncumsum(x)\n\n[1]  1  3  7 12 17 20 20\n\n\n\nEmpirically derivatives tend to become noisy, while integrals tend to become smooth."
  },
  {
    "objectID": "W05.html#fundamental-theorem-of-calculus",
    "href": "W05.html#fundamental-theorem-of-calculus",
    "title": "W#05: Summary Statistics, Exploratory Data Analysis, Principal Component Analysis",
    "section": "Fundamental theorem of calculus",
    "text": "Fundamental theorem of calculus\nThe integral of the derivative is the function itself.\nThis is not a proof but shows the idea:\n\nf <- c(1,2,4,5,5,3,0)\nantiderivative <- cumsum(f)\ndiff(c(0, antiderivative)) # We have to put 0 before to regain the full vector\n\n[1] 1 2 4 5 5 3 0\n\nderivative <- diff(f)\ncumsum(c(1,derivative)) # We have to put in the first value (here 1) manually because it was lost during the diff\n\n[1] 1 2 4 5 5 3 0\n\n\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Developing Schedule\n\n\n\n\n\nThe schedule is tentative it will be continuously updated also with readings/viewings, homework and learnings.\nImportant Links:\nGitHub organization of the course: https://github.com/CU-F23-MDSSB-01-Concepts-Tools\nOrganization repository with non-public information and FAQ in Discussions: https://github.com/CU-F23-MDSSB-01-Concepts-Tools/Organization\n\n\n\n\nTabular Schedule\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopic\nTools Course\n\n\n\n\n1\n09-04, 09-07\nWhat is Data Science? Course Organization, Our software toolkit  Slides Week 1, Achievements Week 1\nR\n\n\n2\n09-11, 09-14\nData Visualization, Data Formats  Slides Week 2 Intro, Slides Week 2 Main Part, Repo with Example Visualization of our data science profiles Achievements Week 2\nR\n\n\n3\n09-18, 09-21\nData Import, Data Wrangling, Relational Data  Slides Week 3\npython\n\n\n4\n09-25, 09-28\nMath refresh, Function Programming, Descriptive Statistics  Slides Week 4\npython\n\n\n5\n10-02, 10-05\nSummary Statistics, Exploratory Data Analysis, Principle Component Analysis, Calculus  Slides Week 5\nR\n\n\n6\n10-09, 10-12\nModeling, Linear Model, Classification Problems, Logistic Regression, Prediction\npython\n\n\n7\n10-16, 10-19\nProbability, Random Variables, Performance Metrics\nR\n\n\n8\n10-23, 10-26\nStatistics , Hypothesis Testing, Simulating Data\npython\n\n\n9\n10-30, 11-02\nModeling Mindsets, Cluster Analysis\nR\n\n\n10\n11-06, 11-09\nDomain Knowledge, Communication\npython\n\n\n11\n11-13, 11-16\nTo be decided\nR\n\n\n12\n11-20, 11-23\nTo be decided\npython\n\n\n13\n11-27, 11-30\nTo be decided\nR / python\n\n\n14\n12-04, 12-07\nCourse Review, Exam Preparation\nPresentations\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nAll topics in the future are subject to change and adaptation to student needs. Topics in italics are tentative.\n\n\n\n\n\n1 Achievements Week 1\nYou have\n\nread the Syllabus and understood the course organization\na running R with RStudio installation an you computer\ndone the git-GitHub dance and know what the tools are in principle\nrendered quarto documents and understood its idea\nmade your first steps with R\nfinished Homework 01 (double check your if your html-file looks nice and that all files including all necessary figure are also in the repository on GitHub)\n\nAdditional material:\n\nQuarto: Watch https://www.youtube.com/watch?v=_f3latmOhew from Mine Çetinkaya-Rundel (co-developer of quarto, R for Data Science, datasciencebox).\nR: Read the following sections in R for Data Science\n\nCh 1:Introduction\nWhole Game (the outline of the part)\n\n\n\n\n2 Achievements Week 2\nYou have\nTo be completed\nAdditional material:\n\nggplot: Read the following sections in R for Data Science\n\nCh 2: Data Visualization\nIf you want to go deeper already now, read the chapter Ch 10: Layers\n\n\nTo be completed\n\n\n3 Achievements Week 3\nYou have To be completed\nAdditional material:\nRead the following sections in R for Data Science\n\nCh 6: Tidy Data\nCh 8: Data Import\nCh 20: Joins\n\n\n\n4 Achievements Week 4\nYou have To be completed\nAdditional material:\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146.\nRead the following sections in R for Data Science\n\nCh 26: Functions\nCh 27: Iteration\nCh 11: Exploratory Data Analysis"
  },
  {
    "objectID": "W8.html#same-phenomenon-different-data",
    "href": "W8.html#same-phenomenon-different-data",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Same phenomenon different data?",
    "text": "Same phenomenon different data?\nQuestion: Is the data of OWiD and WHO the same?\n\n\n\nNo.\nWhy?\nWhat are the data sources of WHO and OWiD?\n\n\n\n\n\n\n\n\n\nOWiD documentation refers to have data from CSSE at Johns Hopkins University which document various data sources (e.g., a newspaper from Germany)\nWHO documentation says: “WHO collected the numbers of confirmed COVID-19 cases and deaths through official communications under the International Health Regulations (IHR, 2005), complemented by monitoring the official ministries of health websites and social media accounts.” For Germany this is the Robert-Koch-Institut RKI."
  },
  {
    "objectID": "W8.html#good-reasons-for-different-data",
    "href": "W8.html#good-reasons-for-different-data",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Good reasons for different data?",
    "text": "Good reasons for different data?\n\nDuring the pandemic daily new case numbers were relevant for decisions about safety measures.\nIn reality, data comes with delays.\n\nExample: Recent new cases in Germany (RKI). Notice many new cases several days ago."
  },
  {
    "objectID": "W8.html#conflict-day-to-day-consistency-and-correctness",
    "href": "W8.html#conflict-day-to-day-consistency-and-correctness",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Conflict day-to-day consistency and correctness",
    "text": "Conflict day-to-day consistency and correctness\n\nFixing daily cases is useful to record the numbers on which daily safety decisions are based.\nCorrected cases (which also change data from the past) are better for analysis in retrospect. It reflects the actual pandemic better."
  },
  {
    "objectID": "W8.html#reported-cases-and-real-cases",
    "href": "W8.html#reported-cases-and-real-cases",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Reported cases and real cases?",
    "text": "Reported cases and real cases?\nCase numbers are to inform us about real cases. What type of data analysis question is this? (Descriptive, Exploratory, Inferential, Predictive, Causal, Mechanistic)\n\n\nInferential: “Quantify whether the discovery is likely to hold in a new sample.”\n\nHere: What do reported cases tell us about cases in the whole population?\n\nLimitations\n\nWe cannot test all\nTests are not on a random sample\nMild/asymptomatic cases remain unnoticed even to individuals\n…\n\n\nThe unknown: What is the dark figure?"
  },
  {
    "objectID": "W8.html#excercise-for-german-new-case-counts",
    "href": "W8.html#excercise-for-german-new-case-counts",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Excercise for German new case counts",
    "text": "Excercise for German new case counts\n\n\nCan we infer the real incidence (= new cases per 100,000)?\nWhat can we infer the trend of the real incidence?\n\n\nIncidence: Not really, we would need a either a random sample (then we can infer the fraction of infected), or an idea how to estimate the dark figure.\nTrend: Yes! Under the assumptions that reported cases do reflect a relevant part of the pandemic and the limitation remain mostly constant during the observed trend."
  },
  {
    "objectID": "W8.html#smoothing-time-series",
    "href": "W8.html#smoothing-time-series",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Smoothing time series",
    "text": "Smoothing time series\n\n\n\nx <- c(1, 2, 5, 3, 0)\nx\n\n\n[1] 1 2 5 3 0\n\n\n\n\n\nzoo::rollmean(x, k = 3, na.pad = TRUE) # for centered window\n\n\n[1]       NA 2.666667 3.333333 2.666667       NA\n\n\n\n\n\n(x + lag(x, n = 1) + lag(x, n = 2))/3 # for lagged window\n\n\n[1]       NA       NA 2.666667 3.333333 2.666667\n\n\n\n\nCentered: Leaves smoothed data close to real data.\nLagged: Lags the smoothed data, but can be consistently computed for the newest day\nRemember: Data with weekly seasonality is best smoothed with a weekly window!"
  },
  {
    "objectID": "W8.html#total-death-per-million-and-human-development",
    "href": "W8.html#total-death-per-million-and-human-development",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Total death per million and human development",
    "text": "Total death per million and human development\n\nWhat findings? Explanations?"
  },
  {
    "objectID": "W8.html#level-of-measurement",
    "href": "W8.html#level-of-measurement",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Level of measurement",
    "text": "Level of measurement\n\nNominal: the data can only be categorized\nOrdinal: the data can be categorized and ranked\nInterval: the data can be categorized, ranked, and evenly spaced\nRatio: the data can be categorized, ranked, evenly spaced, and has a natural zero.\n\nWhat is the difference of level of measurement and data type?\n\nMainly perspective:\n\nLevel of measurement is about the variable/the thing which is measured.\nData type is more about the technical way to store data."
  },
  {
    "objectID": "W8.html#scales-in-surveysquestionaires",
    "href": "W8.html#scales-in-surveysquestionaires",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Scales in surveys/questionaires",
    "text": "Scales in surveys/questionaires\nLikert scale: Strongly disagree … [scale steps] … Strongly agree\nRating scales: Extreme statement … [scale steps] … Opposite statement\nWhat level of measurement do these questions have?\n \n\nOrdinal clearly, interval assuming scale steps are equal, ratio assuming 5 as natural zero"
  },
  {
    "objectID": "W8.html#dealing-with-missing-values",
    "href": "W8.html#dealing-with-missing-values",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nCoding in the ESS data:\n\nInterval data coded numerically 0, 1, …, 10\nMissing values with numerical codes 77, 88, 99\n\nWhat is the reason for missing data? This can be important for inferential questions!\n\n\n\n\nFor numerical computations these must be filtered out or coded as NA!\n\ness |> select(euftf) |> \n mutate(euftf_na = euftf  |> na_if(77) |> na_if(88) |> na_if(99)) |> \n summarize(across(.fns = function(x) mean(x, na.rm = TRUE)))\n\n# A tibble: 1 × 2\n  euftf euftf_na\n  <dbl>    <dbl>\n1  13.7     5.20"
  },
  {
    "objectID": "W8.html#emotional-attachment",
    "href": "W8.html#emotional-attachment",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Emotional attachment",
    "text": "Emotional attachment\nQuestion: What is the relation of the emotional attachment of Europeans to their own country and to Europe?\n\n\ness |> \n filter(essround == 9) |> \n count(atchctr, atcherp) |> \n na.omit() |> \n ggplot(aes(atchctr, atcherp, \n            size = n, color = n)) + \n geom_point() +\n geom_smooth(aes(weight = n), \n             method = 'loess', \n             formula = 'y ~ x') +\n scale_color_continuous(type = \"viridis\") +\n scale_size_area(max_size = 10) + ylim(c(0,10)) +\n coord_fixed() + \n guides(size = \"none\") +\n theme_classic(base_size = 24)\n\n\n\n\n\n\n\n\nDifferences between lowess used in seaborn.regplot and loess, the default in ggplot::geom_smooth: https://stats.stackexchange.com/questions/161069/difference-between-loess-and-lowess"
  },
  {
    "objectID": "W8.html#emotional-attachment-eu-integration",
    "href": "W8.html#emotional-attachment-eu-integration",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Emotional attachment EU integration",
    "text": "Emotional attachment EU integration\nQuestion: What is the relation of the emotional attachment to the own country to attachment to Europe compared to the attitude about European integration?\n\n\nEmotional attachment to the own country and Europe is positively related. (“Positive” here means the sign of the correlation. It does not mean “good”!)\nNo compensation like “Emotion must be split between both.”\nRelation of country attachment to EU integration is weak but non-linear."
  },
  {
    "objectID": "W8.html#weighting-after-count",
    "href": "W8.html#weighting-after-count",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Weighting after count",
    "text": "Weighting after count\nHow many rows does the the data frame ess |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() have?\n\n11 times 11 = 121 (when each combination has a non-zero number of cases)\nHow many observations (not NA) do we have in the data set?\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> summarize(n = sum(n))\n\n# A tibble: 1 × 1\n      n\n  <int>\n1 45552\n\n\nWeighting points correctly is important!"
  },
  {
    "objectID": "W8.html#three-different-smooth-plots",
    "href": "W8.html#three-different-smooth-plots",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Three different smooth plots",
    "text": "Three different smooth plots\n\nA: Counts unweightedB: Counts weightedC: Individual cases\n\n\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> \n ggplot(aes(atchctr, euftf)) + geom_smooth() \n\n\n\n\nMakes no sense because, just 121 points in a square.\n\n\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> \n ggplot(aes(atchctr, euftf, weight = n)) + geom_smooth() \n\n\n\n\nMakes sense weighted by the counts.\n\n\n\ness |> filter(essround == 9) |> \n ggplot(aes(atchctr, euftf)) + geom_smooth()\n\n\n\n\nSimilar to B. Lower uncertainty! Different y-axis limits!\nNote, geom_smooth uses stats::loess for less than 1,000 cases (as before), otherwise (as here) mgcv::gam() because it is more efficient computationally. We omit details here."
  },
  {
    "objectID": "W8.html#significance-of-nonlinear-relation",
    "href": "W8.html#significance-of-nonlinear-relation",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Significance of nonlinear relation?",
    "text": "Significance of nonlinear relation?\n\n\nTaking into account the real number of cases the uncertainty range indicates that the non-linear relationship is fairly certain, although small in magnitude.\nNote, we have not looked at uncertainty measures in detail yet.\nMain message here: Low uncertainty and large effect size is not the same!\n\nIn statistics the first is called significant.\nIn common language the second is often called significant.\n\nThis dual use of significant is a source of confusion in science communication!"
  },
  {
    "objectID": "W8.html#linear-models",
    "href": "W8.html#linear-models",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear models",
    "text": "Linear models\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n fit(atcherp ~ atchctr, data = ess) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    2.62    0.0411       63.8       0\n2 atchctr        0.414   0.00504      82.1       0\n\n\nInterpretation?\n\nWhen atchctr = 0 the average atcherp is 2.62. For an increase of country attachment by one there is an average increase of 0.414 in European attachment.\n\n\nFor EU integration attitude.\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(euftf ~ atchctr, data = ess) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)   5.00     0.0481     104.   0           \n2 atchctr       0.0333   0.00589      5.65 0.0000000157"
  },
  {
    "objectID": "W8.html#r-squared-of-a-fitted-model",
    "href": "W8.html#r-squared-of-a-fitted-model",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "R-squared of a fitted model",
    "text": "R-squared of a fitted model\n\\(R^2\\) is the percentage of variability in the response explained by the regression model.\nR-squared is also called coefficient of determination.\nDefinition:\n\\(R^2 = 1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}\\)\nwhere \\(SS_\\text{res} = \\sum_i(y_i - f_i)^2 = \\sum_i e_i^2\\) is the sum of the squared residuals, and\n\\(SS_\\text{tot} = \\sum_i(y_i - \\bar y)^2\\) the total sum of squares which is proportional to the variance of \\(y\\). (\\(\\bar y\\) is the mean of \\(y\\).)\n\\(R^2\\) is the square of the correlation coefficient, hence the name. (No math on this today.)"
  },
  {
    "objectID": "W8.html#linear-models-r-squared",
    "href": "W8.html#linear-models-r-squared",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear models R-squared",
    "text": "Linear models R-squared\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n fit(atcherp ~ atchctr, data = ess) |>\n glance()  # glance shows summary statistics of model fit\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df   logLik     AIC     BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>    <dbl>   <dbl>   <dbl>\n1     0.122         0.122  2.45     6747.       0     1 -112254. 224514. 224540.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nInterpretation R-square?\n\n12.2% of the variance of European emotional attachment can be explained by a linear relation with country emotional attachment.\n\n\nFor EU integration attitude.\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(euftf ~ atchctr, data = ess) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df  logLik    AIC    BIC\n      <dbl>         <dbl> <dbl>     <dbl>      <dbl> <dbl>   <dbl>  <dbl>  <dbl>\n1  0.000702      0.000680  2.75      32.0    1.57e-8     1 -1.11e5 2.21e5 2.21e5\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"
  },
  {
    "objectID": "W8.html#linear-model-with-more-predictors",
    "href": "W8.html#linear-model-with-more-predictors",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear model with more predictors",
    "text": "Linear model with more predictors\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(euftf ~ atchctr + atcherp, data = ess) |> tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    4.05    0.0478       84.8 0       \n2 atchctr       -0.114   0.00599     -19.1 1.12e-80\n3 atcherp        0.354   0.00509      69.7 0       \n\n\n\nNote, that atchctr now has a negative coefficient!\nThe tiny bit of positive relation explained by atchctr in a one predictor model can better be explained by atcherp (which we know is correlated with atchctr)."
  },
  {
    "objectID": "W8.html#corona-deaths-vs.-human-development",
    "href": "W8.html#corona-deaths-vs.-human-development",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Corona deaths vs. Human development",
    "text": "Corona deaths vs. Human development\n\nNote: Not weighted by population!"
  },
  {
    "objectID": "W8.html#linear-model-total-deaths-vs.-hdi",
    "href": "W8.html#linear-model-total-deaths-vs.-hdi",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear model: Total deaths vs. HDI",
    "text": "Linear model: Total deaths vs. HDI\n\nowid_aug22 <- owid |> \n filter(date == \"2022-08-31\", !is.na(human_development_index), \n        !is.na(total_deaths_per_million), !is.na(continent))\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index, data = owid_aug22) |> tidy()\n\n# A tibble: 2 × 5\n  term                    estimate std.error statistic  p.value\n  <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)               -2350.      385.     -6.11 5.84e- 9\n2 human_development_index    4917.      522.      9.43 1.71e-17"
  },
  {
    "objectID": "W8.html#adding-a-main-effect-of-continents",
    "href": "W8.html#adding-a-main-effect-of-continents",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Adding a main effect of continents",
    "text": "Adding a main effect of continents\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index + continent, \n        data = owid_aug22) |> tidy()\n\n# A tibble: 7 × 5\n  term                    estimate std.error statistic       p.value\n  <chr>                      <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)               -754.       392.    -1.92  0.0560       \n2 human_development_index   1901.       666.     2.86  0.00480      \n3 continentAsia               58.5      213.     0.274 0.784        \n4 continentEurope           1685.       279.     6.04  0.00000000867\n5 continentNorth America     742.       254.     2.92  0.00397      \n6 continentOceania          -312.       298.    -1.05  0.297        \n7 continentSouth America    1910.       311.     6.15  0.00000000489\n\n\nA main effect by categorical dummy variables allows for different intercepts per continent."
  },
  {
    "objectID": "W8.html#adding-as-interaction",
    "href": "W8.html#adding-as-interaction",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Adding as interaction",
    "text": "Adding as interaction\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index * continent, \n        data = owid_aug22) |> tidy()\n\n# A tibble: 12 × 5\n   term                                    estimate std.error statistic  p.value\n   <chr>                                      <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)                               -1555.      581.    -2.67  8.19e- 3\n 2 human_development_index                    3329.     1019.     3.27  1.31e- 3\n 3 continentAsia                               563.      940.     0.598 5.50e- 1\n 4 continentEurope                           14759.     1956.     7.55  2.32e-12\n 5 continentNorth America                    -1346.     1512.    -0.890 3.75e- 1\n 6 continentOceania                           1125.     1424.     0.790 4.31e- 1\n 7 continentSouth America                    -4243.     3436.    -1.23  2.19e- 1\n 8 human_development_index:continentAsia     -1027.     1419.    -0.724 4.70e- 1\n 9 human_development_index:continentEurope  -15377.     2351.    -6.54  6.40e-10\n10 human_development_index:continentNorth…    2394.     2099.     1.14  2.56e- 1\n11 human_development_index:continentOcean…   -2318.     2063.    -1.12  2.63e- 1\n12 human_development_index:continentSouth…    7684.     4543.     1.69  9.25e- 2\n\n\n\nNote the * for interaction effect!\nAlso main effects for both variables are in as coefficients.\nAfrica has been chosen as reference category (because it is first in the alphabet).\nAn interaction effect allows for different slopes for each continent!"
  },
  {
    "objectID": "W8.html#regression-lines-by-continent",
    "href": "W8.html#regression-lines-by-continent",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Regression lines by continent",
    "text": "Regression lines by continent\n\nThe relation between deaths and human development is reverse in Europe."
  },
  {
    "objectID": "W8.html#simpsons-paradox",
    "href": "W8.html#simpsons-paradox",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nSlopes for all groups can be in the opposite direction of the main effect’s slope!\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "hw-02.html",
    "href": "hw-02.html",
    "title": "Homework 02",
    "section": "",
    "text": "The goal of this assignment is to introduce you to Data Visualization and Data Wrangling in R and python.\nTo that end, we provide the repository hw2-USERNAME two starter quarto documents\n\none for R\none for python\n\nFirst, solve the exercises in RStudio with R.\nOnce you had the introduction to python you do the same in python.\nThe learning goal is to see the differences but, more important, the similarity of how the concepts are implemented.\nFind the instructions in the README in your repository!"
  },
  {
    "objectID": "W12.html#probability-topics-for-data-science",
    "href": "W12.html#probability-topics-for-data-science",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability Topics for Data Science",
    "text": "Probability Topics for Data Science\nToday concepts and topics\n\nThe concept of probability form the mathematical perspective.\nWhat are probabilistic events, probability functions and random variables.\nHow do random variables relate to data?\n(Binomial distribution)"
  },
  {
    "objectID": "W12.html#sample-space-atomic-events-events",
    "href": "W12.html#sample-space-atomic-events-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Sample space, atomic events, events",
    "text": "Sample space, atomic events, events\nIn the following, we say \\(S\\) is the sample space which is a set of atomic events.\nExample for sample spaces:\n\nFor a coin toss the atomic events are \\(H\\) (for HEADS) and \\(T\\) for TAILS, and the sample space is \\(S = \\{H,T\\}\\).\n\nFor the selection of one person of a group of \\(N\\) individuals labeled \\(1,\\dots,N\\), the sample space is \\(S = \\{1,\\dots,N\\}\\).\nFor two successive coin tosses the atomic events are \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\). The sample space is \\(\\{HH,HT, TH, TT\\}\\). Important: The atomic events for two coin tosses tare not \\(H\\) and \\(T\\).\n\nAn event \\(A\\) is a subset of the sample space \\(A \\subset S\\).\nImportant: Note the difference of atomic events and events."
  },
  {
    "objectID": "W12.html#example-events-for-one-coin-toss",
    "href": "W12.html#example-events-for-one-coin-toss",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example events for one coin toss",
    "text": "Example events for one coin toss\n\nThe set with one atomic event is a subset \\(\\{H\\} \\subset \\{H,T\\}\\).\nAlso the sample space \\(S = \\{H,T\\} \\subset \\{H,T\\}\\) is an event. It is called the sure event.\nAlso the empty set \\(\\{\\} = \\emptyset \\subset \\{H,T\\}\\) is an event. It is called the impossible event.\nIn interpretation, the event \\(\\{H,T\\}\\) means: The coin comes up HEAD or TAIL.\nThe empty set is interpreted as the event that it comes up neither HEADS nor TAILS."
  },
  {
    "objectID": "W12.html#more-example-events",
    "href": "W12.html#more-example-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "More example events",
    "text": "More example events\n\n2 coin tosses: The event \\(\\{HH, TH\\}\\) means “The first toss comes up HEAD or TAIL and the second is HEADS.”\n\nThe event \\(\\{HT, TH, HH\\}\\) means that “We have HEAD once or twice and it does not matter what coins.”\nThe event \\(\\{TT, HH\\}\\) means “Both coins show the same side.”\n\nQuiz questions for three coin tosses:\n\nWhat is the event “The coins show one HEAD”? \\(\\{HTT, THT, TTH\\}\\)\nWhat is the event “The first and the third coin are not HEAD? \\(\\{THT, TTT\\}\\)\nHow many atomic events exist? \\(2^3=8\\)\n\n\nFor selecting one random person:\nThe event \\(\\{2,5,6\\}\\) means that the selected person is either 2, 5, or 6. (Not all three people which is a different random variable!)"
  },
  {
    "objectID": "W12.html#the-set-of-all-events",
    "href": "W12.html#the-set-of-all-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The set of all events",
    "text": "The set of all events\nThe collection of all events is called a sigma-algebra. (This is a mathematical term which linguistic meaning we do not analyze deeper here.)\nDefinition: A sigma-algebra \\(\\mathcal{F}(S)\\) is a collection of subsets of a sample space \\(S\\) when it has the following properties\n\nThe empty set (the impossible event) is part of it \\(\\emptyset \\in \\mathcal{F}(S)\\)\nWhen \\(A \\in \\mathcal{F}(S)\\) then its complement \\(A^c \\in \\mathcal{F}(S)\\). That means: For any event \\(A\\) also its opposite \\(A^c = S \\setminus A\\) (read \\(S\\) minus the elements of \\(A\\)) is an event.\n\\(\\mathcal{F}(S)\\) is closed under the countable set union of its members. That means if \\(A_1,A_2,A_3, \\dots \\in \\mathcal{F}(S)\\) the \\(\\bigcup_{i}^\\infty A_i = A_1 \\cup A_2 \\cup A_3 \\cup \\dots \\in \\mathcal{F}(S)\\).\n\nThe mathematical technicality is not central here. Important is: The sigma-algebra is the set of all possible events and this is usually larger / more complex then one may naively think."
  },
  {
    "objectID": "W12.html#the-power-set",
    "href": "W12.html#the-power-set",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The power set",
    "text": "The power set\n\n\n\nA sigma-algebra \\(\\mathcal{F}(S)\\) is a subset of the set of all subsets (also called power set) of the sample space sometimes denoted \\(\\mathcal{P}(S)\\) or \\(2^S\\).\nThe notation \\(2^S\\) matches the fact that the power set of a set with \\(n\\) elements has \\(2^n\\) elements.\n\n\n\nExample powerset of a three element set."
  },
  {
    "objectID": "W12.html#example-for-the-set-of-all-events",
    "href": "W12.html#example-for-the-set-of-all-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example for the set of all events",
    "text": "Example for the set of all events\n\nFor 3 coin tosses: How many events exist? \\(2^3=8\\) atomic events \\(\\to\\) \\(2^8=256\\) event\nHow is it for four coin tosses? \\(2^{(2^4)} = 65536\\)\nWe select two out of five people at random (without replacement). How many atomic events? How many events?\n\n\nAtomic events: 12, 13, 14, 15, 23, 24, 25, 34, 35, 45 (here the order does not matter)\nThe number can be computed by “n choose k” \\({n \\choose k} =\\frac{n!}{(n-k)!k!}\\). Here: \\({5\\choose 2}\\).\n\nchoose(5,2)\n\n[1] 10\n\n\nThus there are \\(2^{10} = 1024\\) events.\nExample event: “Person 1 is among the selected.” = \\(\\{12, 13, 14, 15\\}\\)\nThese are typical problems of combinatorics, the theory of counting, which is basic for many probability models. We do not go deeper into it here."
  },
  {
    "objectID": "W12.html#probability-function",
    "href": "W12.html#probability-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability function",
    "text": "Probability function\nDefinition: For a collection of events (in a sigma-algebra \\(\\mathcal{F}(S)\\)) a function \\(\\text{Pr}: \\mathcal{F}(S) \\to \\mathbb{R}\\) is a probability function when\n\nThe probability of any event is between 0 and 1: \\(0\\leq \\text{Pr}(A) \\leq 1\\). (So, actually a probability function is a function \\(\\text{Pr}: \\mathcal{F}(S) \\to [0,1]\\).)\nThe probability of the event coinciding with the whole sample space (the sure event) is 1: \\(\\text{Pr}(S) = 1\\).\nFor events \\(A_1, A_2, \\dots, A_n \\in \\mathcal{F}(S)\\) which are pairwise disjoint we can sum up their probabilities:\n\\[\\text{Pr}(A_1 \\cup A_2\\cup\\dots\\cup A_n) = \\text{Pr}(A_1) + \\text{Pr}(A_2) + \\dots + \\text{Pr}(A_n) \\]\n\nThis captures the essence of how we think about probabilities mathematically. Most important: We can only easily add probabilities when they do not share atomic events."
  },
  {
    "objectID": "W12.html#some-basic-probability-rules",
    "href": "W12.html#some-basic-probability-rules",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Some basic probability rules",
    "text": "Some basic probability rules\n\nWe can compute the probabilities of all events by summing the probabilities of the atomic events in it. So, the probabilities of the atomic events are building blocks for the whole probability function.\n\\(\\text{Pr}(\\emptyset) = 0\\)\nFor any events \\(A,B \\subset S\\) it holds\n\n\\(\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cap B)\\)\n\\(\\text{Pr}(A \\cap B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cup B)\\)\n\\(\\text{Pr}(A^c) = 1 - \\text{Pr}(A)\\)\n\n\nRecap from the motivation of logistic regression: When the probability of an event is \\(A\\) is \\(\\text{Pr}(A)=p\\), then its odds (in favor of the event) are \\(\\frac{p}{1-p}\\). The logistic regression model “raw” predictions are log-odds \\(\\log\\frac{p}{1-p}\\)."
  },
  {
    "objectID": "W12.html#random-variable",
    "href": "W12.html#random-variable",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Random variable",
    "text": "Random variable\n\nA random variable is a numerical function where values come with probabilities.\nIn some statistical model, we consider variables in a data frame as random variables, for example the response variable in a generalized linear model.\n\nFormally, a random variable is\n\na function \\(X: S \\to \\mathbb{R}\\)\nwhich assigns a value to each atomic event in the sample space.\n\nTogether with a probability function \\(\\text{Pr}: \\mathcal{F}(S)\\to [0,1]\\) probabilities can be assigned to values of the random variable (see the probability mass function in two slides)."
  },
  {
    "objectID": "W12.html#examples-of-random-variables",
    "href": "W12.html#examples-of-random-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Examples of random variables",
    "text": "Examples of random variables\n\n\nFor two coin tosses a random variable can be the number of HEADS. In this case, each atomic event is mapped to a number: Either 0, 1, or 2.\nFor 62 randomly selected organ donations a random variable can be the number of complications. Each atomic event is mapped to an integer from 0 to 62. (Note, an atomic event are 62 randomly selected organ donations. So, the set of events is \\(2^{62} \\approx 4.61\\cdot 10^{18}\\).)\nIn the palmer penguins dataset we can consider a variable, e.g. flipper length, to be a random variable. The atomic event would be the random selection of a penguin and the random variable is its flipper length. So we map each penguin to its flipper length.\n\n\n\nA random variable is a way to look at a numerical aspect of a sample space. It often simplfies because many atomic events may be mapped to the same number."
  },
  {
    "objectID": "W12.html#probability-mass-function-pmf",
    "href": "W12.html#probability-mass-function-pmf",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability mass function (pmf)",
    "text": "Probability mass function (pmf)\nFor\n\na random variable \\(X\\) and\na probability function \\(\\text{Pr}\\)\n\nthe probability mass function \\(f_X: \\mathbb{R} \\to [0,1]\\) is defined as\n\\[f_X(x) = \\text{Pr}(X=x),\\]\nwhere \\(\\text{Pr}(X=x)\\) is an abbreviation for \\(\\text{Pr}(\\{a\\in S\\text{ for which } X(a) = x\\})\\)."
  },
  {
    "objectID": "W12.html#example-pmf-for-2-coin-tosses",
    "href": "W12.html#example-pmf-for-2-coin-tosses",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example pmf for 2 coin tosses",
    "text": "Example pmf for 2 coin tosses\nTwo coin tosses \\(S = \\{HH, HT, TH, TT\\}\\)\n\nWe define \\(X\\) to be the number of heads:\n\\(X(HH) = 2\\), \\(X(TH) = 1\\), \\(X(HT) = 1\\), and \\(X(TT) = 0\\).\n\nWe assume the probability function \\(\\text{Pr}\\) assigns for each atomic event a probability of 0.25.\nThen the probability mass function is \\[\\begin{align} f_X(0) = & \\text{Pr}(X=0) = \\text{Pr}(\\{TT\\}) & = 0.25 \\\\\nf_X(1) = & \\text{Pr}(X=1) = \\text{Pr}(\\{HT,TH\\}) & = 0.25 + 0.25 = 0.5 \\\\\nf_X(2) = &\\text{Pr}(X=2) = \\text{Pr}(\\{HH\\}) & = 0.25\\end{align}\\]\nNote that \\(\\text{Pr}(\\{HT,TH\\}) = \\text{Pr}(\\{HT\\}) + \\text{Pr}(\\{HT\\})\\) by adding the probabilities of the atomic events.\nFor all \\(x\\) which are not 0, 1, or 2 it is obviously \\(f_X(x) = 0\\)."
  },
  {
    "objectID": "W12.html#example-roll-two-dice",
    "href": "W12.html#example-roll-two-dice",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example: Roll two dice 🎲 🎲",
    "text": "Example: Roll two dice 🎲 🎲\nRandom variable: The sum of both dice.\n\nEvents: All 36 combinations of rolls 1+1, 1+2, 1+3, 1+4, 1+5, 1+6, 2+1, 2+2, 2+3, 2+4, 2+5, 2+6, 3+1, 3+2, 3+3, 3+4, 3+5, 3+6, 4+1, 4+2, 4+3, 4+4, 4+5, 4+6, 5+1, 5+2, 5+3, 5+4, 5+5, 5+6, 6+1, 6+2, 6+3, 6+4, 6+5, 6+6\n\n\nPossible values of the random variable:  2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 (These are numbers.)\n\n\nProbability mass function: (Assuming each number has probability of \\(\\frac{1}{6}\\) for each die.)\n\n\n\\(\\text{Pr}(2) = \\text{Pr}(12) = \\frac{1}{36}\\)\n\\(\\text{Pr}(3) = \\text{Pr}(11) = \\frac{2}{36}\\)\n\\(\\text{Pr}(4) = \\text{Pr}(10) = \\frac{3}{36}\\)\n\\(\\text{Pr}(5) = \\text{Pr}(9) = \\frac{4}{36}\\)\n\\(\\text{Pr}(6) = \\text{Pr}(8) = \\frac{5}{36}\\)\n\\(\\text{Pr}(7) = \\frac{6}{36}\\)\n\n\ntibble(Value = 0:15, pmf=c(0,c(0:6,5:0)/36,0,0)) |> \n ggplot(aes(Value,pmf)) + geom_line() + geom_point() +\n theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W12.html#binomial-distribution-1",
    "href": "W12.html#binomial-distribution-1",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nThe number of HEADS in several coin tosses and the number of complications in randomly selected organ donations are examples of random variable which have a binomial distribution.\n\nDefinition: The binomial distribution with parameters \\(n\\) and \\(p\\) is the number of successes in a sequence of \\(n\\) independent Bernoulli trials which each delivers a success with probability \\(p\\) and a failure with probability \\((1-p)\\).\n\nThe default model for the number of successes drawn from a sample of size \\(n\\) drawn from a population of size \\(N\\) with replacement.\nWhen \\(N\\) is much larger than \\(n\\) it is also a good approximation for drawing without replacement."
  },
  {
    "objectID": "W12.html#binomial-probability-mass-function",
    "href": "W12.html#binomial-probability-mass-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Binomial probability mass function",
    "text": "Binomial probability mass function\n\\[f(k,n,p) = \\Pr(k;n,p) = \\Pr(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\\]\nwhere \\(k\\) is the number of successes, \\(n\\) is the number of Bernoulli trials, and \\(p\\) the success probability.\nProbability to have exactly 3 complications in 62 randomly selected organ donations with complication probability \\(p=0.1\\) is\n\n# x represents k, and size represents n\ndbinom(x = 3, size = 62, prob = 0.1)\n\n[1] 0.07551437\n\n\nThe probability to have 3 complications or less can be computed as\n\ndbinom(3, 62, 0.1) + dbinom(2, 62, 0.1) + dbinom(1, 62, 0.1) + dbinom(0, 62, 0.1)\n\n[1] 0.1209787\n\n\n\nThis was the p-value we computed with simulation for the hypothesis testing example."
  },
  {
    "objectID": "W12.html#distribution-functions-are-vectorized",
    "href": "W12.html#distribution-functions-are-vectorized",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Distribution functions are vectorized!",
    "text": "Distribution functions are vectorized!\nCompute the p-value:\n\ndbinom(0:3, 62, 0.1) |> sum()\n\n[1] 0.1209787\n\n\nPlotting the probability mass function\n\n\ntibble(x = 0:62) |> \n mutate(pr = dbinom(x, size = 62, prob = 0.1)) |> \n ggplot(aes(x, pr)) + \n geom_col() + \n theme_minimal(base_size = 24)"
  },
  {
    "objectID": "W12.html#other-plots-of-binomial-mass-function",
    "href": "W12.html#other-plots-of-binomial-mass-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Other plots of binomial mass function",
    "text": "Other plots of binomial mass function\nChanging the sample size:\n\n\ntibble(samplesize = 0:62) |> \n mutate(pr = dbinom(3, size = samplesize, prob = 0.1)) |> \n ggplot(aes(samplesize, pr)) + \n geom_col() + \n theme_minimal(base_size = 24)\n\n\n\n\n\n\nThe probability of 3 successes is most likely for sample sizes around 30. Sensible?\n\nChanging the success probability:\n\n\ntibble(probs = seq(0,0.3,0.01)) |> \n mutate(pr = dbinom(3, size = 62, prob = probs)) |> \n ggplot(aes(probs, pr)) + \n geom_col() + \n scale_x_continuous(breaks = seq(0,0.3,0.05)) + \n theme_minimal(base_size = 24)\n\n\n\n\n\n\nThe probability of 3 successes is most likely for success probabilities around 0.05."
  },
  {
    "objectID": "W12.html#expected-value-discrete-rv",
    "href": "W12.html#expected-value-discrete-rv",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Expected value discrete RV",
    "text": "Expected value discrete RV\nA discrete random variable takes only a finite (or at least discrete) set of values.\nFor \\(X: S \\to \\mathbb{R}\\), when \\(S\\) is finite, there is naturally only a set of values \\(x_1,\\dots,x_k\\in\\mathbb{R}\\) which \\(X\\) can be. We call their probabilities \\(p_1,\\dots,p_k\\) with \\(p_i = \\text{Pr}(X=x_i) = \\text{Pr}(\\{a \\in S \\text{ for which } X(a) = x_i \\})\\).\n(The probability of all other values in \\(\\mathbb{R}\\) is zero.).\nThe expected value of \\(X\\) is \\(E(X) = \\sum_{i=1}^k p_i x_i = p_1x_1 + \\dots + p_kx_k.\\)\nExamples: \\(X\\) is a die roll 🎲. \\(E(X) = 1\\cdot\\frac{1}{6} + 2\\cdot\\frac{1}{6} + 3\\cdot\\frac{1}{6} + 4\\cdot\\frac{1}{6} + 5\\cdot\\frac{1}{6} + 6\\cdot\\frac{1}{6} = \\frac{21}{6} = 3.5\\)\n\\(X\\) sum of two die rolls 🎲🎲.\n\\(E(X) = 2\\cdot\\frac{1}{36} + 3\\cdot\\frac{2}{36} + 4\\cdot\\frac{3}{36} + 5\\cdot\\frac{4}{36} + 6\\cdot\\frac{5}{36} + 7\\cdot\\frac{6}{36} + 8\\cdot\\frac{5}{36} +\\)\n\\(+ 9\\cdot\\frac{4}{36} + 10\\cdot\\frac{3}{36} + 11\\cdot\\frac{2}{36} + 12\\cdot\\frac{1}{36} = 7\\)"
  },
  {
    "objectID": "W12.html#expected-value-binomial-distribution",
    "href": "W12.html#expected-value-binomial-distribution",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Expected value binomial distribution",
    "text": "Expected value binomial distribution\nFor \\(X \\sim \\text{Binom}(n,p)\\) (read “\\(X\\) has a binomial distribution with samplesize \\(n\\) and success probability \\(p\\)”)\nThe expected value of \\(X\\) is by definition\n\\[E(X) = \\underbrace{\\sum_{k = 0}^n k}_{\\text{sum over successes}} \\cdot \\underbrace{\\binom{n}{k}p^k(1-p)^{n-k}}_{\\text{probability of successes}}\\]\nComputation shows that \\(E(X) = p\\cdot n\\).\nExample: For \\(n = 62\\) organ donations with complication probability \\(p=0.1\\), the expected number of complications is \\(E(X) = 6.2\\)."
  },
  {
    "objectID": "W12.html#general-systematic-of-functions-for-distributions-in-r",
    "href": "W12.html#general-systematic-of-functions-for-distributions-in-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "General systematic of functions for distributions in R",
    "text": "General systematic of functions for distributions in R\nIn R we usually have 4 function for each distribution: The d, p, q, and r version. For the binomial distribution:\n\ndbinom the density function (more on the name later)\npbinom distribution function\nqbinom the quantile function, and\nrbinom random number generator."
  },
  {
    "objectID": "W12.html#probability-mass-function-d",
    "href": "W12.html#probability-mass-function-d",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability mass function d",
    "text": "Probability mass function d\n\nThe mass function (or density function, more on this later) dbinom\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = dbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nGives the probability for the number \\(x\\): \\(\\text{Pr}(X = x)\\) or \\(f_X(x)\\)."
  },
  {
    "objectID": "W12.html#distribution-function-p",
    "href": "W12.html#distribution-function-p",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Distribution function p",
    "text": "Distribution function p\n\nThe distribution function, or cumulative probability function pbinom\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = pbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nGives the probability that the random variable is less or equal to \\(x\\):\n\\(\\text{Pr}(X \\leq x)\\)."
  },
  {
    "objectID": "W12.html#quantile-function-q",
    "href": "W12.html#quantile-function-q",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Quantile function q",
    "text": "Quantile function q\n\nThe quantile function, qbinom with argument \\(p\\) representing the fraction of lowest values of \\(X\\) among all values for which we want the \\(x\\) value for.\n\n\n\nprobs <- seq(0, 1, by = 0.01)\ntibble(p = probs) |> \n mutate(x = qbinom(p, size = 10, prob = 0.5)) |> \n ggplot(aes(p, x)) + geom_line() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nA point \\((p,x)\\) means: When we want a \\(p\\)-fraction of the probability mass, we need all events with values lower or equal to \\(x\\)."
  },
  {
    "objectID": "W12.html#calculus-relations",
    "href": "W12.html#calculus-relations",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Calculus relations",
    "text": "Calculus relations\n\nQuantile, distribution and mass function all carry the full information about the distribution of a random variable \\(X\\).\nThe mass function is the derivative of the distribution function.\n(The distribution function is the anti-derivative of the mass function.)\n\n\npbinom(0:5, size = 5, prob = 0.5) \n\n[1] 0.03125 0.18750 0.50000 0.81250 0.96875 1.00000\n\n# Next comes its derivative (have to append a 0 before first)\npbinom(0:5, size = 5, prob = 0.5) |> append(0, after = 0) |> diff()\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\ndbinom(0:5, size = 5, prob = 0.5)\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n# Next comes its anti-derivative\ndbinom(0:5, size = 5, prob = 0.5) |> cumsum()\n\n[1] 0.03125 0.18750 0.50000 0.81250 0.96875 1.00000"
  },
  {
    "objectID": "W12.html#more-calculus-relations",
    "href": "W12.html#more-calculus-relations",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "More calculus relations",
    "text": "More calculus relations\n\nThe quantile function is the inverse of the distribution function.\nWe plot the inverse function by interchanging the x and y aesthetic.\n\n\n\nprobs <- seq(0, 1, by = 0.01)\nx <- 0:10\nq <- tibble(p = probs) |> mutate(x = qbinom(p, size = 10, prob = 0.5)) \np <- tibble(x = x) |> mutate(p = pbinom(x, size = 10, prob = 0.5)) \nq_plot <- q |> ggplot(aes(p, x)) + geom_line()\nqinv_plot <- q |> ggplot(aes(x, p)) + geom_line()\np_plot <- p |> ggplot(aes(x, p)) + geom_col()\npinv_plot <- p |> ggplot(aes(p, x)) + geom_col(orientation = \"y\")\nlibrary(patchwork)\n(q_plot | p_plot) / (pinv_plot | qinv_plot)"
  },
  {
    "objectID": "W12.html#random-number-generator-r",
    "href": "W12.html#random-number-generator-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Random number generator r",
    "text": "Random number generator r\n\nRandom binomial numbers are drawn with rbinom\n\n\n# 10 random binomial numbers for 62 trials with success probability 0.1\nrbinom(10, size = 62, prob = 0.1)\n\n [1]  5  5  5 10  6  3  8 10  6  5\n\n\n\nWe can reproduce the null distribution from hypothesis testing with 62 organ donations and 10% complication probability this way.\n\nWe produce 100,000 random consultants\nThen we compute the fraction of which have 3 or less complications\n\n\n\nset.seed(2022)\ns <- rbinom(100000, size = 62, prob = 0.1)\nsum(s<=3)/100000\n\n[1] 0.12038\n\n# Two other samples\nsum(rbinom(100000, size = 62, prob = 0.1)<=3)/100000\n\n[1] 0.11918\n\nsum(rbinom(100000, size = 62, prob = 0.1)<=3)/100000\n\n[1] 0.12034"
  },
  {
    "objectID": "W12.html#empirical-distributions",
    "href": "W12.html#empirical-distributions",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Empirical distributions",
    "text": "Empirical distributions\n\n\n\n\n\\(X\\): select a random person from Europe (in 2018, willing to answer survey) and ask its attitude towards the European union from 0 to 10\n\n\n\n\nWhat is the distribution of the answer?\n\n\neu <- ess |> select(euftf) |> drop_na() |> \n count(euftf) |> mutate(prob = n/sum(n)) \neu\n\n# A tibble: 11 × 3\n   euftf     n   prob\n   <dbl> <int>  <dbl>\n 1     0  3361 0.0736\n 2     1  1787 0.0391\n 3     2  2830 0.0620\n 4     3  3586 0.0786\n 5     4  3739 0.0819\n 6     5 10286 0.225 \n 7     6  4589 0.101 \n 8     7  5165 0.113 \n 9     8  4692 0.103 \n10     9  1786 0.0391\n11    10  3826 0.0838\n\n\n\nMass function and distribution function\n\neu_mass <- eu |> ggplot(aes(euftf, prob)) + geom_col()\neu_distr <- eu |> mutate(cumprob = cumsum(prob)) |> \n ggplot(aes(euftf, cumprob)) + geom_col()\neu_mass | eu_distr"
  },
  {
    "objectID": "W12.html#what-could-be-next",
    "href": "W12.html#what-could-be-next",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "What could be next?",
    "text": "What could be next?\n\nContinuous distribution function\n\nTheoretical and empirical\n\nThe central limit theorem and why it is important empirically\nIndependence of probabilistic events\nConditional probability and the confusion matrix\nMarkov chains"
  },
  {
    "objectID": "W12.html#pca-description",
    "href": "W12.html#pca-description",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "PCA Description",
    "text": "PCA Description\nPrinciple component analysis\n\nis a dimensionality-reduction technique, that means it can be used to reduce the number of variables\ncomputes new variables which represent the data in a different way\ntransforms the data linearly to a new coordinate system where most of the variation in the data can be described with fewer variables than the original data\ncan be seen as unsupervised learning technique because there is no response variable. (Response variable are often produced/supervised by humans for training, e.g., a spam dummy.)\n\nToday: Quick walk through how to use and interpret it."
  },
  {
    "objectID": "W12.html#other-data-of-owid-corona-data",
    "href": "W12.html#other-data-of-owid-corona-data",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "“Other” data of OWiD Corona data",
    "text": "“Other” data of OWiD Corona data\n\n\n\nSelect the variables listed as Others in the OWiD corona data documentation\nRemove those which have many NAs\n\n\nowid <- read_csv(\"data/owid-covid-data.csv\")\nowid_inds <- owid |> \n # Filter for one day and remove rows where continent is NA\n # These are rows with data for continents or world regions\n filter(date == \"2022-10-01\", !is.na(continent)) |> \n # These are the \"Other\" variables\n select(iso_code, continent, location, \n        population:human_development_index) |>\n # We remove the ones with many NA's\n select(-handwashing_facilities, -male_smokers, \n        - female_smokers, -extreme_poverty) |> \n drop_na()\nowid_inds |> count(continent)\n\n# A tibble: 6 × 2\n  continent         n\n  <chr>         <int>\n1 Africa           39\n2 Asia             42\n3 Europe           39\n4 North America    20\n5 Oceania           6\n6 South America    12\n\n\n\nWe have 158 countries and 11 numeric variables.\n\nnames(owid_inds)\n\n [1] \"iso_code\"                   \"continent\"                 \n [3] \"location\"                   \"population\"                \n [5] \"population_density\"         \"median_age\"                \n [7] \"aged_65_older\"              \"aged_70_older\"             \n [9] \"gdp_per_capita\"             \"cardiovasc_death_rate\"     \n[11] \"diabetes_prevalence\"        \"hospital_beds_per_thousand\"\n[13] \"life_expectancy\"            \"human_development_index\""
  },
  {
    "objectID": "W12.html#two-variables",
    "href": "W12.html#two-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Two Variables",
    "text": "Two Variables\nExample for the new axes.\n\n\n\n\n\n\n\nThe two arrows show the two eigenvectors of the covariance matrix of the two variables scaled by the square root of the corresponding eigenvalues, and shifted so their origins are at the means of both variables."
  },
  {
    "objectID": "W12.html#computation-in-r",
    "href": "W12.html#computation-in-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Computation in R",
    "text": "Computation in R\nThe basic function is base-R’s prcomp (there is an older princomp which is not advisable to use).\nThese 3 commands all deliver identical results for prcomp\n\n# prcomp can take a formula with no response variable as 1st argument\nP <- prcomp(~ median_age + life_expectancy, data= owid_inds)\n# prcomp can take a data frame with all numerical vectors as 1st argument\nP <- owid_inds |> select(median_age, life_expectancy) |> prcomp()\n# This is an example using the formula placeholder \".\" for 'take all\n# and the pipe's placeholder \"_\" which one uses when the output should be \n# piped to an argument other than the 1st\nP <- owid_inds |> select(median_age, life_expectancy) |> prcomp(~ ., data= _)\n\n\n\nThe standard output\n\nP\n\nStandard deviations (1, .., p=2):\n[1] 10.69296  2.95809\n\nRotation (n x k) = (2 x 2):\n                      PC1        PC2\nmedian_age      0.8091382  0.5876184\nlife_expectancy 0.5876184 -0.8091382\n\n\n\nThe summary output\n\nsummary(P)\n\nImportance of components:\n                           PC1     PC2\nStandard deviation     10.6930 2.95809\nProportion of Variance  0.9289 0.07109\nCumulative Proportion   0.9289 1.00000"
  },
  {
    "objectID": "W12.html#the-prcomp-object",
    "href": "W12.html#the-prcomp-object",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The prcomp object",
    "text": "The prcomp object\nIncludes 4 different related entities.\n\n\nThe standard deviations related to each principal component.\n\nP$sdev\n\n[1] 10.69296  2.95809\n\n\nThe matrix of variable loadings. (It is also the matrix which rotates the original data vectors.)\n\nP$rotation\n\n                      PC1        PC2\nmedian_age      0.8091382  0.5876184\nlife_expectancy 0.5876184 -0.8091382\n\n\n\nThe means for each original variable.\n\nP$center\n\n     median_age life_expectancy \n       31.15823        73.57741 \n\n\nNote, there are also standard deviations of original variables in $scale when this is set to be used.\nThe centered (scaled, if set) and rotated data.\n\nP$x\n\n             PC1         PC2\n1   -15.30147798 -0.30158529\n2     8.46967981 -0.01934856\n3     0.27527455 -3.88170827\n4     2.78495581 -2.23213257\n5     2.41746174 -2.06645786\n6     4.56375002  1.44493027\n7    11.25046735 -4.01861358\n8    15.39339097  1.33826834\n9     0.66547151  1.19688861\n10    2.74344303  1.56895631\n11    3.18635423 -2.27431439\n12   -3.54022934 -1.35069463\n13   10.29045200  0.53669877\n14    8.10950037  4.39071615\n15   13.34251723 -0.26236181\n16   -4.37020966 -4.46229111\n17  -16.93776248  2.29190137\n18   -3.12614815 -0.04891248\n19   -5.87404744 -1.71082388\n20   11.42328837  3.57162574\n21   -6.67861938  0.07776886\n22    3.24786441 -0.48704930\n23    2.34605999 -1.11724672\n24   11.82248935  6.76586095\n25  -18.02037585  1.74049561\n26  -18.10128967  1.68173378\n27   -6.70529480 -0.22585660\n28  -18.39505599  4.29856418\n29   13.48895662 -1.14471974\n30   -4.76750699 -2.72397157\n31  -22.33121139  8.86767576\n32    7.31198595 -2.84986882\n33    8.06063011  1.73515374\n34    3.02452659 -2.39183806\n35  -14.14471452  1.16878824\n36    5.91429898 -3.98849568\n37   13.27749971  3.57109260\n38    9.31944327 -2.38070454\n39   13.23408329  2.43962673\n40   13.31812495  0.62211827\n41   -8.45956818  1.84938433\n42   -2.58376417 -2.49754876\n43   -0.45747329 -4.57451459\n44   -5.67290445 -2.15797206\n45   -3.03035412 -1.88260370\n46  -15.80534846  6.85901626\n47  -13.85952988 -1.09586840\n48   12.37252460  2.60490418\n49  -15.68152626  5.15690932\n50  -13.29042762 -1.02861793\n51   -5.67641179  3.46274744\n52   14.31618860  0.09869783\n53   14.10959178 -0.97825052\n54  -10.69666185  1.01571059\n55  -17.82510905  1.30143881\n56    6.21550849  4.27584778\n57   17.05009523  2.80094774\n58  -13.72522235  1.78240564\n59   16.53294823  1.30072809\n60   -2.11451419 -0.08048350\n61   -6.25743781 -5.43736541\n62  -16.87582996  2.54697854\n63   -6.08601239  0.11265381\n64  -11.17711330  3.71942404\n65   -4.06917156 -5.04699279\n66   11.84595123  4.52123414\n67   10.50055615 -4.00707238\n68   -4.69555434  1.43141323\n69   -2.59500849  0.41096866\n70    2.82790704 -1.78074007\n71  -10.77812658 -4.14764718\n72   11.22789301 -2.62610131\n73    5.06757765 -7.92793256\n74   19.38298293  1.80093025\n75    0.72013226 -0.58016293\n76   20.28385695  1.07098094\n77   -5.87954413 -5.44718170\n78   -0.43840629 -0.34630735\n79  -13.06983814 -0.99200808\n80   -9.49927316 -0.46289021\n81    3.18052090 -0.05396172\n82   -5.18108013 -1.13341907\n83   -8.79273555  0.60636402\n84   11.31620717  6.10157310\n85    3.09816863 -4.36520494\n86  -15.24495644  0.64165663\n87   -2.13848412 -0.72819134\n88   11.36862762  5.34867729\n89   12.00765033 -1.99802605\n90  -13.19370320 -1.50216242\n91  -16.04098955 -0.13418567\n92    0.49949992 -2.82903407\n93  -20.32523538  2.87209737\n94   14.35685668 -0.63801521\n95    5.88052317  2.52479529\n96   -0.63823938 -2.28346164\n97    4.22661012  5.14255609\n98   -4.24849920  1.49654154\n99    8.36665685  1.99447523\n100   0.56232000 -3.42607145\n101 -18.36842342  2.38992835\n102  -5.45400436  4.00738947\n103  -6.62666413 -1.35520031\n104  14.85726268  0.03436403\n105  10.57470625 -3.08810461\n106  -2.59145830 -2.99748955\n107 -19.54962201 -0.40822638\n108   7.73202903  2.86834452\n109  12.09579308 -2.11939679\n110   2.14576170 -3.73447438\n111  -9.90291188  0.60344735\n112   1.71857540 -4.84803258\n113  -3.37392111 -3.28148244\n114   0.19300798 -3.76842892\n115  -6.20040822 -1.60178884\n116  11.63842401  2.08413905\n117  17.14952515  1.98332087\n118   4.50938307 -4.94698996\n119  11.03457271  4.95777153\n120   6.24446705  5.76757879\n121   4.56869580  0.07669216\n122  -0.09619205  1.22461257\n123 -11.95340613 -4.74163201\n124   1.51252948 -0.82038499\n125   3.97524412  3.10618305\n126  14.99736068 -1.51997588\n127  10.45367524  2.69444253\n128  15.34502876  1.57504058\n129  -8.72053195 -5.61948425\n130  -8.67330823  5.37709112\n131  15.45980409 -0.45496597\n132  17.47043209  0.35018930\n133   4.37972753 -1.02453034\n134 -14.12936912 -0.04359145\n135  -2.37577176  0.61961970\n136  13.38272012 -1.67914822\n137  15.65777638 -1.23810515\n138  -7.81416124 -2.61307574\n139 -15.65950282 -1.34019889\n140   9.33445203  2.36362626\n141 -13.04278322 -4.43283183\n142 -16.88124094  3.23514328\n143  -8.73494696 -3.04695782\n144   4.03988214  3.01717785\n145   3.08240087 -1.62063733\n146   2.77409099 -3.06806436\n147 -17.93950488 -0.41300384\n148   7.39535433  7.24604370\n149   4.88055588 -1.88433903\n150  12.35121731 -0.59914731\n151   8.88283060 -0.07771314\n152   6.13991994 -0.89560137\n153  -3.48506054 -0.23541152\n154  -2.63795972 -0.04042384\n155   2.23758320 -0.62751947\n156 -13.16791532 -0.34642241\n157 -16.58206363 -0.06985186\n158 -16.45498504  2.98855475"
  },
  {
    "objectID": "W12.html#pca-as-exploratory-data-analysis",
    "href": "W12.html#pca-as-exploratory-data-analysis",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "PCA as Exploratory Data Analysis",
    "text": "PCA as Exploratory Data Analysis\nSuppose we do a PCA with all 158 countries (rows) and all 11 numeric variables.\n\nHow long will the vector of standard deviations be? 11\n\nWhat dimensions will the rotation matrix have? 11 x 11\n\nWhat dimensions will the rotated data frame have? 158 x 11\n\n\nWhen we do a PCA for exploration there are 3 things to look at:\n\nThe data in PC coordinates - the centered (scaled, if set) and rotated data.\n\nThe rotation matrix - the variable loadings.\nThe variance explained by each PC - based on the standard deviations."
  },
  {
    "objectID": "W12.html#all-variables",
    "href": "W12.html#all-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "All variables",
    "text": "All variables\nNow, with scale = TRUE (recommended). Data will be centered and scaled (a.k.a. standardized) first.\n\nowid_PCA <- owid_inds |> select(-iso_code, -continent, -location) |> \n prcomp(~ ., data = _, scale = TRUE)\nowid_PCA\n\nStandard deviations (1, .., p=11):\n [1] 2.34241265 1.22235717 1.03373351 0.99489682 0.92287148 0.66475775\n [7] 0.58432716 0.45329540 0.25966139 0.21927321 0.06713354\n\nRotation (n x k) = (11 x 11):\n                                    PC1         PC2         PC3          PC4\npopulation                  0.008865647 -0.03734342  0.47595823  0.870416239\npopulation_density         -0.074719077 -0.46044638 -0.10288680  0.046602918\nmedian_age                 -0.409689271  0.06447942  0.12973022 -0.004468196\naged_65_older              -0.390007415  0.24159016 -0.02468961  0.056783114\naged_70_older              -0.385548522  0.26731058 -0.03864289  0.046944080\ngdp_per_capita             -0.307869646 -0.34923225 -0.10326975 -0.049084375\ncardiovasc_death_rate       0.203402666  0.30746105  0.55758803 -0.314193965\ndiabetes_prevalence        -0.015935611 -0.52365806  0.58702717 -0.310745215\nhospital_beds_per_thousand -0.281461985  0.34868645  0.26585730 -0.180587716\nlife_expectancy            -0.385997705 -0.16628186  0.03523644 -0.001063976\nhuman_development_index    -0.401390496 -0.11256612  0.07650908 -0.066730340\n                                   PC5        PC6          PC7         PC8\npopulation                  0.01039838 -0.1088643 -0.009530076 -0.01188854\npopulation_density          0.85694114  0.1686582 -0.030330909  0.05784099\nmedian_age                  0.02964348  0.1083349  0.151566064 -0.04089755\naged_65_older               0.06392873  0.3010299  0.133281270 -0.30838406\naged_70_older               0.04970870  0.2883236  0.134769694 -0.32266678\ngdp_per_capita             -0.04210924 -0.7155301  0.255572328 -0.38998232\ncardiovasc_death_rate       0.30899782 -0.1596115  0.556827122  0.11586467\ndiabetes_prevalence        -0.24595405  0.3128238 -0.194418712 -0.28853159\nhospital_beds_per_thousand  0.24614720 -0.3347129 -0.712831766  0.04133735\nlife_expectancy            -0.15876730  0.1057214  0.114444482  0.64382282\nhuman_development_index    -0.12068739 -0.1252574  0.075440136  0.36145260\n                                   PC9        PC10         PC11\npopulation                 -0.01568443  0.04316042  0.008103028\npopulation_density          0.01930238  0.04356815  0.017850825\nmedian_age                  0.43211922 -0.76462456  0.050093900\naged_65_older              -0.12286262  0.19597621 -0.724308013\naged_70_older              -0.18333355  0.25044108  0.687015497\ngdp_per_capita             -0.19559611 -0.02457951 -0.012540620\ncardiovasc_death_rate      -0.07816501  0.06233211 -0.006416554\ndiabetes_prevalence        -0.04883139  0.05032474  0.006819981\nhospital_beds_per_thousand -0.11397971 -0.01992701 -0.007573789\nlife_expectancy            -0.58309459 -0.13429476 -0.009463395\nhuman_development_index     0.60349415  0.53386017  0.010100958"
  },
  {
    "objectID": "W12.html#data-in-pc-coordinates",
    "href": "W12.html#data-in-pc-coordinates",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Data in PC coordinates",
    "text": "Data in PC coordinates\n\n\n\nStart plotting PC1 against PC2. By default these are the most important ones. Drill deeper later.\nUse the function augment to append the original data. Here used to draw labels and color by continent.\nNote: augment also created the variables names like .fittedPC1\n\n\n\nplotdata <- owid_PCA |> parsnip::augment(owid_inds)\nplotdata |> ggplot(aes(.fittedPC1, .fittedPC2, color = continent)) +\n geom_point() + \n geom_text(data = plotdata |> \n            filter(.fittedPC2< -3 | .fittedPC1< -5 | .fittedPC1>4), \n           mapping = aes(.fittedPC1, .fittedPC2, label = iso_code),\n           color = \"black\") +\n coord_fixed() + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W12.html#variable-loadings",
    "href": "W12.html#variable-loadings",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Variable loadings",
    "text": "Variable loadings\n\nThe columns of the rotation matrix shows how the original variables load on the principle components.\nWe can try to interpret these loadings and give names to components.\ntidy extracts the rotation matrix in long format with a PC, a column (for the original variable name), and a value variable .\n\n\nowid_PCA |> parsnip::tidy(matrix = \"rotation\") |> \n filter(PC<=6) |> \n ggplot(aes(value, column, fill=value)) + geom_col() +\n facet_wrap(~PC, nrow = 1)"
  },
  {
    "objectID": "W12.html#variance-explained",
    "href": "W12.html#variance-explained",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Variance explained",
    "text": "Variance explained\n\nPrinciple components are by default sorted by importance.\nThe squares of the standard deviation for each component gives its variances and variances have to sum up to the number of variables (in the standardized case).\nThis way the tidy command creates a variable percent which gives the fraction of the total variance explained by each component.\n\n\nowid_PCA |> tidy(matrix = \"eigenvalues\") |> \n ggplot(aes(PC, percent)) + geom_col()"
  },
  {
    "objectID": "W12.html#interpretations-1",
    "href": "W12.html#interpretations-1",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (1)",
    "text": "Interpretations (1)\n\n\nThe first component explains almost 50% of the variance. So most emphasize should be on this.\nTo reach more than 75% of the total variance the first four components are needed.\nAfter the fifth component the added explained variance drops substantially. This is another typical reason to cut off the rest.\nTaking the five components explains 89.9% of the variance of the original 11 variables!"
  },
  {
    "objectID": "W12.html#interpretations-2",
    "href": "W12.html#interpretations-2",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (2)",
    "text": "Interpretations (2)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population.\nPC4 focuses mostly on population.\nPC5 mostly on population density."
  },
  {
    "objectID": "W12.html#interpretations-3",
    "href": "W12.html#interpretations-3",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (3)",
    "text": "Interpretations (3)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths."
  },
  {
    "objectID": "W12.html#interpretations-4",
    "href": "W12.html#interpretations-4",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (4)",
    "text": "Interpretations (4)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population."
  },
  {
    "objectID": "W12.html#interpretations-5",
    "href": "W12.html#interpretations-5",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (5)",
    "text": "Interpretations (5)\n\n\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population."
  },
  {
    "objectID": "W12.html#interpretations-6",
    "href": "W12.html#interpretations-6",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (6)",
    "text": "Interpretations (6)\n\n\nPC4 focuses mostly on population.\nPC5 mostly on population density."
  },
  {
    "objectID": "W12.html#apply-pca",
    "href": "W12.html#apply-pca",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Apply PCA",
    "text": "Apply PCA\n\nBesides standardization, PCA may benefit by preprocessing steps of data transformation with variables with skew distributions (log, square-root, or Box-Cox transformation). This may result in less outliers.\nPCA is a often a useful step of exploratory data analysis when you have a large number of numerical variables to show the empirical dimensionality of the data and its structure\nLimitation: PCA is only sensitive for linear relation ships (no U-shaped) or the like\nThe can be used as predictors in a model instead of the raw variables."
  },
  {
    "objectID": "W12.html#properties-and-relations-of-pca",
    "href": "W12.html#properties-and-relations-of-pca",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Properties and relations of PCA",
    "text": "Properties and relations of PCA\n\nThe principal components (the columns of the rotation matrix) are maximally uncorrelated (actually they are even orthogonal).\nThis also holds for the columns of the rotated data.\nThe total variances of all prinicipal components sum up to the number of variables (when variables are standardized)\nThe PCA is unique. All principle components together are a complete representation of the data. (Unlike other technique of dimensionality reduction which may rely on starting values, random factors, or tuning parameters)\nA technique similar in spirit is factor analysis (e.g. factanal). It is more theory based as it requires to specify to the theoriezed number of factors up front.\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Learning Resources",
    "section": "",
    "text": "All the materials here are freely accessible, many are community standards. The Schedule might refer to specific sections in these materials."
  },
  {
    "objectID": "literature.html#r",
    "href": "literature.html#r",
    "title": "Learning Resources",
    "section": "1 R",
    "text": "1 R\n\nR for Data Science (2nd Edition) https://r4ds.hadley.nz// by Hadley Wickham, Garrett Grolemand, Mine Çetinkaya-Rundel and the R data science community around is our core resource for self-learning data science with R using tidyverse tools.\nTidy Modeling with R https://www.tmwr.org/ by Max Kuhn and Julia Silge is the resource for learning to work with tidymodels building upon the tidyverse tools.\nPart of the course builds on or be inspired by material in Data Science in the Box https://datasciencebox.org/ by Mine Çetinkaya-Rundel and the data science education community around. You can also use the website for accompanying self-study on selected topics."
  },
  {
    "objectID": "literature.html#python",
    "href": "literature.html#python",
    "title": "Learning Resources",
    "section": "2 python",
    "text": "2 python\n\nPython Data Science Handbook https://jakevdp.github.io/PythonDataScienceHandbook/ by Jake VanderPlas is a core resource for data science with python. Also there is a good video playlist https://www.youtube.com/playlist?list=PLWKjhJtqVAbkmRvnFmOd4KhDdlK1oIq23 and a 4-hour “full-course” video https://youtu.be/rfscVS0vtbw."
  },
  {
    "objectID": "literature.html#data-visualization",
    "href": "literature.html#data-visualization",
    "title": "Learning Resources",
    "section": "3 Data Visualization",
    "text": "3 Data Visualization\nggplot2: Elegant Graphics for Data Analysis https://ggplot2-book.org is a resource on understanding the logic of ggplot better.\nA great source for Graphic Design with ggplot2: https://rstudio-conf-2022.github.io/ggplot2-graphic-design/. Look at the Introduction to see what cool things are possible. Work through Concepts of the ggplot2 Package Pt. 1 and Concepts of the ggplot2 Package Pt. 2 for a full introduction to all of ggplot2.\nWebsites on how to decide for what visualization to choose:\nhttps://www.data-to-viz.com/\nhttps://datavizcatalogue.com/search.html"
  },
  {
    "objectID": "literature.html#mathematics",
    "href": "literature.html#mathematics",
    "title": "Learning Resources",
    "section": "4 Mathematics",
    "text": "4 Mathematics\n\nHow to read math https://www.youtube.com/watch?v=Kp2bYWRQylk"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Need help for Homework?",
    "section": "",
    "text": "Do homework together with other students. Working together is encouraged!\nCommit and push your intermediate results often, instructors may check your code directly.\nRead error messages carefully! Often they are informative or even tell you what to do! (Hint: Uninformative error messages are also common. Maybe you get at least a hint where the problem lies. You will become more advanced on this, error messages which are not helpful now may become helpful in the future.)\nDo the homework before the next Data Science Tools Session and prepare questions to ask in class.\nIt is absolutely OK to decide to ask instructors for help outside of class. We try to care, if we are not too busy with other things. If you do, please do first: Document and describe where you get stuck as precise as possible. Try to find out why as much as possible.\n\nThe instructors prefer that you ask for help in the Discussions section of the Organization repo https://github.com/CU-F23-MDSSB-01-Concepts-Tools/Organization/discussions. Only registered students and instructors can see these discussions. Maybe, there is already an answer.\nYou can also write to instructors directly. Use a Teams chat! You can also write an email."
  },
  {
    "objectID": "W06.html#purpose-of-modeling",
    "href": "W06.html#purpose-of-modeling",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Purpose of modeling",
    "text": "Purpose of modeling\nWe use models to\n\nexplain relations between variables\nmake predictions\n\nFirst, we focus on linear models."
  },
  {
    "objectID": "W06.html#palmer-penguins",
    "href": "W06.html#palmer-penguins",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Palmer Penguins",
    "text": "Palmer Penguins\nWe use the dataset Palmer Penguins\nChinstrap, Gentoo, and Adélie Penguins\n  \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "W06.html#body-mass-in-grams",
    "href": "W06.html#body-mass-in-grams",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Body mass in grams",
    "text": "Body mass in grams\n\npenguins |>\n  ggplot(aes(body_mass_g)) +\n  geom_histogram()"
  },
  {
    "objectID": "W06.html#flipper-length-in-millimeters",
    "href": "W06.html#flipper-length-in-millimeters",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Flipper length in millimeters",
    "text": "Flipper length in millimeters\n\npenguins |>\n  ggplot(aes(flipper_length_mm)) +\n  geom_histogram()"
  },
  {
    "objectID": "W06.html#relate-variables-as-a-line",
    "href": "W06.html#relate-variables-as-a-line",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Relate variables as a line",
    "text": "Relate variables as a line\nA line is a shift-scale transformation of the identity function usually written in the form\n\\[f(x) = a\\cdot x + b\\]\nwhere \\(a\\) is the slope, \\(b\\) is the intercept.1\n\n\na <- 0.5\nb <- 1\nfunc <- function(x) a*x + b\nggplot() + geom_function(fun = func, size = 2) + \n    xlim(c(0,2)) + ylim(c(0,2)) + coord_fixed() + # Set axis limits and make axis equal\n    # intercept line:\n    geom_line(data=tibble(x=c(0,0),y=c(0,1)), mapping = aes(x,y), color = \"blue\") +\n    # slope:\n    geom_line(data=tibble(x=c(1.5,1.5),y=c(1.25,1.75)), mapping = aes(x,y), color = \"red\") +\n    # x-interval of length one:\n    geom_line(data=tibble(x=c(0.5,1.5),y=c(1.25,1.25)), mapping = aes(x,y), color = \"gray\") +\n    theme_classic(base_size = 24)\n\n\n\n\n\n\nThis a scale and a shift in the \\(y\\) direction. Note: For lines there are always an analog transformations on the \\(x\\) direction."
  },
  {
    "objectID": "W06.html#penguins-linear-model",
    "href": "W06.html#penguins-linear-model",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Penguins: Linear model",
    "text": "Penguins: Linear model\nFlipper length as a function of body mass.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W06.html#penguins-other-smoothing-method",
    "href": "W06.html#penguins-other-smoothing-method",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Penguins: Other smoothing method",
    "text": "Penguins: Other smoothing method\nFlipper length as a function of body mass with loess1 smoothing.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"loess\") + \n theme_classic(base_size = 24)\n\n\n\n\n\n\nThis is a less theory-driven and more data-driven model. Why? We don’t have a simple mathematical form of the function.\nloess = locally estimated scatterplot smoothing"
  },
  {
    "objectID": "W06.html#terminology",
    "href": "W06.html#terminology",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Terminology",
    "text": "Terminology\n\nResponse variable:1 Variable whose behavior or variation you are trying to understand, on the y-axis\nExplanatory variable(s):2 Other variable(s) that you want to use to explain the variation in the response, on the x-axis\nPredicted value: Output of the model function.\n\nThe model function gives the typical (expected) value of the response variable conditioning on the explanatory variables\nResidual(s): A measure of how far away a case is from its predicted value (based on a particular model)\nResidual = Observed value - Predicted value\nThe residual tells how far above/below the expected value each case is\n\n\nAlso dependent variable in statistics or empirical social sciences.Also independent variable(s) in statistics or empirical social sciences."
  },
  {
    "objectID": "W06.html#more-explanatory-variables",
    "href": "W06.html#more-explanatory-variables",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "More explanatory variables",
    "text": "More explanatory variables\nHow does the relation between flipper length and body mass change with different species?\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm, \n            color = species)) +\n geom_point() +\n geom_smooth(method = \"lm\",\n             se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W06.html#technical-how-to-color-penguins-but-keep-one-model",
    "href": "W06.html#technical-how-to-color-penguins-but-keep-one-model",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Technical: How to color penguins but keep one model?",
    "text": "Technical: How to color penguins but keep one model?\nPut the mapping of the color aesthetic into the geom_point command.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n                                                y = flipper_length_mm)) +\n geom_point(aes(color = species)) +\n geom_smooth(method = \"lm\",\n                                                    se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W06.html#models---upsides-and-downsides",
    "href": "W06.html#models---upsides-and-downsides",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can reveal patterns that are not evident in a graph of the data. This is an advantage of modeling over simple visual inspection of data.\nThe risk is that a model is imposing structure that is not really there in the real world data.\n\nPeople imagined animal shapes in the stars. This is maybe a good model to detect and memorize shapes, but it has nothing to do with these animals.\nEvery model is a simplification of the real world, but there are good and bad models (for particular purposes).\nA skeptical (but constructive) approach to a model is always advisable."
  },
  {
    "objectID": "W06.html#variation-around-a-model",
    "href": "W06.html#variation-around-a-model",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Variation around a model",
    "text": "Variation around a model\nis as interesting and important as the model!\nStatistics is the explanation of uncertainty of variation in the context of what remains unexplained.\n\nThe scattered data of flipper length and body mass suggests that there maybe other factors that account for some parts of the variability.\nOr is it randomness?\nAdding more explanatory variables can help (but need not)"
  },
  {
    "objectID": "W06.html#all-models-are-wrong",
    "href": "W06.html#all-models-are-wrong",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "All models are wrong …",
    "text": "All models are wrong …\n… but some are useful. (George Box)\nExtending the range of the model:\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE, \n                                                fullrange = TRUE) +\n    xlim(c(0,7000)) + ylim(c(0,230)) +\n theme_classic(base_size = 24)\n\n\n\n\n\n\n\nThe model predicts that penguins with zero weight still have flippers of about 140 mm on average.\nIs the model useless?"
  },
  {
    "objectID": "W06.html#two-model-purposes",
    "href": "W06.html#two-model-purposes",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Two model purposes",
    "text": "Two model purposes\nModels can be used for:\n\nExplanation: Understand the relations hip of variables in a quantitative way.\nFor the linear model, interpret slope and intercept.\nPrediction: Plug in new values for the explanatory variable(s) and receive the expected response value.\nFor the linear model, predict the flipper length of new penguins by their body mass."
  },
  {
    "objectID": "W06.html#in-r-tidymodels",
    "href": "W06.html#in-r-tidymodels",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "In R: tidymodels",
    "text": "In R: tidymodels\n\n\n\nFrom https://datasciencebox.org"
  },
  {
    "objectID": "W06.html#our-goal",
    "href": "W06.html#our-goal",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Our goal",
    "text": "Our goal\nPredict flipper length from body mass\naverage flipper_length_mm \\(= \\beta_0 + \\beta_1\\cdot\\) body_mass_g"
  },
  {
    "objectID": "W06.html#step-1-specify-model",
    "href": "W06.html#step-1-specify-model",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlibrary(tidymodels)\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "W06.html#step-2-set-the-model-fitting-engine",
    "href": "W06.html#step-2-set-the-model-fitting-engine",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Step 2: Set the model fitting engine",
    "text": "Step 2: Set the model fitting engine\n\nlinear_reg() |> \n    set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "W06.html#step-3-fit-model-and-estimate-parameters",
    "href": "W06.html#step-3-fit-model-and-estimate-parameters",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Step 3: Fit model and estimate parameters",
    "text": "Step 3: Fit model and estimate parameters\nOnly now, the data and the variable selection comes in.\nUse of formula syntax\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\n\n\nNote: The fit command does not follow the tidyverse principle the the data comes first. Instead, the formula comes first. This is to relate to existing traditions of a much older established way of modeling in R."
  },
  {
    "objectID": "W06.html#what-does-the-output-say",
    "href": "W06.html#what-does-the-output-say",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "What does the output say?",
    "text": "What does the output say?\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\n\naverage flipper_length_mm \\(= 136.72956 + 0.01528\\cdot\\) body_mass_g\n\n\nInterpretation:\nThe penguins have a flipper length of 138 mm plus 0.01528 mm for each gram of body mass (that is 15.28 mm per kg). Penguins with zero mass have a flipper length of 138 mm. However, this is not in the range where the model was fitted …"
  },
  {
    "objectID": "W06.html#show-output-in-tidy-form",
    "href": "W06.html#show-output-in-tidy-form",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Show output in tidy form",
    "text": "Show output in tidy form\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n    tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107"
  },
  {
    "objectID": "W06.html#parameter-estimation",
    "href": "W06.html#parameter-estimation",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nNotation from statistics: \\(\\beta\\)’s for the population parameters and \\(b\\)’s for the parameters estimated from the sample statistics.\n\\[\\hat y = \\beta_0 + \\beta_1 x\\]\nIs what we cannot have. (\\(\\hat y\\) stands for predicted value of \\(y\\). )\n\nWe estimate \\(b_0\\) and \\(b_1\\) in\n\\[\\hat y = b_0 + b_1 x\\]\n\nA typical follow-up data analysis question is what the fitted values \\(b_0\\) and \\(b_1\\) tell us about the population-wide values \\(\\beta_0\\) and \\(\\beta_1\\)?\nWhat type of question is it?\nDescriptive, Exploratory, Inferential, Predictive, Causal, Mechanistic\n\n\n\n\nA typical inferential question."
  },
  {
    "objectID": "W06.html#fitting-method-least-squares-regression",
    "href": "W06.html#fitting-method-least-squares-regression",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Fitting method: Least squares regression",
    "text": "Fitting method: Least squares regression\n\nThe regression line shall minimize the sum of the squared residuals (or, identically, their mean).\nMathematically: The residual for case \\(i\\) is \\(e_i = \\hat y_i - y_i\\).\nNow we want to minimize \\(\\sum_{i=1}^n e_i^2\\)\n(or equivalently \\(\\frac{1}{n}\\sum_{i=1}^n e_i^2\\) the the mean of squared errors, which we will look at later)."
  },
  {
    "objectID": "W06.html#visualization-of-residuals",
    "href": "W06.html#visualization-of-residuals",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Visualization of residuals",
    "text": "Visualization of residuals\nThe residuals are the gray lines between predictid values on the regression line and the actual values."
  },
  {
    "objectID": "W06.html#proporties-of-least-squares-regression",
    "href": "W06.html#proporties-of-least-squares-regression",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Proporties of least squares regression",
    "text": "Proporties of least squares regression\nThe regression lines goes through the point (mean(x), mean(y)).\n\nmean(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4201.754\n\nmean(penguins$flipper_length_mm, na.rm = TRUE)\n\n[1] 200.9152"
  },
  {
    "objectID": "W06.html#proporties-of-least-squares-regression-1",
    "href": "W06.html#proporties-of-least-squares-regression-1",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Proporties of least squares regression",
    "text": "Proporties of least squares regression\nResiduals sum up to zero\n\npengmod <- linear_reg() |>  set_engine(\"lm\") |> fit(flipper_length_mm ~ body_mass_g, data = penguins)\npengmod$fit$residuals |> sum()\n\n[1] -3.765044e-14\n\n\nThere is no correlation between residuals and the explanatory variable\n\ncor(pengmod$fit$residuals, na.omit(penguins$body_mass_g))\n\n[1] -1.353445e-16\n\n\nThe correlation of \\(x\\) and \\(y\\) is the slope \\(b_1\\) corrected by their standard deviations.\n\ncorrelation <- cor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")\nsd_flipper <- sd(penguins$flipper_length_mm, na.rm = T)\nsd_mass <- sd(penguins$body_mass_g, na.rm = T)\nc(correlation, sd_flipper, sd_mass)\n\n[1]   0.8712018  14.0617137 801.9545357\n\ncorrelation * sd_flipper / sd_mass\n\n[1] 0.01527592\n\npengmod$fit$coefficients\n\n (Intercept)  body_mass_g \n136.72955927   0.01527592"
  },
  {
    "objectID": "W06.html#linear-model-when-explanatory-variables-are-categorical",
    "href": "W06.html#linear-model-when-explanatory-variables-are-categorical",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Linear model when explanatory variables are categorical",
    "text": "Linear model when explanatory variables are categorical\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ species, data = penguins) |> \n    tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\nWhat happened? Two of the three species categories appear as variables now.\n\nCategorical variables are automatically encoded to dummy variables\nEach coefficient describes the expected difference between flipper length of that particular species compared to the baseline level\nWhat is the baseline level?\n\n\n\nThe first category! (Here alphabetically \"Adelie\")"
  },
  {
    "objectID": "W06.html#how-do-dummy-variables-look",
    "href": "W06.html#how-do-dummy-variables-look",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "How do dummy variables look",
    "text": "How do dummy variables look\n\n\n\nspecies\nspeciesChinstrap\nspeciesGentoo\n\n\n\n\nAdelie\n0\n0\n\n\nChinstrap\n1\n0\n\n\nGentoo\n0\n1\n\n\n\nThen the computation is as usual with the zero-one variables."
  },
  {
    "objectID": "W06.html#interpretation",
    "href": "W06.html#interpretation",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Interpretation",
    "text": "Interpretation\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ species, data = penguins) |> \n    tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\n\nFlipper length of the baseline species is the intercept.\nFlipper length of the two other species add their coefficient"
  },
  {
    "objectID": "W06.html#compare-to-a-visualization",
    "href": "W06.html#compare-to-a-visualization",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Compare to a visualization",
    "text": "Compare to a visualization\n\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\n\nThe red dots are the average values for species.\nThe rest is a boxplot. More on these later."
  },
  {
    "objectID": "W06.html#where-a-linear-model-is-bad",
    "href": "W06.html#where-a-linear-model-is-bad",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Where a linear model is bad",
    "text": "Where a linear model is bad\nTotal corona cases in Germany in the first wave 2020."
  },
  {
    "objectID": "W06.html#log-transformation",
    "href": "W06.html#log-transformation",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "\\(\\log\\) transformation",
    "text": "\\(\\log\\) transformation\nInstead of Cumulative_cases we look at \\(\\log(\\)Cumulative_cases\\()\\)\n\nAlmost perfect fit of a linear model.\nThe model is \\(y=\\beta_0 e^{\\beta_1\\cdot x}\\) (\\(y=\\) Cumulative cases, \\(x=\\) Days).\n\\(\\log(y)=\\log(\\beta_0) + \\beta_1\\cdot x\\) (A linear model!)"
  },
  {
    "objectID": "W06.html#log-transformation-1",
    "href": "W06.html#log-transformation-1",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "\\(\\log\\) transformation",
    "text": "\\(\\log\\) transformation\n\nWhat is the difference to the penguin model?\n\n\n\\(x\\) has an ordered structure and no duplicates\n\nThe fit looks so good. Why?\n\n\nThis shows exponential growth (more later).\nMaybe we can go after a mechanistic explanation here."
  },
  {
    "objectID": "W06.html#however-it-works-only-in-a-certain-range",
    "href": "W06.html#however-it-works-only-in-a-certain-range",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "However, it works only in a certain range …",
    "text": "However, it works only in a certain range …"
  },
  {
    "objectID": "W06.html#backup-correlations",
    "href": "W06.html#backup-correlations",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Backup Correlations",
    "text": "Backup Correlations"
  },
  {
    "objectID": "W06.html#correlation-and-linear-regression",
    "href": "W06.html#correlation-and-linear-regression",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Correlation and linear regression",
    "text": "Correlation and linear regression\n\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107\n\n\n\npenguins_standardized <- penguins |> \n mutate(flipper_length_mm_s = scale(flipper_length_mm)[,1],\n        body_mass_g_s = scale(body_mass_g)[,1])\nlinear_reg() |> \n set_engine(\"lm\") |> \n fit(flipper_length_mm_s ~ body_mass_g_s, data = penguins_standardized) |> tidy()\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   -9.33e-16    0.0266 -3.51e-14 1.00e+  0\n2 body_mass_g_s  8.71e- 1    0.0266  3.27e+ 1 4.37e-107\n\n\n\ncor(penguins_standardized$body_mass_g_s,\n    penguins_standardized$flipper_length_mm_s, \n    use = \"pairwise.complete.obs\")\n\n[1] 0.8712018"
  },
  {
    "objectID": "W06.html#correlation-and-linear-regression-1",
    "href": "W06.html#correlation-and-linear-regression-1",
    "title": "W#06: Summary Statistics, Exploratory Data Analysis, Modeling, Fitting a Linear Model",
    "section": "Correlation and linear regression",
    "text": "Correlation and linear regression\nWhen the two variables in the linear regression are standardized (standard scores)\n\nthe intercept is zero\nthe coefficient coincides with the correlation\n\n\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts\n\n\n\nThe function scale is a function for standardization from base R which delivers a matrix which is unconventional for tidyverse.\nThe function cor is also from base R. It outputs NA whenever one value in one variable is NA. Therefore a methods to use has to be specified. use = \"pairwise.complete.obs\" keeps all values where both variables are not NA."
  },
  {
    "objectID": "hw-01.html",
    "href": "hw-01.html",
    "title": "Homework 01",
    "section": "",
    "text": "The goal of this assignment is to introduce you to R, RStudio, Git, and GitHub, which you’ll be using throughout the course both to learn the data science concepts discussed in the course and to analyze real data and come to informed conclusions."
  },
  {
    "objectID": "hw-01.html#prerequisites",
    "href": "hw-01.html#prerequisites",
    "title": "Homework 01",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites\nFirst, you need to achieve the following\n\nReview the Lecture of Week 1\nRead Happy Git with R: Why Git? Why GitHub? (It is OK do a fast read only. You may reconsider later.)\nYou have a GitHub account to become a member of the GitHub organization. If you do not have an account follow the instructions on the front page of https://github.com/CU-F23-MDSSB-01-Concepts-Tools.\nYou have provided your GitHub username in this form https://forms.office.com/e/fraU0w0gQq (If you provided your name wrongly, write an email to the instructor.)\nSoftware installations on your computer:\n\nRead Happy Git with R: Install or upgrade R and RStudio\nFollow the instructions to install R and RStudio\nRead Happy Git with R: Install Git\nFollow the instructions (choose the highly recommended) to install Git SCM (= Source Control Managment)\nInstall quarto CLI (= Command Line Interface) https://quarto.org/docs/get-started/.\nNote: In the following you will do similar things as described in Tutorial: Hello, Quarto and the following two tutorials.\n\nIn R, install some packages from the following list. You can use the command install.packages(\"tidyverse\",\"gitcreds\",\"usethis\")\nNotes: tidyverse is a collection of many packages it can take some time to install. usethis an gitcreds provide convenient functions to tell your git-installation your GitHub-credentials to push your work back to github via a Personal Access Token (PAT). Read Happy Git with R: Personal access token for HTTPS for how PATs are used similar but not identical to passwords.\n\n\n\n\n\n\n\nMore software\n\n\n\n\n\nSoon, we will need more software: python using the IDE (integrated development environment) Visual Studio Code."
  },
  {
    "objectID": "hw-01.html#terminology",
    "href": "hw-01.html#terminology",
    "title": "Homework 01",
    "section": "1.2 Terminology",
    "text": "1.2 Terminology\nWe’ve already thrown around a few new terms, so let’s define them before we proceed.\n\nR and python: Names of the programming language we will be using throughout the course.\nRStudio: An integrated development environment developed for R. In other words, a convenient interface for writing and running code.\nGit: A version control system.\nGitHub: A web platform for hosting version controlled files and facilitating collaboration among users.\nquarto: A command line tool which can render (among other things) qmd-files (with text and code chunks) to nice looking html\nRepository: A Git repository contains all of your project’s files and stores each file’s revision history.\nIt’s common to refer to a repository as a repo.\n\nIn this course, assignment you work on will be contained in a personalized git repo.\nFor individual assignments, only you will have access to the repo. For team assignments, all team members will have access to a single repo where they work collaboratively.\nAll repos associated with this course are housed in the GitHub organization https://github.com/CU-F23-MDSSB-01-Concepts-Tools. The organization is set up such that students can only see repos they have access to, but the course instructors can see all of them."
  },
  {
    "objectID": "hw-01.html#starting-slowly-step-by-step",
    "href": "hw-01.html#starting-slowly-step-by-step",
    "title": "Homework 01",
    "section": "1.3 Starting slowly step by step",
    "text": "1.3 Starting slowly step by step\nAs the course progresses, you are encouraged to explore beyond what the assignments dictate; a willingness to experiment will make you a much better programmer! Before we get to that stage, however, you need to build some basic fluency in the tools and the workflow we use. First, we will explore the fundamental building blocks of all of these tools.\nBefore you can get started with the analysis, you need to make sure you:\n\nhave a GitHub account\nare a member of the course GitHub organization https://github.com/CU-F23-MDSSB-01-Concepts-Tools\nhave the needed software stack installation on your local machine (see the Prerequisites Checklist above)\n\nIf you failed to confirm any of these, it means you have not yet completed the prerequisites for this assignment. Please go back to Prerequisites and complete them before continuing the assignment."
  },
  {
    "objectID": "hw-01.html#step-0.-authenticate-git-to-access-your-github-content",
    "href": "hw-01.html#step-0.-authenticate-git-to-access-your-github-content",
    "title": "Homework 01",
    "section": "2.1 Step 0. Authenticate git to access your GitHub content",
    "text": "2.1 Step 0. Authenticate git to access your GitHub content\nBefore you can clone your repository you need to tell GitHub that you are authorized to do this, and to that end you need to make a Personal Access Token (PAT) in your GitHub account and make this available to git and RStudio on your local machine.\nThere are several ways to do this (e.g. from the command line) but as we will use RStudio anyway, we can use a convenient way provided there. Read more about PATs and how to use them in Happy Git with R: Personal access token for HTTPS (in particular the TL;DR which describes what we use).\nOpen RStudio and install the packages usethis and gitcreds if you haven’t done already: Go to the “Console” pane at the bottom left. Type in\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nand hit Enter. Now the packages should be installed.\nNow, use two commands. Copy them to the console and click Enter:\nusethis::create_github_token()\nThis opens http://github.com and you may need to log in. Then you can make the PAT (read more details in “Happy Git with R”). For today, you can go the fast way and do not think about the options and click “Generate token”. (If you feel safe enough you can also go for a token without an expiration day against advise from GitHub, if not you just have to do the “dance” again when the PAT expires.)\nUse the clipboard icon 📋 to copy the PAT. Go back to RStudio and do in the console:\ngitcreds::gitcreds_set()\nIn the dialog in the console paste your PAT from the clipboard and press Enter. That should be it and you do not need to repeat these steps until the PAT expires. (If the PAT expires you have to make a new one in the same way.)"
  },
  {
    "objectID": "hw-01.html#step-1.-get-url-of-repo-to-be-cloned",
    "href": "hw-01.html#step-1.-get-url-of-repo-to-be-cloned",
    "title": "Homework 01",
    "section": "2.2 Step 1. Get URL of repo to be cloned",
    "text": "2.2 Step 1. Get URL of repo to be cloned\nOn GitHub https://github.com/CU-F23-MDSSB-01-Concepts-Tools, open your repository for Homework 1.\n\nOn GitHub, click on the green Code button, select HTTPS (this might already be selected by default, and if it is, you’ll see the text Use Git or checkout with SVN using the web URL). Click on the clipboard icon 📋 to copy the repo URL."
  },
  {
    "objectID": "hw-01.html#step-2.-go-to-rstudio",
    "href": "hw-01.html#step-2.-go-to-rstudio",
    "title": "Homework 01",
    "section": "2.3 Step 2. Go to RStudio",
    "text": "2.3 Step 2. Go to RStudio\nGo to your RStudio. Select “New Project…” either from the File menu or from the special project menu on the top right of the RStudio window.\n\nIn the New Project Wizard, click on “Version Control” and then “Git”.\n\n\n\n\n\n\nImportant\n\n\n\nIf “Version Control” or “Git” is not available in your RStudio, then either you haven’t installed git on you computer or your RStudio installation has not recognized it properly. In a correct installation RStudio would recognize git on your machine automatically when started. You have to solve this issue first to continue.\n\n\nThen paste the repositories URL (which should still be in your clipboard) into the “Repository URL:” field. Leave directory name as it is automatically chosen, but make sure that you create the directory in a the folder where you want to store the course material on your computer via the “Browse …” button.\n\n\n\n\n\n\nOrganize your Computer for your Studies!\n\n\n\n\n\n\nCreate a folder for Data Science Concepts and Tools\nPut all repos there\n\n\n\n\nWhen you click “Create Project”\n\ngit will create a new directory in the folder, copies all the files from github to it, and initializes your git repository locally\nRStudio will switch to that as the new project"
  },
  {
    "objectID": "hw-01.html#step-1.-update-the-yaml",
    "href": "hw-01.html#step-1.-update-the-yaml",
    "title": "Homework 01",
    "section": "4.1 Step 1. Update the YAML",
    "text": "4.1 Step 1. Update the YAML\nOpen the quarto (qmd) file in your project. In the YAML change the author name to your name, and “Render” the document.\n This calls quarto, and quarto renders the document. In this case, that means, quarto creates a new file “hw-01-R.md” in the html format as specified in the YAML.\nWhen the file was rendered successfully, RStudio shows it in the “Viewer” pane at the bottom right. At the same place you can look in the “Files” pane you can check if the file is there. You can click on it an select to show it in your Brwoser.\nNow you see a nice html-page in the Browser! You made a page like every ordinary site on the internet. So you made an important step towards publishing you data science work on the internet. However now the html page is only locally on your computer.\n\nIf you do not find the “Render” button in your RStudio installation, then either quarto is not installed or RStudio has not recognized. You have to fix this issue first before you can continue. Another source of error while rendering could be that you haven’t installed the tidyverse package."
  },
  {
    "objectID": "hw-01.html#step-2-commit",
    "href": "hw-01.html#step-2-commit",
    "title": "Homework 01",
    "section": "4.2 Step 2: Commit",
    "text": "4.2 Step 2: Commit\nGo to the Git pane in your RStudio (top right corner).\nYou should see that your qmd (quarto) file and its output, your html-file, are listed there as recently changed files.\nNext, click on Diff. This will pop open a new window that shows you the difference between the last committed state of the document and its current state that includes your changes. (Click on the file “hw-01-R.qmd”.) If you’re happy with these changes, click on the checkboxes of all files in the list, and type “Update author name” in the Commit message box and hit Commit and then close the window.\n\nYou don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we may tell you exactly when to commit and what commit message to use. As the semester progresses we will let you make these decisions."
  },
  {
    "objectID": "hw-01.html#step-3.-push",
    "href": "hw-01.html#step-3.-push",
    "title": "Homework 01",
    "section": "4.3 Step 3. Push",
    "text": "4.3 Step 3. Push\nNow that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub.\nWhy?\nSo that others can see your changes.\nAnd by others, we mean the course instructors (your repos in this course are private to you and us, only).\nGo the Git pane and click on Push.\nThought exercise: Which of the steps “updating the YAML”, “committing”, and “pushing” involves talking to GitHub?1"
  },
  {
    "objectID": "hw-01.html#check-what-you-did",
    "href": "hw-01.html#check-what-you-did",
    "title": "Homework 01",
    "section": "4.4 Check what you did",
    "text": "4.4 Check what you did\nGo to your repository on https://github.com/JU-F22-MDSSB-MET-01 and check if the files appear there as in you local folder.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nYou may wonder why clicking on the the html-file does not show the page as you saw it locally in your browser.\nThe reason is, that GitHub is essentially a code editor. So, it shows you the html-code. This code was created by rendering. DO not touch it “by hand”. It will be overwritten every time you render again.\nViewing the html as in the browser is not possible here. The instructor will clone you repository and can then look at your html locally on their machines.\nContext Information: It is possible to publish webpages from GitHub. You can learn this later.\nContext Information: GitHub provides its own gfm = GitHub Flavoured Markdown. The README.md files are written in this. gfm is very similar to the Markdown language we use."
  },
  {
    "objectID": "W04.html#definition-sets-and-vectors",
    "href": "W04.html#definition-sets-and-vectors",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Definition: Sets and vectors",
    "text": "Definition: Sets and vectors\nA set is mathematical model for the collection of different things.\nExamples\n\n\\(\\{3, \\text{Hi}, 😀, 🖖 \\}\\)\n\\(\\{1,3,5\\}\\)\nThe natural numbers \\(\\mathbb{N} = \\{1, 2, 3, \\dots\\}\\) (infinite!)\n\\(\\{\\mathtt{\"EWR\"}, \\mathtt{\"LGA\"}, \\mathtt{\"JFK\"}\\}\\)\nthese are origin airports in flights"
  },
  {
    "objectID": "W04.html#math-sets-and-vectors-1",
    "href": "W04.html#math-sets-and-vectors-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Math: Sets and vectors",
    "text": "Math: Sets and vectors\nA vector is an ordered collection of things (elements) of the same type.\nIn a set each thing can only be once and the order does not matter!\n\\(\\{1,3,5\\} = \\{3,5,1\\} = \\{1,1,1,3,5,5\\}\\)\nFor vectors:\n\\([1\\ 3\\ 5] \\neq [3\\ 5\\ 1]\\) because we compare component-wise, so we cannot even compare with those with the vector \\([1\\ 1\\ 1\\ 3\\ 5\\ 5]\\)"
  },
  {
    "objectID": "W04.html#math-set-operations",
    "href": "W04.html#math-set-operations",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Math: Set operations",
    "text": "Math: Set operations\nSets \\(A = \\{🐺, 🦊, 🐶\\}\\) and \\(B = \\{🐶, 🐷, 🐹\\}\\), \\(C = \\{🐶, 🐷\\}\\):\n\n\nSet union \\(A \\cup B\\) = {🐺, 🦊, 🐶, 🐷, 🐹}\n\\(x \\in A \\cup B\\) when \\(x \\in A\\) | (or) \\(x\\in B\\)\nSet intersection \\(A \\cap B\\) = {🐶}\n\\(x \\in A \\cap B\\) when \\(x \\in A\\) & (and) \\(x\\in B\\)\nSet difference \\(A \\setminus B = \\{🐺, 🦊\\}\\), \\(B \\setminus A\\) = {🐷, 🐹}\nSubset: \\(C \\subset B\\) but \\(C \\not\\subset A\\)\n\n\n\nSee the analogy of set operations and logical operations."
  },
  {
    "objectID": "W04.html#set-operations-in-r",
    "href": "W04.html#set-operations-in-r",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Set operations in R",
    "text": "Set operations in R\nunique shows the set of elements in a vector\n\nunique(flights$origin)\n\n[1] \"EWR\" \"LGA\" \"JFK\"\n\n\n\nsetequal tests for set equality\n\nsetequal(c(\"EWR\",\"LGA\",\"JFK\"), c(\"EWR\",\"EWR\",\"LGA\",\"JFK\"))\n\n[1] TRUE\n\n\n\n\nunion, intersect, setdiff treat vectors as sets and operate as expected\n\nunion(1:5,3:7)\n\n[1] 1 2 3 4 5 6 7\n\nintersect(1:5,3:7)\n\n[1] 3 4 5\n\nsetdiff(1:5,3:7)\n\n[1] 1 2"
  },
  {
    "objectID": "W04.html#sets-take-away",
    "href": "W04.html#sets-take-away",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Sets: Take-away",
    "text": "Sets: Take-away\n\nSet operations are not a daily business in data science\nHowever, they are useful for data exploration!\nKnowing set operations is key to understand probability:\n\nA sample space is the set of all atomic events.\nAn event is a subset of the sample\nA probability function assigns probabilities to all events."
  },
  {
    "objectID": "W04.html#functions-mathematically",
    "href": "W04.html#functions-mathematically",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Functions mathematically",
    "text": "Functions mathematically\nConsider two sets: The domain \\(X\\) and the codomain \\(Y\\).\nA function \\(f\\) assigns each element of \\(X\\) to exactly one element of \\(Y\\).\n\n\nWe write \\(f : X \\to Y\\)\n“\\(f\\) maps from \\(X\\) to \\(Y\\)”\nand \\(x \\mapsto f(x)\\)\n“\\(x\\) maps to \\(f(x)\\)”\nThe yellow set is called the image of \\(f\\).\n\n\n\n\n\n\nPicture from wikipedia."
  },
  {
    "objectID": "W04.html#conventions-in-mathematical-text",
    "href": "W04.html#conventions-in-mathematical-text",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Conventions in mathematical text",
    "text": "Conventions in mathematical text\n\nSets are denoted with capital letters.\nTheir elements with (corresponding) small letters.\nFunctions are often called \\(f\\), \\(g\\), or \\(h\\).\nOther terminology can be used!\n\n\nImportant in math\n\nWhen you read math:\nKeep track of what objects are! What are functions, what are sets, what are numbers, …1\nWhen you write math: Define what objects are.\n\n\nWatch: How to read math https://www.youtube.com/watch?v=Kp2bYWRQylk"
  },
  {
    "objectID": "W04.html#is-this-a-mathematical-function",
    "href": "W04.html#is-this-a-mathematical-function",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Is this a mathematical function?",
    "text": "Is this a mathematical function?\n \\(\\ \\mapsto\\ \\) \nInput from \\(X = \\{\\text{A picture where a face can be recognized}\\}\\).\nFunction: Upload input at https://funny.pho.to/lion/ and download output.\nOutput from \\(Y = \\{\\text{Set of pictures with a specific format.}\\}\\)\n\nYes, it is a function. Important: Output is the same for the same input!"
  },
  {
    "objectID": "W04.html#is-this-a-mathematical-function-1",
    "href": "W04.html#is-this-a-mathematical-function-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Is this a mathematical function?",
    "text": "Is this a mathematical function?\nInput a text snippet. Function: Enter text at https://www.craiyon.com. Output a picture.\n\n\n\n\nOther examples:\n\n“Nuclear explosion broccoli”\n“The Eye of Sauron reading a newspaper”\n“The legendary attack of Hamster Godzilla wearing a tiny Sombrero”\n\n  \n\n\n\nNo, it is not a function. It has nine outcomes and these change when run again."
  },
  {
    "objectID": "W04.html#graphs-of-functions",
    "href": "W04.html#graphs-of-functions",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Graphs of functions",
    "text": "Graphs of functions\n\nA function is characterized by the set all possible pairs \\((x,f(x))\\).\nThis is called its graph.\nWhen domain and codomain are real numbers then the graph can be shown in a Cartesian coordinate system. Example \\(f(x) = x^3 - x^2\\)"
  },
  {
    "objectID": "W04.html#some-functions-f-mathbbr-to-mathbbr",
    "href": "W04.html#some-functions-f-mathbbr-to-mathbbr",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Some functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)",
    "text": "Some functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)\n\n\n\\(f(x) = x\\) identity function\n\\(f(x) = x^2\\) square function\n\\(f(x) = \\sqrt{x}\\) square root function\n\\(f(x) = e^x\\) exponential function\n\\(f(x) = \\log(x)\\) natural logarithm\n\nSquare and square root function are inverse of each other. Exponential and natural logarithm, too.\n\n\\(\\sqrt[2]{x}^2 = \\sqrt[2]{x^2} = x\\), \\(\\log(e^x) = e^{\\log(x)} = x\\)\n\nIdentity function graph as mirror axis.\n\n\n\n\n\n\n\n\n\n\n\n\\(e\\) is Euler’s number \\(2.71828\\dots\\). The natural logarithm is also often called \\(\\ln\\). The square root function is \\(\\mathbb{R}_{\\geq 0} \\to \\mathbb{R}\\), the logarithm \\(\\mathbb{R}_{>0} \\to \\mathbb{R}\\)."
  },
  {
    "objectID": "W04.html#shifts-and-scales",
    "href": "W04.html#shifts-and-scales",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Shifts and scales",
    "text": "Shifts and scales\nHow can we shift, stretch, or shrink a graph vertically and horizontally?\n\n\n\\(y\\)-shift\\(x\\)-shift\\(y\\)-scale\\(x\\)-scale\n\n\n\n\nAdd a constant to the function.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = x^3 - x^2 + a\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\n\n\n\n\n\n\n\n\n\n\n\n\nSubtract a constant from all \\(x\\) within the function definition.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = (x - a)^3 - (x - a)^2\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nAttention:\nShifting \\(a\\) units to the right needs subtracting \\(a\\)!\nYou can think of the coordinate system being shifted in direction \\(a\\) while the graph stays.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiply a constant to all \\(x\\) within the function definition.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = a(x^3 - x^2)\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nNegative numbers flip the graph around the \\(x\\)-axis.\n\n\n\n\n\n\n\n\n\n\n\n\nDivide all \\(x\\) within the function definition by a constant.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = (x/a)^3 - (x/a)^2\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nNegative numbers flip the graph around the \\(y\\)-axis.\nAttention: Stretching needs a division by \\(a\\)!\nYou can think of the coordinate system being stretched multiplicatively by \\(a\\) while the graph stays."
  },
  {
    "objectID": "W04.html#math-polynomials-and-exponentials",
    "href": "W04.html#math-polynomials-and-exponentials",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Math: Polynomials and exponentials",
    "text": "Math: Polynomials and exponentials\nA polynomial is a function which is composed of (many) addends of the form \\(ax^n\\) for different values of \\(a\\) and \\(n\\).\nIn an exponential the \\(x\\) appears in the exponent.\n\\(f(x) = x^3\\) vs. \\(f(x) = e^x\\)\n\nFor \\(x\\to\\infty\\), any exponential will finally “overtake” any polynomial."
  },
  {
    "objectID": "W04.html#rules-for-exponentiation",
    "href": "W04.html#rules-for-exponentiation",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Rules for exponentiation",
    "text": "Rules for exponentiation\n\n\n\\(x^0\\)\n\\(0^x\\)\n\\(0^0\\)\n\\((x\\cdot y)^a\\)\n\\(x^{-a}\\), \\(x^{-1}\\)\n\\(x^\\frac{a}{b}\\), \\(x^\\frac{1}{2}\\)\n\\((x^a)^b\\)\n\n\n\\(x^0 = 1\\)\n\n\n\\(0^x = 0\\) for \\(x\\neq 0\\)\n\n\n\\(0^0 = 1\\) (discontinuity in \\(0^x\\))\n\n\n\\((x\\cdot y)^a = x^a\\cdot x^b\\)\n\n\n\\(x^{-a} = \\frac{1}{x^a}\\), \\(x^{-1} = \\frac{1}{x}\\)\n\n\n\\(x^\\frac{a}{b} = \\sqrt[b]{x^a} = (\\sqrt[b]{x})^a,\\ x^\\frac{1}{2} = \\sqrt{x}\\)\n\n\n\\((x^a)^b = x^{a\\cdot b} = (x^b)^a \\neq x^{a^b} = x^{(a^b)}\\)\nExample: \\((4^3)^2 = 64^2 = 4096 \\qquad 4^{3^2} = 4^9 = 262144\\)"
  },
  {
    "objectID": "W04.html#more-rules-for-exponentiation",
    "href": "W04.html#more-rules-for-exponentiation",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "More rules for exponentiation",
    "text": "More rules for exponentiation\n\n\n\\(x^a\\cdot x^b\\)\n\n\n\\(x^a\\cdot x^b = x^{a+b}\\) Multiplication of powers (with same base \\(x\\)) becomes addition of exponents.\n\n\n\n\n\n\n\\((x+y)^a\\)\n\n\nNo “simple” form! For \\(a\\) integer use binomial expansion. \\((x+y)^2 = x^2 + 2xy + y^2\\)\n\\((x+y)^3 = x^3 + 3x^2y + 3xy^2 + y^3\\)\n\\((x+y)^n = \\sum_{k=0}^n {n \\choose k} x^{n-k}y^k\\)\n\n\n\n\n\nPascal’s triangle\n\n\n\n\n\nFrom wikipedia\n\n\n\nWe meet it again in Probability:\nA row represents a binomial distribution\nWhich tends to mimics the normal distribution more and more\nand is related to the central limit theorem"
  },
  {
    "objectID": "W04.html#logarithms",
    "href": "W04.html#logarithms",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Logarithms",
    "text": "Logarithms\nDefinition: A logarithm of \\(a\\) for some base \\(b\\) is the value of the exponent which brings \\(b\\) to \\(a\\): \\(\\log_b(a) = x\\) means that \\(b^x =a\\)\nMost common:\n\n\\(\\log_{10}\\) for logarithmic axes in plots\n\\(\\log_{e}\\) natural logarithm (also \\(\\log\\) or \\(\\ln\\))\n\n\n\n\n\\(\\log_{10}(100) =\\)\n\n\n\\(2\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(1) =\\)\n\n\n\\(0\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(6590) =\\)\n\n\n\\(3.818885\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(0.02) =\\)\n\n\n\\(-1.69897\\)"
  },
  {
    "objectID": "W04.html#rules-for-logarithms",
    "href": "W04.html#rules-for-logarithms",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Rules for logarithms",
    "text": "Rules for logarithms\nUsually only one base is used in the same context, because changing base is easy:\n\\(\\log_c(x) = \\frac{\\log_b(x)}{\\log_b(c)} = \\frac{\\log(x)}{\\log(c)}\\)\n\n\n\\(\\log(x\\cdot y)\\)\n\n\n\\(= \\log(x) + \\log(y)\\) Multiplication \\(\\to\\) addition.\n\n\n\n\n\n\n\\(\\log(x^y)\\)\n\n\n\\(= y\\cdot\\log(x)\\)\n\n\n\n\n\n\\(\\log(x+y)\\)\n\n\ncomplicated!\n\n\n\n\n\nAlso changing bases for powers is easy: \\(x^y = (e^{\\log(x)})^y = e^{y\\cdot\\log(x)}\\)"
  },
  {
    "objectID": "W04.html#input-to-output",
    "href": "W04.html#input-to-output",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Input \\(\\to\\) output",
    "text": "Input \\(\\to\\) output\n\n\nMetaphorically, a function is a machine or a blackbox that for each input yields an output.\nThe inputs of a function are also called arguments.\n\n\nDifference to math terminolgy:\nThe output need not be the same for the same input.\n\n\n\nPicture from wikipedia."
  },
  {
    "objectID": "W04.html#function-as-objects-in-r",
    "href": "W04.html#function-as-objects-in-r",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Function as objects in R",
    "text": "Function as objects in R\nfunction is a class of an object in R\n\nclass(c)\n\n[1] \"function\"\n\nclass(ggplot2::ggplot)\n\n[1] \"function\"\n\n\nCalling the function without brackets writes its code or some information.\n\nsd # This function is written in R, and we see its code\n\nfunction (x, na.rm = FALSE) \nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n<bytecode: 0x561f52a8b5e0>\n<environment: namespace:stats>\n\nc # This function is not written in R but is a R primitive\n\nfunction (...)  .Primitive(\"c\")\n\nggplot2::ggplot # This function is not written solely in R\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \n{\n    UseMethod(\"ggplot\")\n}\n<bytecode: 0x561f4bab00f0>\n<environment: namespace:ggplot2>"
  },
  {
    "objectID": "W04.html#define-your-own-functions-in-r",
    "href": "W04.html#define-your-own-functions-in-r",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Define your own functions! (in R)",
    "text": "Define your own functions! (in R)\n\nadd_one <- function(x) {\n  x + 1 \n}\n# Test it\nadd_one(10)\n\n[1] 11\n\n\nThe skeleton for a function definition is\nfunction_name <- function(input){\n  # do something with the input(s)\n  # return something as output\n}\n\nfunction_name should be a short but evocative verb.\nThe input can be empty or one or more name or name=expression terms as arguments.\nThe last evaluated expression is returned as output.\nWhen the body or the function is only one line {} can be omitted. For example\nadd_one <- function(x) x + 1"
  },
  {
    "objectID": "W04.html#flexibility-of-inputs-and-outputs",
    "href": "W04.html#flexibility-of-inputs-and-outputs",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Flexibility of inputs and outputs",
    "text": "Flexibility of inputs and outputs\n\nArguments can be specified by name=expression or just expression (then they are taken as the next argument)\nDefault values for arguments can be provided. Useful when an argument is a parameter.\n\n\n\nmymult <- function(x = 2, y = 3) x * (y - 1)\nmymult(3,4)\n\n\n[1] 9\n\n\n\n\n\nmymult()\n\n\n[1] 4\n\n\n\n\n\nmymult(y = 3, x = 6)\n\n\n[1] 12\n\n\n\n\n\nmymult(5)\n\n\n[1] 10\n\n\n\n\n\nmymult(y = 2)\n\n\n[1] 2\n\n\n\n\nFor complex output use a list\n\n\nmymult <- function(x = 2, y = 3) \n  list(out1 = x * (y - 1), out2 = x * (y - 2))\nmymult()\n\n\n$out1\n[1] 4\n\n$out2\n[1] 2"
  },
  {
    "objectID": "W04.html#vectorized-functions",
    "href": "W04.html#vectorized-functions",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Vectorized functions",
    "text": "Vectorized functions\nMathematical functions in programming are often “vectorized”:\n\nOperations on a single value are applied to each component of the vector.\nOperations on two values are applied “component-wise” (for vectors of the same length)\n\n\nlog10(c(1,10,100,1000,10000))\n\n[1] 0 1 2 3 4\n\nc(1,1,2) + c(3,1,0)\n\n[1] 4 2 2\n\n(0:5)^2\n\n[1]  0  1  4  9 16 25"
  },
  {
    "objectID": "W04.html#recall-vector-creation-functions",
    "href": "W04.html#recall-vector-creation-functions",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Recall: Vector creation functions",
    "text": "Recall: Vector creation functions\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from=-0.5, to=1.5, by=0.1)\n\n [1] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9\n[16]  1.0  1.1  1.2  1.3  1.4  1.5\n\nseq(from=0, to=1, length.out=10)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000\n\nrep(1:3, times=3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\nrep(1:3, each=3)\n\n[1] 1 1 1 2 2 2 3 3 3"
  },
  {
    "objectID": "W04.html#plotting-and-transformation",
    "href": "W04.html#plotting-and-transformation",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Plotting and transformation",
    "text": "Plotting and transformation\nVector creation and vectorized functions are key for plotting and transformation.\n\nfunc <- function(x) x^3 - x^2    # Create a vectorized function\ndata <- tibble(x = seq(-0.5,1.5,by =0.01)) |>    # Vector creation\n    mutate(y = func(x))        # Vectorized transformation using the function\ndata |> ggplot(aes(x,y)) + geom_line() + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W04.html#conveniently-ggploting-functions",
    "href": "W04.html#conveniently-ggploting-functions",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Conveniently ggploting functions",
    "text": "Conveniently ggploting functions\n\nggplot() +\n geom_function(fun = log) +\n geom_function(fun = function(x) 3*x - 4, color = \"red\") +\n theme_minimal(base_size = 20)\n\n\n\n\n\n\nLine 3 shows another important concept: anonymous functions. The function function(x) 3*x - 4 is defined on the fly without a name."
  },
  {
    "objectID": "W04.html#conditional-statements",
    "href": "W04.html#conditional-statements",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Conditional statements",
    "text": "Conditional statements\n\nif executes a code block if a condition is TRUE\nelse executes a code block if the condition is FALSE\n\nSkeleton\nif (condition) {\n  # code block\n} else {\n  # code block\n}\nExample: A piece-wise defined function\n\n\n\npiecewise <- function(x) {\n  if (x < 2) {\n    0.5 * x\n  } else {\n    x - 1\n  }\n}\n\n\n\npiecewise(1)\n\n[1] 0.5\n\npiecewise(2)\n\n[1] 1\n\npiecewise(3)\n\n[1] 2\n\n\n\n\n\nProblem: piecewise is not vectorized. piecewise(c(1,2,3)) does not work."
  },
  {
    "objectID": "W04.html#vectorized-operations-with-map",
    "href": "W04.html#vectorized-operations-with-map",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Vectorized operations with map",
    "text": "Vectorized operations with map\n\nmap functions apply a function to each element of a vector.1\nmap(.x, .f, ...) applies the function .f to each element of the vector of .x and returns a list.\nmap_dbl returns a double vector (other variants exist)\n\n\n\n\nmap(c(1,2,3), piecewise) \n\n[[1]]\n[1] 0.5\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 2\n\nmap_dbl(c(1,2,3), piecewise) \n\n[1] 0.5 1.0 2.0\n\npiecewise_vectorized <- \n function(x) map_dbl(x, piecewise) \n\n\n\npiecewise_vectorized(seq(0,3,by = 0.5))\n\n[1] 0.00 0.25 0.50 0.75 1.00 1.50 2.00\n\ntibble(x = seq(0,3,by = 0.5)) |> \n  mutate(y = piecewise_vectorized(x)) |> \n  ggplot(aes(x,y)) + geom_line() + theme_minimal(base_size = 20)\n\n\n\n\n\n\nIn tidyverse they are provided in the package purrr"
  },
  {
    "objectID": "W04.html#map-and-reduce",
    "href": "W04.html#map-and-reduce",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "map and reduce",
    "text": "map and reduce\nInstead of a list or a vector reduce returns a single value.\nTo that end it needs a function with two arguments. It applies it to the first two elements of the vector, then to the result and the third element, then the result and the fourth element, and so on.\n\n1:10 |> reduce(\\(x,y) x + y)\n\n[1] 55\n\n\nNote: \\(x) is a short way to write an anonymous function as function(x).\n\nExample: Reading multiple files\n\n\nInstead of\na <-read_csv(\"a.csv\")\nb <-read_csv(\"b.csv\")\nc <-read_csv(\"c.csv\")\nd <-read_csv(\"d.csv\")\ne <-read_csv(\"e.csv\")\nf <-read_csv(\"f.csv\")\ng <-read_csv(\"g.csv\")\n\nbind_rows(a,b,c,d,e,f,g)\n\nWrite\nletter[1:7] |> \n map(\\(x) read_csv(paste0(x,\".csv\"))) |> \n reduce(bind_rows)"
  },
  {
    "objectID": "W04.html#function-programming-take-away",
    "href": "W04.html#function-programming-take-away",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Function programming: Take away",
    "text": "Function programming: Take away\n\nFunctions are the most important building blocks of programming.\nFunctions can and often should be vectorized.\nVectorized functions are the basis for plotting and transformation.\nmap functions are powerful tools for iterative tasks!\nExpect to not get the idea first but to love them later."
  },
  {
    "objectID": "W04.html#descriptive-vs.-inferential-statistics",
    "href": "W04.html#descriptive-vs.-inferential-statistics",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Descriptive vs. Inferential Statistics",
    "text": "Descriptive vs. Inferential Statistics\n\nThe process of using and analyzing summary statistics\n\nSolely concerned with properties of the observed data.\n\nDistinct from inferential statistics:\n\nInference of properties of an underlying distribution given sampled observations from a larger population.\n\n\nSummary Statistics are used to summarize a set of observations to communicate the largest amount of information as simple as possible."
  },
  {
    "objectID": "W04.html#summary-statistics",
    "href": "W04.html#summary-statistics",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Summary statistics",
    "text": "Summary statistics\nUnivariate (for one variable)\n\nMeasures of location, or central tendency\nMeasures of statistical dispersion\nMeasure of the shape of the distribution like skewness or kurtosis\n\nBivariate (for two variables)\n\nMeasures of statistical dependence or correlation"
  },
  {
    "objectID": "W04.html#measures-of-central-tendency-1",
    "href": "W04.html#measures-of-central-tendency-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nGoal: For a sequence of numerical observations \\(x_1,\\dots,x_n\\) we want to measure\n\nthe “typical” value.\na value summarizing the location of values on the numerical axis.\n\nThree different ways:\n\nArithmetic mean (also mean, average): Sum of the all observations divided by the number of observations \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\)\nMedian: Assume \\(x_1 \\leq x_2 \\leq\\dots\\leq x_n\\). Then the median is middlemost values in the sequence \\(x_\\frac{n+1}{2}\\) when \\(n\\) odd. For \\(n\\) even there are two middlemost values and the median is \\(\\frac{x_\\frac{n}{2} + x_\\frac{n+1}{2}}{2}\\)\nMode: The value that appears most often in the sequence."
  },
  {
    "objectID": "W04.html#philosophy-of-aggregation",
    "href": "W04.html#philosophy-of-aggregation",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Philosophy of aggregation",
    "text": "Philosophy of aggregation\n\n\nThe mean represents total value per value.\nExample: per capita income in a town is the total income per individual\nThe median represents the value such that half of the values are lower and higher.\nIn a democracy where each value is represented by one voter preferring it, the median is the value which is unbeatable by an absolute majority. Half of the people prefer higher the other half lower values. (Median voter model)\nThe mode represents the most common value.\nIn a democracy, the mode represents the winner of a plurality vote where each value runs as a candidate and the winner is the one with the most votes."
  },
  {
    "objectID": "W04.html#mean-median-mode-properties",
    "href": "W04.html#mean-median-mode-properties",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Mean, Median, Mode properties",
    "text": "Mean, Median, Mode properties\nDo they deliver one unambiguous answer for any sequence?\n\nMean and median, yes.\nThe mode has no rules for a tie.\n\n\nCan they by generalized to variables with ordered or even unordered categories?\n\n\nMean: No.\nMedian: For ordered categories (except when even number and the two middlemost are not the same) Mode: For any categorical variable.\n\n\nIs the measure always also in the data sequence?\n\n\nMean: No.\nMedian: Yes, for sequences of odd length.\nMode: Yes."
  },
  {
    "objectID": "W04.html#generalized-means",
    "href": "W04.html#generalized-means",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Generalized means1",
    "text": "Generalized means1\nFor \\(x_1, \\dots, x_n > 0\\) and \\(p\\in \\mathbb{R}_{\\neq 0}\\) the generalized mean is\n\\[M_p(x_1, \\dots, x_n) = (\\frac{1}{n}\\sum_{i=1}^n x_i^p)^\\frac{1}{p}\\]\nFor \\(p = 0\\) it is \\(M_0(x_1, \\dots, x_n) = (\\prod_{i=1}^n x_i)^\\frac{1}{n}\\).\n\\(M_1\\) is the arithmetic mean. \\(M_0\\) is called the geometric mean. \\(M_{-1}\\) the harmonic mean.\nNote: Generalized means are often only reasonable when all values are positive \\(x_i > 0\\).\n\n\n\\(M_0\\) can also be expressed as the exponential (\\(\\exp(x) = e^x\\)) of the mean of the the \\(\\log\\)’s of the \\(x_i\\)’s: \\(\\exp(\\log((\\prod_{i=1}^n x_i)^\\frac{1}{n})) = \\exp(\\frac{1}{n}\\sum_{i=1}^n\\log(x_i))\\).\nAlso called power mean or \\(p\\)-mean."
  },
  {
    "objectID": "W04.html#box-cox-transformation-function",
    "href": "W04.html#box-cox-transformation-function",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Box-Cox transformation function1",
    "text": "Box-Cox transformation function1\nFor \\(p \\in \\mathbb{R}\\): \\(f(x) = \\begin{cases}\\frac{x^p - 1}{p} & \\text{for $p\\neq 0$} \\\\ \\log(x) & \\text{for $p= 0$}\\end{cases}\\)\n\n\nThe \\(p\\)-mean is\n\\[M_p(x) = f^{-1}(\\frac{1}{n}\\sum_{i=1}^n f(x_i))\\]\nwith \\(x = [x_1, \\dots, x_n]\\). \\(f^{-1}\\) is the inverse2 of \\(f\\).\n\n\n\n\n\n\n\n\nAlso a common transformation to pre-process data to make it closer to a normal distribution.That means \\(f^-1(f(x)) = x =f(f^-1(x))\\)."
  },
  {
    "objectID": "W04.html#application-the-wisdom-of-the-crowd",
    "href": "W04.html#application-the-wisdom-of-the-crowd",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Application: The Wisdom of the Crowd",
    "text": "Application: The Wisdom of the Crowd\n\n\n\nThe collective opinion of a diverse group of independent individuals rather than that of a single expert.\nThe classical wisdom-of-the-crowds finding is about point estimation of a continuous quantity.\nPopularized by James Surowiecki (2004).\nThe opening anecdote is about Francis Galton’s1 surprise in 1907 that the crowd at a county fair accurately guessed the weight of an ox’s meat when their individual guesses were averaged.\n\n\n\n\n\n\nGalton (1822-1911) was a half-cousin to Charles Darwin and one of the founding fathers of statistics. He also was a scientific racist, see https://twitter.com/kareem_carr/status/1575506343401775104?s=20&t=8T5TzrayAWNShmOSzJgCJQ.."
  },
  {
    "objectID": "W04.html#galtons-data",
    "href": "W04.html#galtons-data",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Galton’s data1",
    "text": "Galton’s data1\nWhat is the weight of the meat of this ox?\n\n\n\n\nlibrary(readxl)\ngalton <- read_excel(\"data/galton_data.xlsx\")\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\") + geom_vline(xintercept = median(galton$Estimate), color = \"blue\") + geom_vline(xintercept = Mode(galton$Estimate), color = \"purple\")\n\n\n\n\n787 estimates, true value 1198, mean 1196.7, median 1208, mode 1218\nKenneth Wallis dug out the data from Galton’s notebook and put it here https://warwick.ac.uk/fac/soc/economics/staff/kfwallis/publications/galton_data.xlsx"
  },
  {
    "objectID": "W04.html#viertelfest-bremen-2008",
    "href": "W04.html#viertelfest-bremen-2008",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Viertelfest Bremen 20081",
    "text": "Viertelfest Bremen 20081\nHow many lots will be sold by the end of the festival?\n\nviertel <- read_csv(\"data/Viertelfest.csv\")\nviertel |> ggplot(aes(`Schätzung`)) + geom_histogram() + geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\")\n\n\n\n\n1226 estimates, the maximal value is 29530000!\nWe should filter out the highest values for the histogram…\nData collected as additional guessing game at the Lottery “Haste mal ’nen Euro?”, data provided by Jan Lorenz https://docs.google.com/spreadsheets/d/1HiYhUrYrsbeybJ10mwsae_hQCawZlUQFOOZzcugXzgA/edit#gid=0"
  },
  {
    "objectID": "W04.html#viertelfest-bremen-2008-1",
    "href": "W04.html#viertelfest-bremen-2008-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Viertelfest Bremen 2008",
    "text": "Viertelfest Bremen 2008\nHow many lots will be sold by the end of the festival?\n\nviertel <- read_csv(\"data/Viertelfest.csv\")\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\") + geom_vline(xintercept = exp(mean(log(viertel$Schätzung))), color = \"orange\")\n\n\n1226 estimates, true value 10788, mean 53163.9, median 9843, mode 10000,\ngeometric mean 10510.1"
  },
  {
    "objectID": "W04.html#log_10-transformation-viertelfest",
    "href": "W04.html#log_10-transformation-viertelfest",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "\\(\\log_{10}\\) transformation Viertelfest",
    "text": "\\(\\log_{10}\\) transformation Viertelfest\n\nviertel |> mutate(log10Est = log10(Schätzung)) |> ggplot(aes(log10Est)) + geom_histogram(binwidth = 0.05) + geom_vline(xintercept = log10(10788), color = \"green\") + \n geom_vline(xintercept = log10(mean(viertel$Schätzung)), color = \"red\") + geom_vline(xintercept = log10(median(viertel$Schätzung)), color = \"blue\") + geom_vline(xintercept = log10(Mode(viertel$Schätzung)), color = \"purple\") + geom_vline(xintercept = mean(log10(viertel$Schätzung)), color = \"orange\")\n\n\n1226 estimates, true value 10788, mean 53163.9, median 9843, mode 10000,\ngeometric mean 10510.1"
  },
  {
    "objectID": "W04.html#wisdom-of-the-crowd-insights",
    "href": "W04.html#wisdom-of-the-crowd-insights",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Wisdom of the crowd insights",
    "text": "Wisdom of the crowd insights\n\n\nIn Galton’s sample the different measures do not make a big difference\nIn the Viertelfest data the arithmetic mean performs very bad!\nThe mean is vulnerable to extreme values.\nQuoting Galton on the mean as a democratic aggregation function:\n“The mean gives voting power to the cranks in proportion to their crankiness.”\nThe mode tends to be on focal values as round numbers (10,000). In Galton’s data this is not so pronounced beause estimators used several units which Galton had to convert.\nHow to choose a measure to aggregate the wisdom?\n\nBy the nature of the estimate problem? Is the scale mostly clear? (Are we in the hundreds, thousands, ten thousands, …)\nBy the nature of the distribution?\nThere is no real insurance against a systematic bias in the population."
  },
  {
    "objectID": "W04.html#measures-of-dispersion-1",
    "href": "W04.html#measures-of-dispersion-1",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Measures of dispersion1",
    "text": "Measures of dispersion1\nGoal: We want to measure\n\nHow spread out values are around the central tendency.\nHow stretched or squeezed is the distribution?\n\nVariance is the mean of the squared deviation from the mean: \\(\\text{Var}(x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2\\) where \\(\\mu\\) (mu) is the mean.\nStandard deviation is the square root of the variance \\(\\text{SD}(x) = \\sqrt{\\text{Var}(x)}\\).\nThe standard deviation is often denoted \\(\\sigma\\) (sigma) and the variance \\(\\sigma^2\\).\nMean absolute deviation (MAD) is the mean of the absolute deviation from the mean: \\(\\text{MAD}(x) = \\frac{1}{n}\\sum_{i=1}^n|x_i - \\mu|\\).\nRange is the difference of the maximal and the minimal value \\(\\max(x) - \\min(x)\\).\nAlso called variability, scatter, or spread."
  },
  {
    "objectID": "W04.html#examples-of-measures-of-dispersion",
    "href": "W04.html#examples-of-measures-of-dispersion",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Examples of measures of dispersion",
    "text": "Examples of measures of dispersion\n\n\n\nvar(galton$Estimate)\n\n[1] 5415.013\n\nsd(galton$Estimate)\n\n[1] 73.58677\n\nmad(galton$Estimate)\n\n[1] 51.891\n\nrange(galton$Estimate)\n\n[1]  896 1516\n\ndiff(range(galton$Estimate))\n\n[1] 620\n\n\n\n\nvar(viertel$Schätzung)\n\n[1] 719774887849\n\nsd(viertel$Schätzung)\n\n[1] 848395.5\n\nmad(viertel$Schätzung)\n\n[1] 8771.803\n\nrange(viertel$Schätzung)\n\n[1]      120 29530000\n\ndiff(range(viertel$Schätzung))\n\n[1] 29529880\n\n\n\n\n\n\nVariance (and standard deviation) in statistics is usually computed with \\(\\frac{1}{n-1}\\) instead of \\(\\frac{1}{n}\\) to provide an unbiased estimator of the potentially underlying population variance. We omit more detail here."
  },
  {
    "objectID": "W04.html#standardization",
    "href": "W04.html#standardization",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Standardization",
    "text": "Standardization\nVariables are standardized by subtracting their mean and then dividing by their standard deviations.\nA value from a standardized variable is called a standard score or z-score.\n\\(z_i = \\frac{x_i - \\mu}{\\sigma}\\)\nwhere \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation of the vector \\(x\\).\n\nThis is a shift-scale transformation. We shift by the mean and scale by the standard deviation.\nA standard score \\(z_i\\) shows how mean standard deviations \\(x_i\\) is away from the mean of \\(x\\)."
  },
  {
    "objectID": "W04.html#achievements-and-next-steps",
    "href": "W04.html#achievements-and-next-steps",
    "title": "W#04: Math refresh, Function Programming, Descriptive Statistics",
    "section": "Achievements and next steps",
    "text": "Achievements and next steps\n\nWe have learned about the data science process\nYou made essential steps in data visualization and data wrangling with New York City Flights in the Homework\nYou can write and render reproducible reports\nWe had some math refreshment\nWe learned some data science data and programming concepts in R and in Python. Reconsider them in later homework!\n\nNext steps coming (you will receive individual repositories for this):\n\nHomework mimicking data science projects\nSome exploratory data analysis in a sandbox\nThinking about your own data science project (in groups of 2-3)\n\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "",
    "text": "Developing Syllabus\n\n\n\n\n\nThis syllabus may change during the course of the semester with some additions and clarifications.\nAll substantial changes will marked here with date.\nImportant Links:\nGitHub organization of the course: https://github.com/CU-F23-MDSSB-01-Concepts-Tools\nOrganization repository with non-public information and FAQ in Discussions: https://github.com/CU-F23-MDSSB-01-Concepts-Tools/Organization"
  },
  {
    "objectID": "index.html#module-names",
    "href": "index.html#module-names",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "1 Module Names",
    "text": "1 Module Names\nThis syllabus is for two modules which require each other.\n\nMDSSSB-DSOC-02 Data Science Concepts (Core area, 5 credits)\nMDSSSB-MET-01 Data Science Tools (Methods area, 5 credits)\n\nOffered in the Master program: Data Science for Society and Business (DSSB). See the module description in the DSSB Handbook."
  },
  {
    "objectID": "index.html#module-components",
    "href": "index.html#module-components",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "2 Module Components",
    "text": "2 Module Components\n\nData Science Concepts (5 Credits)\nData Science Tools in R (2.5 credits)\nData Science Tools in python (2.5 credits)\n\nAll are courses offered in the Fall term and have no entry requirement.\nData Science Concepts and Data Science Tools are co-requirements.\nThe Concepts treated in the lectures are applied in exercises and homework in the Tools course."
  },
  {
    "objectID": "index.html#class-meeting-information",
    "href": "index.html#class-meeting-information",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "3 Class Meeting Information",
    "text": "3 Class Meeting Information\nData Science Concepts:\nMonday 09:45-11:00 and 11:15-12:30\nLocation: East Hall Room 1 in-person\nVideo streaming in Teams provided for late-arriving students: Team F23_MDSSB-DSOC-02_Data Science Concepts, General Channel\nData Science Tools:\nThursday 09:45-11:00 and 11:15-12:30\nLocation: East Hall Room 2 in-person\nVideo streaming in Teams provided for late-arriving students: Teams F23_MDSSB-MET-01-A_Data Science Tools in R and F23_MDSSB-MET-01-B_Data Science Tools in python, General Channel\nNote: The R and python courses are provided in the same time slot. Details in the schedule."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "4 Instructors",
    "text": "4 Instructors\nConcepts:\nJan Lorenz Email: jlorenz@constructor.univeristy\nTools in R:\nArmin Müller Email: armmueller@constructor.university\nTools in python:\nPeter Steiglechner Email: psteiglechner@constructor.university"
  },
  {
    "objectID": "index.html#format-and-workload",
    "href": "index.html#format-and-workload",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "5 Format and Workload",
    "text": "5 Format and Workload\nConcepts: Lectures, sometimes with aspects of Tutorials (35 hours in presence, total expected workload 125 hours)\nTools: Tutorials, sometimes with aspects of Lectures (35 hours in presence, total expected workload 125 hours)\nBesides homework, both modules require a decent amount of self-study.\nWorkload homework and self-study is expect to be 90/125 = 72% of the total workload.\nHomework assignments are given along the concepts treated in Concepts to be solved with the tools treated in Tools."
  },
  {
    "objectID": "index.html#intended-learning-outcomes",
    "href": "index.html#intended-learning-outcomes",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "6 Intended Learning Outcomes",
    "text": "6 Intended Learning Outcomes\nThe module descriptions can be found in the DSSB Handbook\n\n6.1 From the handbook\nBy the end of the Concepts module, you will be able to:\n\nunderstand and use the mathematical foundations of statistical learning algorithms\nexplain and classify data science problems\nexplain and classify data-driven approaches\nunderstand the application of data science techniques to typical situations and tasks in business and societal research, including the search, retrieval, preparation, and statistical analysis of data\ninterpret complexity analysis and performance evaluation of data science problems and algorithms\n\nBy the end of the Tools module, you will be able to:\n\nexplain basic concepts of imperative and object-oriented programming\nwrite, test, and debug programs\nperform data handling and data manipulation tasks in R and Python\napply your knowledge to implement own functions in R and Python\neffectively use core packages and libraries of R and Python for data analysis\nknow about the typical applications of R and Python in data science\nimplement and apply advanced data mining methods with appropriate tools\nperform a full cycle of data analysis\n\n\n\n6.2 Main Learning Goal\nOur main goal to help you build a good basis for your more and more independent work in the whole study program. That means you can\n\nlearn core concepts in data science on your own, for example\n\nconcepts to explore data (import, wrangle, visualize)\nlearn and explore mathematics and statistics through the data science lens\nlearn concepts to model and draw conclusions from data (model, infer, predict)\n\ncreate and maintain a digital working environment on your computer to do data science\nlearn to program in the data science languages R and python, and become able to learn new skills in these independently\ndo a data science project of your own interest"
  },
  {
    "objectID": "index.html#examination-and-assessment",
    "href": "index.html#examination-and-assessment",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "7 Examination and Assessment",
    "text": "7 Examination and Assessment\n\n7.1 Concepts Module\nAssessment Type: Written Examination\nDuration: 120 min\nWeight: 100%\nScope: All intended learning outcomes of the module.\nCompletion: to pass this module, the examination has to be passed with at least 45%\nAn exam will take place after all lectures in December. The date will be published by the university adminstration later.\nThere are no additional achievements necessary and there are no bonus options.\n\n\n7.2 Tools Module\nModule achievement: 50% of the assignments correctly solved\nProgramming and analysis assignments will appear step by step during the courses.\nTo pass the module you have to solve half of them by the end of the semester.\nAssessment Type: Project Report\nLength: 4000 - 5000 words Weight: 100% Scope: All intended learning outcomes of the module.\nMore information of project formats and a rubric for the grading will be provided later.\nBonus option: Students receive 0.33 points grade improvements on their project grade (on the numerical grade as specified here https://constructor.university/sites/default/files/2023-02/Grading_Table_2023.pdf) when all assignments are solved by the end of the semester. (Note, the bonus is not necessary to reach the best grade in the module.)\nAll assignments and the final project must be delivered in personalized repositories in the GitHub organization https://github.com/CU-F23-MDSSB-01-Concepts-Tools."
  },
  {
    "objectID": "index.html#module-policies",
    "href": "index.html#module-policies",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "8 Module Policies",
    "text": "8 Module Policies\nThere are no formal requirement about attendance and active participation. However, we rely on your engagement in a many-facted way including:\n\nPreparation (looking at readings and material before and after class, being informed about syllabus and course material)\nFocus (avoid distraction during in class and self-learning activities)\nPresence (listening and responding during group activities)\nAsking questions (in class, out of class, online, offline, when you get stuck conclude by writing a question)\nSpecificity (being as specific as possible when describing your problem or question)\nSynthesizing (making connections between concepts from reading and discussion)\nPersistence (you don’t need to understand everything immediately, but stay engaged, try again, confusion shows that you pay attention)"
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "9 Academic Integrity",
    "text": "9 Academic Integrity\nAll involved parties (professors and lecturers, instructors and students) are expected to abide by the word and spirit of the “Code of Academic Integrity”: https://constructor.university/student-life/student-services/university-policies/academic-policies/code-of-academic-integrity. Violations of the Code might be brought to the attention of the Academic Integrity Committee."
  },
  {
    "objectID": "index.html#artifical-intelligence-ai-use-policy",
    "href": "index.html#artifical-intelligence-ai-use-policy",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "10 Artifical Intelligence (AI) Use Policy",
    "text": "10 Artifical Intelligence (AI) Use Policy\nThis policy covers any generative AI tool, such as ChatGPT, Elicit, etc. This includes text, code, slides, artwork/graphics/video/audio and other products.\nWe instructors encouraged to using and exploring AI tools for these purposes:\n\nLearning by dialog with a chatbot. AI chatbots can be very helpful to explain you concepts on your desired level and get a feeling about how certain topics are treated. You can ask for an easier or more detailed explanation or focus on certain aspects. Note: The capabilities are limited and you likely receive a lot of false information! Using chatbots should remain a small part of your learning process. Rule of thumb: Spend not more than 25% of the learing time with chatting. There is no way around reading textbooks, reading software documentation, learning and understanding concepts, searching for help online, asking instructors or fellow students.\nHave code snippets written. Tools like GitHub Copilot (free to use for students and teachers in VSCode) are heavily used and currently revolutionize code writting a bit. They can speed up writung your code. However, they do not deliver always correct solutions. There is no way around understanding yourself what a code is doing! Do not spend endless hours asking for new code with new prompts, spend time understanding a language and the functions and objects you are using! Copilots can be a great help to get a skeleton of code and an idea how your solution might look, they rarely deliver the complete code, expect that the code does not work, expect that the code seems to works but the results are wrong. You are 100% accountable for the code you produce, with or without the help of a copilot!\nHave a draft text snippet written. Data science is also about formulating and describing research questions, describing data, documenting code, interpreting results, and deriving conclusion form the results. This is all verbal text and chatbots are good in writing text which often appears as well readable. You can use this inspire you and to polish and improve your texts. However, you are 100% accountable for the text you deliver. You are expected to know what your text is about and to be able to answer questions about what your text means! Text generated purely by chatbots, it often becomes evident that you do not. We consider such cases worse than incomplete but sensible text. Also text written by chatbots is often very generic and less specific. In general, we value more specific text higher than generic text. Large parts of very generic text is considered worse than a shorter more specific text. In extreme case, we may a long very generic text will be considered worse than no text at all\nNote: Philosophical and legal questions around the training and use of chatbots and code copilots are controversially contested and re-examined constantly! We encourage to engage with such questions and become aware of arguments and debates.\n\nIf any part of this AI policy is confusing or uncertain, please reach out to us for a conversation before submitting your work."
  },
  {
    "objectID": "index.html#schedule-and-homework",
    "href": "index.html#schedule-and-homework",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "11 Schedule and Homework",
    "text": "11 Schedule and Homework\nThe Schedule is on an extra page and will be updated continuously with\n\nLinks to slides\nNote on what homework you are expected to do\nSome questions which you should be able to answer after each week. Test yourself.\n\nHomework page will successively appear as extra pages. See the sidebar to the left."
  },
  {
    "objectID": "index.html#feedback-from-students",
    "href": "index.html#feedback-from-students",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "12 Feedback from Students",
    "text": "12 Feedback from Students\nWe are eager to constantly improve the quality of our teaching. We would be glad to obtain your feedback at any time of the course to improve your learning experience."
  },
  {
    "objectID": "W03.html#readr-and-readxl",
    "href": "W03.html#readr-and-readxl",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "readr and readxl",
    "text": "readr and readxl\n\n\n\n\nread_csv() - comma delimited files\nread_csv2() - semicolon delimited files (common where “,” is used as decimal place)\nread_tsv() - tab delimited files\nread_delim() - reads in files with any delimiter\n…\n\n\n\n\nread_excel() read xls or xlsx files from MS Excel\n…\n\n\n\n\n\nThere are also packages to write data from R to excel files (writexl, openxlsx, xlsx, …)."
  },
  {
    "objectID": "W03.html#other-data-formats",
    "href": "W03.html#other-data-formats",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Other data formats",
    "text": "Other data formats\nR packages, analog libraries will exist for python\n\ngooglesheets4: Google Sheets\nhaven: SPSS, Stata, and SAS files\nDBI, along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc): allows you to run SQL queries against a database and return a data frame\njsonlite: JSON\nxml2: xml\nrvest: web scraping\nhttr: web APIs\n…"
  },
  {
    "objectID": "W03.html#comma-separated-values-csv",
    "href": "W03.html#comma-separated-values-csv",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Comma-separated values (CSV)",
    "text": "Comma-separated values (CSV)\nWe use CSV file when there is no certain reason to do otherwise.1\nCSV files are delimited text file\n\nCan be viewed with any text editor\nShow each row of the data frame in a line\nSeparates the content of columns by commas (or the delimiter character)\nEach cell could be surrounded by quotes (when long text with commas (!) is in cells)\nThe first line is interpreted as listing the variable names by default\n\nreadr tries to guess the data type of variables\nYou can also customize it yourself!\nPotential reasons are: CSV being not provided, or the dataset being very larger and hard-disk storage is an issue. Other formats or more space efficient."
  },
  {
    "objectID": "W03.html#data-import-workflow",
    "href": "W03.html#data-import-workflow",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Data import workflow",
    "text": "Data import workflow\n\nYou download your CSV file to the data/ directory. You may use download.file() for this, but make sure you do not download large amounts of data each time you render your file!\nRead the data with data <- read_csv(\"data/FILENAME.csv\") and read the report in the console.\nExplore if you are happy and iterate by customizing the data import line using specifications until the data is as you want it to be.\n\nGood practices to document the data download:\n\nOne or low number of files: Put the download line(s) in you main document, but comment out # after usage.\nWrite a script (data-download.r) to document the download commands.\nMake your code check first if the file already exist, like this if (!(file.exists(\"DATA_FILE.csv\"))) {DOWNLOAD-CODE}"
  },
  {
    "objectID": "W03.html#download-2.-read",
    "href": "W03.html#download-2.-read",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "1. Download, 2. Read",
    "text": "1. Download, 2. Read\nThis downloads data only if the file does not exist. Then it loads it.\n\nif (!file.exists(\"data/hotels.csv\")) {\n  download.file(url = \"https://raw.githubusercontent.com/rstudio-education/datascience-box/main/course-materials/_slides/u2-d06-grammar-wrangle/data/hotels.csv\", \n                destfile = \"data/hotels.csv\")\n}\nhotels <- read_csv(\"data/hotels.csv\")\n\nRows: 119390 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): hotel, arrival_date_month, meal, country, market_segment, distrib...\ndbl  (18): is_canceled, lead_time, arrival_date_year, arrival_date_week_numb...\ndate  (1): reservation_status_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOutput is a summary how read_csv guessed the data types of columns."
  },
  {
    "objectID": "W03.html#explore-using-spec",
    "href": "W03.html#explore-using-spec",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "3. Explore using spec()",
    "text": "3. Explore using spec()\nAll details to check or customize:\n\nspec(hotels)\n\ncols(\n  hotel = col_character(),\n  is_canceled = col_double(),\n  lead_time = col_double(),\n  arrival_date_year = col_double(),\n  arrival_date_month = col_character(),\n  arrival_date_week_number = col_double(),\n  arrival_date_day_of_month = col_double(),\n  stays_in_weekend_nights = col_double(),\n  stays_in_week_nights = col_double(),\n  adults = col_double(),\n  children = col_double(),\n  babies = col_double(),\n  meal = col_character(),\n  country = col_character(),\n  market_segment = col_character(),\n  distribution_channel = col_character(),\n  is_repeated_guest = col_double(),\n  previous_cancellations = col_double(),\n  previous_bookings_not_canceled = col_double(),\n  reserved_room_type = col_character(),\n  assigned_room_type = col_character(),\n  booking_changes = col_double(),\n  deposit_type = col_character(),\n  agent = col_character(),\n  company = col_character(),\n  days_in_waiting_list = col_double(),\n  customer_type = col_character(),\n  adr = col_double(),\n  required_car_parking_spaces = col_double(),\n  total_of_special_requests = col_double(),\n  reservation_status = col_character(),\n  reservation_status_date = col_date(format = \"\")\n)"
  },
  {
    "objectID": "W03.html#finalize-data-import-option-1",
    "href": "W03.html#finalize-data-import-option-1",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Finalize data import, option 1",
    "text": "Finalize data import, option 1\nWhen\n\nall columns are how they should\nyou consider it not necessary to document the specifications\n\nThen use show_col_types = FALSE to quiet the reading message.\n\nhotels <- read_csv(\"data/hotels.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "W03.html#finalize-data-import-option-2",
    "href": "W03.html#finalize-data-import-option-2",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Finalize data import, option 2",
    "text": "Finalize data import, option 2\n\nCopy the spec(hotels) output into the col_types argument\nIf necessary, customize it\n\n\nhotels <- read_csv(\"data/hotels.csv\", col_types = cols(\n  hotel = col_character(),\n  is_canceled = col_logical(),\n  lead_time = col_integer(),\n  arrival_date_year = col_integer(),\n  arrival_date_month = col_character(),\n  arrival_date_week_number = col_integer(),\n  arrival_date_day_of_month = col_integer(),\n  stays_in_weekend_nights = col_integer(),\n  stays_in_week_nights = col_integer(),\n  adults = col_integer(),\n  children = col_integer(),\n  babies = col_integer(),\n  meal = col_character(),\n  country = col_character(),\n  market_segment = col_character(),\n  distribution_channel = col_character(),\n  is_repeated_guest = col_logical(),\n  previous_cancellations = col_integer(),\n  previous_bookings_not_canceled = col_integer(),\n  reserved_room_type = col_character(),\n  assigned_room_type = col_character(),\n  booking_changes = col_integer(),\n  deposit_type = col_character(),\n  agent = col_integer(),\n  company = col_integer(),\n  days_in_waiting_list = col_integer(),\n  customer_type = col_character(),\n  adr = col_double(),\n  required_car_parking_spaces = col_integer(),\n  total_of_special_requests = col_integer(),\n  reservation_status = col_character(),\n  reservation_status_date = col_date(format = \"\")\n))"
  },
  {
    "objectID": "W03.html#columns-types",
    "href": "W03.html#columns-types",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Columns types",
    "text": "Columns types\n\n\n\ntype function\ndata type\n\n\n\n\ncol_character()\ncharacter\n\n\ncol_date()\ndate\n\n\ncol_datetime()\nPOSIXct (date-time)\n\n\ncol_double()\ndouble (numeric)\n\n\ncol_factor()\nfactor\n\n\ncol_guess()\nlet readr guess (default)\n\n\ncol_integer()\ninteger\n\n\ncol_logical()\nlogical\n\n\ncol_number()\nnumbers mixed with non-number characters\n\n\ncol_skip()\ndo not read\n\n\ncol_time()\ntime"
  },
  {
    "objectID": "W03.html#hotels-data",
    "href": "W03.html#hotels-data",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Hotels data",
    "text": "Hotels data\n\nData from two hotels: one resort and one city hotel\nObservations: Each row represents a hotel booking"
  },
  {
    "objectID": "W03.html#first-look-on-data",
    "href": "W03.html#first-look-on-data",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "First look on data",
    "text": "First look on data\nType the name of the data frame\n\nhotels\n\n# A tibble: 119,390 × 32\n   hotel        is_canceled lead_time arrival_date_year arrival_date_month\n   <chr>        <lgl>           <int>             <int> <chr>             \n 1 Resort Hotel FALSE             342              2015 July              \n 2 Resort Hotel FALSE             737              2015 July              \n 3 Resort Hotel FALSE               7              2015 July              \n 4 Resort Hotel FALSE              13              2015 July              \n 5 Resort Hotel FALSE              14              2015 July              \n 6 Resort Hotel FALSE              14              2015 July              \n 7 Resort Hotel FALSE               0              2015 July              \n 8 Resort Hotel FALSE               9              2015 July              \n 9 Resort Hotel TRUE               85              2015 July              \n10 Resort Hotel TRUE               75              2015 July              \n# ℹ 119,380 more rows\n# ℹ 27 more variables: arrival_date_week_number <int>,\n#   arrival_date_day_of_month <int>, stays_in_weekend_nights <int>,\n#   stays_in_week_nights <int>, adults <int>, children <int>, babies <int>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <lgl>,\n#   previous_cancellations <int>, previous_bookings_not_canceled <int>, …"
  },
  {
    "objectID": "W03.html#look-on-variable-names",
    "href": "W03.html#look-on-variable-names",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Look on variable names",
    "text": "Look on variable names\n\nnames(hotels)\n\n [1] \"hotel\"                          \"is_canceled\"                   \n [3] \"lead_time\"                      \"arrival_date_year\"             \n [5] \"arrival_date_month\"             \"arrival_date_week_number\"      \n [7] \"arrival_date_day_of_month\"      \"stays_in_weekend_nights\"       \n [9] \"stays_in_week_nights\"           \"adults\"                        \n[11] \"children\"                       \"babies\"                        \n[13] \"meal\"                           \"country\"                       \n[15] \"market_segment\"                 \"distribution_channel\"          \n[17] \"is_repeated_guest\"              \"previous_cancellations\"        \n[19] \"previous_bookings_not_canceled\" \"reserved_room_type\"            \n[21] \"assigned_room_type\"             \"booking_changes\"               \n[23] \"deposit_type\"                   \"agent\"                         \n[25] \"company\"                        \"days_in_waiting_list\"          \n[27] \"customer_type\"                  \"adr\"                           \n[29] \"required_car_parking_spaces\"    \"total_of_special_requests\"     \n[31] \"reservation_status\"             \"reservation_status_date\""
  },
  {
    "objectID": "W03.html#second-look-with-glimpse",
    "href": "W03.html#second-look-with-glimpse",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Second look with glimpse",
    "text": "Second look with glimpse\n\nglimpse(hotels)\n\nRows: 119,390\nColumns: 32\n$ hotel                          <chr> \"Resort Hotel\", \"Resort Hotel\", \"Resort…\n$ is_canceled                    <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ lead_time                      <int> 342, 737, 7, 13, 14, 14, 0, 9, 85, 75, …\n$ arrival_date_year              <int> 2015, 2015, 2015, 2015, 2015, 2015, 201…\n$ arrival_date_month             <chr> \"July\", \"July\", \"July\", \"July\", \"July\",…\n$ arrival_date_week_number       <int> 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ arrival_date_day_of_month      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ stays_in_weekend_nights        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ stays_in_week_nights           <int> 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, …\n$ adults                         <int> 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ children                       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ babies                         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ meal                           <chr> \"BB\", \"BB\", \"BB\", \"BB\", \"BB\", \"BB\", \"BB…\n$ country                        <chr> \"PRT\", \"PRT\", \"GBR\", \"GBR\", \"GBR\", \"GBR…\n$ market_segment                 <chr> \"Direct\", \"Direct\", \"Direct\", \"Corporat…\n$ distribution_channel           <chr> \"Direct\", \"Direct\", \"Direct\", \"Corporat…\n$ is_repeated_guest              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ previous_cancellations         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_bookings_not_canceled <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ reserved_room_type             <chr> \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"C\", \"C\",…\n$ assigned_room_type             <chr> \"C\", \"C\", \"C\", \"A\", \"A\", \"A\", \"C\", \"C\",…\n$ booking_changes                <int> 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ deposit_type                   <chr> \"No Deposit\", \"No Deposit\", \"No Deposit…\n$ agent                          <int> NA, NA, NA, 304, 240, 240, NA, 303, 240…\n$ company                        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ days_in_waiting_list           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ customer_type                  <chr> \"Transient\", \"Transient\", \"Transient\", …\n$ adr                            <dbl> 0.00, 0.00, 75.00, 75.00, 98.00, 98.00,…\n$ required_car_parking_spaces    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_of_special_requests      <int> 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 3, …\n$ reservation_status             <chr> \"Check-Out\", \"Check-Out\", \"Check-Out\", …\n$ reservation_status_date        <date> 2015-07-01, 2015-07-01, 2015-07-02, 20…\n\n\nNow, comes the data wrangling, transformation, …"
  },
  {
    "objectID": "W03.html#grammar-of-data-wrangling",
    "href": "W03.html#grammar-of-data-wrangling",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Grammar of Data Wrangling",
    "text": "Grammar of Data Wrangling\n\n\n\n\n\nGrammar of data wrangling: Start with a dataset and pipe it through several manipulations with |>\nmpg |> \n  filter(cyl == 8) |> \n  select(manufacturer, hwy) |> \n  group_by(manufacturer) |> \n  summarize(mean_hwy = mean(hwy))\n\n\nSimilar in python: Make a chain using . to apply pandas methods for data frames one after the other.\nSimilar in ggplot2: Creating a ggplot object, then add graphical layers (geom_ functions) with + (instead of a pipe)\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = trans)) + \n  geom_point() + \n  geom_smooth()"
  },
  {
    "objectID": "W03.html#what-is-the-pipe",
    "href": "W03.html#what-is-the-pipe",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "What is the pipe |>?",
    "text": "What is the pipe |>?\nx |> f(a,b) is the same as f(x,a,b)\nThe outcome of a command is put into the first argument of the next function call. Practice it it see that it is exactly identical!\nReasons for using pipes:\n\nstructure the sequence of your data operations from left to right\navoid nested function calls:\nnested: filter(select(hotels, hotel, adults), adults == 2)\npiped: hotels |> select(hotel, adults) |> filter(adults == 2)\n(base R: hotels[hotels$adults == 2, c(\"hotel\", \"adults\")])\nYou’ll minimize the need for local variables and function definitions\nYou’ll make it easy to add steps anywhere in the sequence of operations\n\n\n\nSince R 4.1.0, the pipe is part of base R. Before you had to load the magrittr package and use %>%. You still find it in a lot of code out in the wild. It is almost the same."
  },
  {
    "objectID": "W03.html#dplyr-uses-verbs-to-manipulate",
    "href": "W03.html#dplyr-uses-verbs-to-manipulate",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "dplyr uses verbs to manipulate",
    "text": "dplyr uses verbs to manipulate\n\nselect: pick columns by name\narrange: reorder rows\nslice: pick rows using index(es)\nfilter: pick rows matching criteria\ndistinct: filter for unique rows\nmutate: add new variables\nsummarise: reduce variables to values\ngroup_by: for grouped operations\n… (many more)"
  },
  {
    "objectID": "W03.html#select-a-single-column",
    "href": "W03.html#select-a-single-column",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "select a single column",
    "text": "select a single column\n\n\n\nhotels |> select(lead_time)     \n\n\n\n# A tibble: 119,390 × 1\n   lead_time\n       <int>\n 1       342\n 2       737\n 3         7\n 4        13\n 5        14\n 6        14\n 7         0\n 8         9\n 9        85\n10        75\n# ℹ 119,380 more rows\n\n\nNote: select(hotels, lead_time) is identical.\n\nWhy does piping |> work?\nEvery dplyr function\n\ntakes a data frame (tibble) as first argument\noutputs a (manipulated) data frame (tibble)\n\n\n\n\n\nIn hotel business, lead time is the time betweeen booking and arrival."
  },
  {
    "objectID": "W03.html#select-more-columns",
    "href": "W03.html#select-more-columns",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Select more columns",
    "text": "Select more columns\n\nhotels |> select(hotel, lead_time)     \n\n\n\n# A tibble: 119,390 × 2\n   hotel        lead_time\n   <chr>            <int>\n 1 Resort Hotel       342\n 2 Resort Hotel       737\n 3 Resort Hotel         7\n 4 Resort Hotel        13\n 5 Resort Hotel        14\n 6 Resort Hotel        14\n 7 Resort Hotel         0\n 8 Resort Hotel         9\n 9 Resort Hotel        85\n10 Resort Hotel        75\n# ℹ 119,380 more rows\n\n\nNote that hotel is a variable, but hotels the data frame object name"
  },
  {
    "objectID": "W03.html#select-helper-starts_with",
    "href": "W03.html#select-helper-starts_with",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Select helper starts_with",
    "text": "Select helper starts_with\n\nhotels |> select(starts_with(\"arrival\"))\n\n\n\n# A tibble: 119,390 × 4\n   arrival_date_year arrival_date_month arrival_date_week_number\n               <int> <chr>                                 <int>\n 1              2015 July                                     27\n 2              2015 July                                     27\n 3              2015 July                                     27\n 4              2015 July                                     27\n 5              2015 July                                     27\n 6              2015 July                                     27\n 7              2015 July                                     27\n 8              2015 July                                     27\n 9              2015 July                                     27\n10              2015 July                                     27\n# ℹ 119,380 more rows\n# ℹ 1 more variable: arrival_date_day_of_month <int>"
  },
  {
    "objectID": "W03.html#bring-columns-to-the-front",
    "href": "W03.html#bring-columns-to-the-front",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Bring columns to the front",
    "text": "Bring columns to the front\n\nhotels |> select(hotel, market_segment, children, everything())\n\n\n\n# A tibble: 119,390 × 32\n   hotel        market_segment children is_canceled lead_time arrival_date_year\n   <chr>        <chr>             <int> <lgl>           <int>             <int>\n 1 Resort Hotel Direct                0 FALSE             342              2015\n 2 Resort Hotel Direct                0 FALSE             737              2015\n 3 Resort Hotel Direct                0 FALSE               7              2015\n 4 Resort Hotel Corporate             0 FALSE              13              2015\n 5 Resort Hotel Online TA             0 FALSE              14              2015\n 6 Resort Hotel Online TA             0 FALSE              14              2015\n 7 Resort Hotel Direct                0 FALSE               0              2015\n 8 Resort Hotel Direct                0 FALSE               9              2015\n 9 Resort Hotel Online TA             0 TRUE               85              2015\n10 Resort Hotel Offline TA/TO         0 TRUE               75              2015\n# ℹ 119,380 more rows\n# ℹ 26 more variables: arrival_date_month <chr>,\n#   arrival_date_week_number <int>, arrival_date_day_of_month <int>,\n#   stays_in_weekend_nights <int>, stays_in_week_nights <int>, adults <int>,\n#   babies <int>, meal <chr>, country <chr>, distribution_channel <chr>,\n#   is_repeated_guest <lgl>, previous_cancellations <int>,\n#   previous_bookings_not_canceled <int>, reserved_room_type <chr>, …"
  },
  {
    "objectID": "W03.html#more-select-helpers",
    "href": "W03.html#more-select-helpers",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "More select helpers",
    "text": "More select helpers\n\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\ncontains(): Contains a literal string\nnum_range(): Matches a numerical range like x01, x02, x03\none_of(): Matches variable names in a character vector\neverything(): Matches all variables\nlast_col(): Select last variable, possibly with an offset\nmatches(): Matches a regular expression (a sequence of symbols/characters expressing a string/pattern to be searched for within text)\n\n\n\nCheck details with ?one_of"
  },
  {
    "objectID": "W03.html#slice-for-certain-rows",
    "href": "W03.html#slice-for-certain-rows",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "slice for certain rows",
    "text": "slice for certain rows\n\nhotels |> slice(2:4)\n\n\n\n# A tibble: 3 × 32\n  hotel        is_canceled lead_time arrival_date_year arrival_date_month\n  <chr>        <lgl>           <int>             <int> <chr>             \n1 Resort Hotel FALSE             737              2015 July              \n2 Resort Hotel FALSE               7              2015 July              \n3 Resort Hotel FALSE              13              2015 July              \n# ℹ 27 more variables: arrival_date_week_number <int>,\n#   arrival_date_day_of_month <int>, stays_in_weekend_nights <int>,\n#   stays_in_week_nights <int>, adults <int>, children <int>, babies <int>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <lgl>,\n#   previous_cancellations <int>, previous_bookings_not_canceled <int>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, …"
  },
  {
    "objectID": "W03.html#filter-for-rows-with-certain-criteria",
    "href": "W03.html#filter-for-rows-with-certain-criteria",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "filter for rows with certain criteria",
    "text": "filter for rows with certain criteria\n\nhotels |> filter(hotel == \"City Hotel\")\n\n\n\n# A tibble: 79,330 × 32\n   hotel      is_canceled lead_time arrival_date_year arrival_date_month\n   <chr>      <lgl>           <int>             <int> <chr>             \n 1 City Hotel FALSE               6              2015 July              \n 2 City Hotel TRUE               88              2015 July              \n 3 City Hotel TRUE               65              2015 July              \n 4 City Hotel TRUE               92              2015 July              \n 5 City Hotel TRUE              100              2015 July              \n 6 City Hotel TRUE               79              2015 July              \n 7 City Hotel FALSE               3              2015 July              \n 8 City Hotel TRUE               63              2015 July              \n 9 City Hotel TRUE               62              2015 July              \n10 City Hotel TRUE               62              2015 July              \n# ℹ 79,320 more rows\n# ℹ 27 more variables: arrival_date_week_number <int>,\n#   arrival_date_day_of_month <int>, stays_in_weekend_nights <int>,\n#   stays_in_week_nights <int>, adults <int>, children <int>, babies <int>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <lgl>,\n#   previous_cancellations <int>, previous_bookings_not_canceled <int>, …"
  },
  {
    "objectID": "W03.html#filter-for-multiple-criteria",
    "href": "W03.html#filter-for-multiple-criteria",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "filter for multiple criteria",
    "text": "filter for multiple criteria\n\nhotels |> filter(\n  babies >= 1,\n  children >= 1, \n  ) |> \n  select(hotel, adults, babies, children)\n\n\n\n# A tibble: 175 × 4\n   hotel        adults babies children\n   <chr>         <int>  <int>    <int>\n 1 Resort Hotel      2      1        1\n 2 Resort Hotel      2      1        1\n 3 Resort Hotel      2      1        1\n 4 Resort Hotel      2      1        1\n 5 Resort Hotel      2      1        1\n 6 Resort Hotel      2      1        1\n 7 Resort Hotel      2      1        1\n 8 Resort Hotel      2      1        2\n 9 Resort Hotel      2      1        2\n10 Resort Hotel      1      1        2\n# ℹ 165 more rows\n\n\nComma-separated conditions are interpreted as all these should be fulfilled.\nThis is identical to the logical AND &.\nhotels |> filter(babies >= 1 & children >= 1)"
  },
  {
    "objectID": "W03.html#filter-for-complexer-criteria",
    "href": "W03.html#filter-for-complexer-criteria",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "filter for complexer criteria",
    "text": "filter for complexer criteria\n\nhotels |> filter(\n  babies >= 1 | children >= 1\n  ) |> \n  select(hotel, adults, babies, children)\n\n\n\n# A tibble: 9,332 × 4\n   hotel        adults babies children\n   <chr>         <int>  <int>    <int>\n 1 Resort Hotel      2      0        1\n 2 Resort Hotel      2      0        2\n 3 Resort Hotel      2      0        2\n 4 Resort Hotel      2      0        2\n 5 Resort Hotel      2      0        1\n 6 Resort Hotel      2      0        1\n 7 Resort Hotel      1      0        2\n 8 Resort Hotel      2      0        2\n 9 Resort Hotel      2      1        0\n10 Resort Hotel      2      1        0\n# ℹ 9,322 more rows\n\n\n| is the logical OR. Only one criterion needs to be fulfilled."
  },
  {
    "objectID": "W03.html#logical-operators",
    "href": "W03.html#logical-operators",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Logical operators1",
    "text": "Logical operators1\n\n\n\noperator\ndefinition\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\nx & y\nx AND y\n\n\nx | y\nx OR y\n\n\nis.na(x)\ntest if x is NA (missing data)\n\n\n!is.na(x)\ntest if x is not NA (not missing data)\n\n\nx %in% y\ntest if x is in y (often used for strings)\n\n\n!(x %in% y)\ntest if x is not in y\n\n\n!x\nnot x\n\n\n\nLogical is sometimes called Boolean"
  },
  {
    "objectID": "W03.html#excursions-the-concept-of-indexing",
    "href": "W03.html#excursions-the-concept-of-indexing",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Excursions: The Concept of Indexing",
    "text": "Excursions: The Concept of Indexing\nSelect and filter can also be achieved by indexing.\nIn (base) R as well as in python.\nSelect ranges of rows and columns\n\nhotels[1:3,5:7]\n\n\n\n# A tibble: 3 × 3\n  arrival_date_month arrival_date_week_number arrival_date_day_of_month\n  <chr>                                 <int>                     <int>\n1 July                                     27                         1\n2 July                                     27                         1\n3 July                                     27                         1\n\n\nYou can use any vector (with non-overshooting indexes)\n\nhotels[c(1:3,100232),c(5:7,1)]\n\n\n\n# A tibble: 4 × 4\n  arrival_date_month arrival_date_week_number arrival_date_day_of_month hotel   \n  <chr>                                 <int>                     <int> <chr>   \n1 July                                     27                         1 Resort …\n2 July                                     27                         1 Resort …\n3 July                                     27                         1 Resort …\n4 October                                  44                        23 City Ho…"
  },
  {
    "objectID": "W03.html#python-is-0-indexed-r-is-1-indexed",
    "href": "W03.html#python-is-0-indexed-r-is-1-indexed",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "python is 0-indexed, R is 1-indexed!",
    "text": "python is 0-indexed, R is 1-indexed!\npython: indexes go from 0 to n-1\nR: indexes go from 1 to n\nBe aware!\nNote: There is no correct way. For some use cases one is more natural for others the other.\nAnalogy: In mathematics there is an unsettled debate if \\(0 \\in \\mathbb{N}\\) or \\(0 \\notin \\mathbb{N}\\)"
  },
  {
    "objectID": "W03.html#excursion-the-concept-of-logical-indexing",
    "href": "W03.html#excursion-the-concept-of-logical-indexing",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Excursion: The Concept of Logical Indexing",
    "text": "Excursion: The Concept of Logical Indexing\nWith logical vectors you can select rows and columns\n\n\n\n\n\ndata <- tibble(x = LETTERS[1:5], y = letters[6:10])\ndata\n\n\n# A tibble: 5 × 2\n  x     y    \n  <chr> <chr>\n1 A     f    \n2 B     g    \n3 C     h    \n4 D     i    \n5 E     j    \n\n\n\n\n\n\ndata[c(TRUE,FALSE,TRUE,FALSE,TRUE),c(TRUE,FALSE)]\n\n\n# A tibble: 3 × 1\n  x    \n  <chr>\n1 A    \n2 C    \n3 E"
  },
  {
    "objectID": "W03.html#logical-vectors-from-conditional-statements",
    "href": "W03.html#logical-vectors-from-conditional-statements",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Logical vectors from conditional statements",
    "text": "Logical vectors from conditional statements\n\n\ndata$x\n\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\n\n\n\n\n\ndata$x %in% c(\"C\",\"E\")\n\n\n[1] FALSE FALSE  TRUE FALSE  TRUE\n\n\n\n\n\n\n\ndata[data$x %in% c(\"C\",\"E\"),]\n\n\n# A tibble: 2 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 E     j    \n\n\n\n\n\n\n\ndata[data$x %in% c(\"C\",\"E\") | \n       data$y %in% c(\"h\",\"i\"),]\n\n\n# A tibble: 3 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 D     i    \n3 E     j    \n\n\n\n\n\n\n\ndata |> \n  filter(\n    x %in% c(\"C\",\"E\") | y %in% c(\"h\",\"i\")\n    )\n\n\n# A tibble: 3 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 D     i    \n3 E     j"
  },
  {
    "objectID": "W03.html#unique-combinations-arranging",
    "href": "W03.html#unique-combinations-arranging",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Unique combinations, arranging",
    "text": "Unique combinations, arranging\ndistinct and arrange\n\nhotels |> \n  distinct(hotel, market_segment) |> \n  arrange(hotel, market_segment)\n\n\n\n# A tibble: 14 × 2\n   hotel        market_segment\n   <chr>        <chr>         \n 1 City Hotel   Aviation      \n 2 City Hotel   Complementary \n 3 City Hotel   Corporate     \n 4 City Hotel   Direct        \n 5 City Hotel   Groups        \n 6 City Hotel   Offline TA/TO \n 7 City Hotel   Online TA     \n 8 City Hotel   Undefined     \n 9 Resort Hotel Complementary \n10 Resort Hotel Corporate     \n11 Resort Hotel Direct        \n12 Resort Hotel Groups        \n13 Resort Hotel Offline TA/TO \n14 Resort Hotel Online TA"
  },
  {
    "objectID": "W03.html#counting",
    "href": "W03.html#counting",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Counting",
    "text": "Counting\ncount\n\nhotels |> \n  count(hotel, market_segment) |>      # This produces a new variable n\n  arrange(n)\n\n\n\n# A tibble: 14 × 3\n   hotel        market_segment     n\n   <chr>        <chr>          <int>\n 1 City Hotel   Undefined          2\n 2 Resort Hotel Complementary    201\n 3 City Hotel   Aviation         237\n 4 City Hotel   Complementary    542\n 5 Resort Hotel Corporate       2309\n 6 City Hotel   Corporate       2986\n 7 Resort Hotel Groups          5836\n 8 City Hotel   Direct          6093\n 9 Resort Hotel Direct          6513\n10 Resort Hotel Offline TA/TO   7472\n11 City Hotel   Groups         13975\n12 City Hotel   Offline TA/TO  16747\n13 Resort Hotel Online TA      17729\n14 City Hotel   Online TA      38748"
  },
  {
    "objectID": "W03.html#create-a-new-variable-with-mutate",
    "href": "W03.html#create-a-new-variable-with-mutate",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Create a new variable with mutate",
    "text": "Create a new variable with mutate\n\nhotels |>\n  mutate(little_ones = children + babies) |>\n  select(children, babies, little_ones) |>\n  arrange(desc(little_ones)) # This sorts in descending order. See the big things!\n\n\n\n# A tibble: 119,390 × 3\n   children babies little_ones\n      <int>  <int>       <int>\n 1       10      0          10\n 2        0     10          10\n 3        0      9           9\n 4        2      1           3\n 5        2      1           3\n 6        2      1           3\n 7        3      0           3\n 8        2      1           3\n 9        2      1           3\n10        3      0           3\n# ℹ 119,380 more rows"
  },
  {
    "objectID": "W03.html#more-mutating",
    "href": "W03.html#more-mutating",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "More mutating",
    "text": "More mutating\n\nhotels |>\n  mutate(little_ones = children + babies) |>\n  count(hotel, little_ones) |>\n  mutate(prop = n / sum(n))\n\n\n\n# A tibble: 12 × 4\n   hotel        little_ones     n       prop\n   <chr>              <int> <int>      <dbl>\n 1 City Hotel             0 73923 0.619     \n 2 City Hotel             1  3263 0.0273    \n 3 City Hotel             2  2056 0.0172    \n 4 City Hotel             3    82 0.000687  \n 5 City Hotel             9     1 0.00000838\n 6 City Hotel            10     1 0.00000838\n 7 City Hotel            NA     4 0.0000335 \n 8 Resort Hotel           0 36131 0.303     \n 9 Resort Hotel           1  2183 0.0183    \n10 Resort Hotel           2  1716 0.0144    \n11 Resort Hotel           3    29 0.000243  \n12 Resort Hotel          10     1 0.00000838"
  },
  {
    "objectID": "W03.html#summarizing",
    "href": "W03.html#summarizing",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Summarizing",
    "text": "Summarizing\n\nhotels |>\n  summarize(mean_adr = mean(adr))\n\n\n\n# A tibble: 1 × 1\n  mean_adr\n     <dbl>\n1     102.\n\n\n\nThat shrinks the data frame to one row!\nDon’t forget to name the new variable (here mean_adr)\nYou can use any function you can apply to a vector!\n(Sometimes you may need to write your own one.)\n\n\n\nIn hoteling, ADR is the average daily rate, the average daily rental income per paid occupied room. A performce indicator."
  },
  {
    "objectID": "W03.html#grouped-operations",
    "href": "W03.html#grouped-operations",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Grouped operations",
    "text": "Grouped operations\n\nhotels |>\n  group_by(hotel) |>\n  summarise(mean_adr = mean(adr))\n\n\n\n# A tibble: 2 × 2\n  hotel        mean_adr\n  <chr>           <dbl>\n1 City Hotel      105. \n2 Resort Hotel     95.0\n\n\nLook at the grouping attributes:\n\nhotels |>\n  group_by(hotel)\n\n\n\n# A tibble: 119,390 × 32\n# Groups:   hotel [2]\n   hotel        is_canceled lead_time arrival_date_year arrival_date_month\n   <chr>        <lgl>           <int>             <int> <chr>             \n 1 Resort Hotel FALSE             342              2015 July              \n 2 Resort Hotel FALSE             737              2015 July              \n 3 Resort Hotel FALSE               7              2015 July              \n 4 Resort Hotel FALSE              13              2015 July              \n 5 Resort Hotel FALSE              14              2015 July              \n 6 Resort Hotel FALSE              14              2015 July              \n 7 Resort Hotel FALSE               0              2015 July              \n 8 Resort Hotel FALSE               9              2015 July              \n 9 Resort Hotel TRUE               85              2015 July              \n10 Resort Hotel TRUE               75              2015 July              \n# ℹ 119,380 more rows\n# ℹ 27 more variables: arrival_date_week_number <int>,\n#   arrival_date_day_of_month <int>, stays_in_weekend_nights <int>,\n#   stays_in_week_nights <int>, adults <int>, children <int>, babies <int>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <lgl>,\n#   previous_cancellations <int>, previous_bookings_not_canceled <int>, …"
  },
  {
    "objectID": "W03.html#grouping-summarizing-visualizing",
    "href": "W03.html#grouping-summarizing-visualizing",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Grouping, summarizing, visualizing",
    "text": "Grouping, summarizing, visualizing\n\nhotels |>\n  group_by(hotel, arrival_date_week_number) |>\n  summarise(mean_adr = mean(adr)) |> \n  ggplot(aes(x = arrival_date_week_number, y = mean_adr, color = hotel)) +\n  geom_line()"
  },
  {
    "objectID": "W03.html#resources",
    "href": "W03.html#resources",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Resources",
    "text": "Resources\n\nFor systemic understanding: Learning resources linked in the syllabus\n\nR for Data Science\nPython Data Science Handbook\n\nFor quick overview to get inspiration\n\nCheatsheets (find some in RStudio -> Help, others by google)\n\nggplot2 Cheatsheet\ndplyr Cheatsheet\n\n\nFor detailed help with a function\n\nHelp file of the function ?FUNCTION-NAME, or search box in Help tab\nReference page on the package webpage\n\nTalk to ChatGPT? Does it work?"
  },
  {
    "objectID": "W03.html#named-vectors",
    "href": "W03.html#named-vectors",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Named vectors",
    "text": "Named vectors\nAll types of vectors can be named upon creation\n\nc(Num1 = 4, Second = 7, Last = 8)\n\n\n\n  Num1 Second   Last \n     4      7      8 \n\n\n\nor names can be set afterward.\n\nx <- 1:4\ny <- set_names(x, c(\"a\",\"b\",\"c\",\"d\"))\ny\n\n\n\na b c d \n1 2 3 4 \n\n\n\n\nNamed vectors can be used for subsetting.\n\ny[c(\"b\",\"d\")]\n\n\n\nb d \n2 4"
  },
  {
    "objectID": "W03.html#reminder-indexing-and-vectorized-thinking",
    "href": "W03.html#reminder-indexing-and-vectorized-thinking",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Reminder: Indexing and vectorized thinking",
    "text": "Reminder: Indexing and vectorized thinking\n\nx <- set_names(1:10,LETTERS[1:10])\nx\n\n\n\n A  B  C  D  E  F  G  H  I  J \n 1  2  3  4  5  6  7  8  9 10 \n\n\n\n\nx[c(4,2,1,1,1,1,4,1,5)]\n\n\n\nD B A A A A D A E \n4 2 1 1 1 1 4 1 5 \n\n\n\n\nRemoving with negative index numbers.\n\nx[c(-3,-5,-2)]\n\n\n\n A  D  F  G  H  I  J \n 1  4  6  7  8  9 10 \n\n\n\n\nMixing does not work.\nx[c(-3,1)]  # Will throw an error"
  },
  {
    "objectID": "W03.html#r-objects-can-have-attributes",
    "href": "W03.html#r-objects-can-have-attributes",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "R objects can have attributes",
    "text": "R objects can have attributes\nIn a named vector, the names are an attribute.\n\nx\n\n A  B  C  D  E  F  G  H  I  J \n 1  2  3  4  5  6  7  8  9 10 \n\nattributes(x)\n\n$names\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\n\n\nAttributes can be assigned freely.\n\nattr(x, \"SayHi\") <- \"Hi\"\nattr(x, \"SayBye\") <- \"Bye\"\nattributes(x)\n\n\n\n$names\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\n$SayHi\n[1] \"Hi\"\n\n$SayBye\n[1] \"Bye\""
  },
  {
    "objectID": "W03.html#attributes-in-data-structures",
    "href": "W03.html#attributes-in-data-structures",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Attributes in data structures",
    "text": "Attributes in data structures\n\nlibrary(nycflights13)\nattributes(airports)\n\n\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n   [1]    1    2    3    4    5    6    7    8    9   10   11   12   13   14\n  [15]   15   16   17   18   19   20   21   22   23   24   25   26   27   28\n  [29]   29   30   31   32   33   34   35   36   37   38   39   40   41   42\n  [43]   43   44   45   46   47   48   49   50   51   52   53   54   55   56\n  [57]   57   58   59   60   61   62   63   64   65   66   67   68   69   70\n  [71]   71   72   73   74   75   76   77   78   79   80   81   82   83   84\n  [85]   85   86   87   88   89   90   91   92   93   94   95   96   97   98\n  [99]   99  100  101  102  103  104  105  106  107  108  109  110  111  112\n [113]  113  114  115  116  117  118  119  120  121  122  123  124  125  126\n [127]  127  128  129  130  131  132  133  134  135  136  137  138  139  140\n [141]  141  142  143  144  145  146  147  148  149  150  151  152  153  154\n [155]  155  156  157  158  159  160  161  162  163  164  165  166  167  168\n [169]  169  170  171  172  173  174  175  176  177  178  179  180  181  182\n [183]  183  184  185  186  187  188  189  190  191  192  193  194  195  196\n [197]  197  198  199  200  201  202  203  204  205  206  207  208  209  210\n [211]  211  212  213  214  215  216  217  218  219  220  221  222  223  224\n [225]  225  226  227  228  229  230  231  232  233  234  235  236  237  238\n [239]  239  240  241  242  243  244  245  246  247  248  249  250  251  252\n [253]  253  254  255  256  257  258  259  260  261  262  263  264  265  266\n [267]  267  268  269  270  271  272  273  274  275  276  277  278  279  280\n [281]  281  282  283  284  285  286  287  288  289  290  291  292  293  294\n [295]  295  296  297  298  299  300  301  302  303  304  305  306  307  308\n [309]  309  310  311  312  313  314  315  316  317  318  319  320  321  322\n [323]  323  324  325  326  327  328  329  330  331  332  333  334  335  336\n [337]  337  338  339  340  341  342  343  344  345  346  347  348  349  350\n [351]  351  352  353  354  355  356  357  358  359  360  361  362  363  364\n [365]  365  366  367  368  369  370  371  372  373  374  375  376  377  378\n [379]  379  380  381  382  383  384  385  386  387  388  389  390  391  392\n [393]  393  394  395  396  397  398  399  400  401  402  403  404  405  406\n [407]  407  408  409  410  411  412  413  414  415  416  417  418  419  420\n [421]  421  422  423  424  425  426  427  428  429  430  431  432  433  434\n [435]  435  436  437  438  439  440  441  442  443  444  445  446  447  448\n [449]  449  450  451  452  453  454  455  456  457  458  459  460  461  462\n [463]  463  464  465  466  467  468  469  470  471  472  473  474  475  476\n [477]  477  478  479  480  481  482  483  484  485  486  487  488  489  490\n [491]  491  492  493  494  495  496  497  498  499  500  501  502  503  504\n [505]  505  506  507  508  509  510  511  512  513  514  515  516  517  518\n [519]  519  520  521  522  523  524  525  526  527  528  529  530  531  532\n [533]  533  534  535  536  537  538  539  540  541  542  543  544  545  546\n [547]  547  548  549  550  551  552  553  554  555  556  557  558  559  560\n [561]  561  562  563  564  565  566  567  568  569  570  571  572  573  574\n [575]  575  576  577  578  579  580  581  582  583  584  585  586  587  588\n [589]  589  590  591  592  593  594  595  596  597  598  599  600  601  602\n [603]  603  604  605  606  607  608  609  610  611  612  613  614  615  616\n [617]  617  618  619  620  621  622  623  624  625  626  627  628  629  630\n [631]  631  632  633  634  635  636  637  638  639  640  641  642  643  644\n [645]  645  646  647  648  649  650  651  652  653  654  655  656  657  658\n [659]  659  660  661  662  663  664  665  666  667  668  669  670  671  672\n [673]  673  674  675  676  677  678  679  680  681  682  683  684  685  686\n [687]  687  688  689  690  691  692  693  694  695  696  697  698  699  700\n [701]  701  702  703  704  705  706  707  708  709  710  711  712  713  714\n [715]  715  716  717  718  719  720  721  722  723  724  725  726  727  728\n [729]  729  730  731  732  733  734  735  736  737  738  739  740  741  742\n [743]  743  744  745  746  747  748  749  750  751  752  753  754  755  756\n [757]  757  758  759  760  761  762  763  764  765  766  767  768  769  770\n [771]  771  772  773  774  775  776  777  778  779  780  781  782  783  784\n [785]  785  786  787  788  789  790  791  792  793  794  795  796  797  798\n [799]  799  800  801  802  803  804  805  806  807  808  809  810  811  812\n [813]  813  814  815  816  817  818  819  820  821  822  823  824  825  826\n [827]  827  828  829  830  831  832  833  834  835  836  837  838  839  840\n [841]  841  842  843  844  845  846  847  848  849  850  851  852  853  854\n [855]  855  856  857  858  859  860  861  862  863  864  865  866  867  868\n [869]  869  870  871  872  873  874  875  876  877  878  879  880  881  882\n [883]  883  884  885  886  887  888  889  890  891  892  893  894  895  896\n [897]  897  898  899  900  901  902  903  904  905  906  907  908  909  910\n [911]  911  912  913  914  915  916  917  918  919  920  921  922  923  924\n [925]  925  926  927  928  929  930  931  932  933  934  935  936  937  938\n [939]  939  940  941  942  943  944  945  946  947  948  949  950  951  952\n [953]  953  954  955  956  957  958  959  960  961  962  963  964  965  966\n [967]  967  968  969  970  971  972  973  974  975  976  977  978  979  980\n [981]  981  982  983  984  985  986  987  988  989  990  991  992  993  994\n [995]  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008\n[1009] 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022\n[1023] 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036\n[1037] 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050\n[1051] 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064\n[1065] 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078\n[1079] 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092\n[1093] 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106\n[1107] 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120\n[1121] 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134\n[1135] 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148\n[1149] 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162\n[1163] 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176\n[1177] 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190\n[1191] 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204\n[1205] 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218\n[1219] 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232\n[1233] 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246\n[1247] 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260\n[1261] 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274\n[1275] 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288\n[1289] 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302\n[1303] 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316\n[1317] 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330\n[1331] 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344\n[1345] 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358\n[1359] 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372\n[1373] 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386\n[1387] 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400\n[1401] 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414\n[1415] 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428\n[1429] 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442\n[1443] 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456\n[1457] 1457 1458\n\n$spec\ncols(\n  id = col_double(),\n  name = col_character(),\n  city = col_character(),\n  country = col_character(),\n  faa = col_character(),\n  icao = col_character(),\n  lat = col_double(),\n  lon = col_double(),\n  alt = col_double(),\n  tz = col_double(),\n  dst = col_character(),\n  tzone = col_character()\n)\n\n$names\n[1] \"faa\"   \"name\"  \"lat\"   \"lon\"   \"alt\"   \"tz\"    \"dst\"   \"tzone\""
  },
  {
    "objectID": "W03.html#three-important-attributes",
    "href": "W03.html#three-important-attributes",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Three important attributes",
    "text": "Three important attributes\n\nNames are used to name element of a vector, also works for lists and therefore also data frames (lists of atomic vectors of the same length)\nDimensions (dim()) is a short numeric vector making a vector behave as a matrix or a higher dimensional array. A vector 1:6 together with dim being c(2,3) is a matrix with 2 rows and 3 columns\n\\(\\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{bmatrix}\\)\nClass is used to implement the S3 object oriented system. We don’t need to know the details here. The class system makes it for example possible that the same function, e.g. print() behaves differently for objects of a different class.\n\nClass plays a role in specifying augmented vectors like factors, dates, date-times, or tibbles."
  },
  {
    "objectID": "W03.html#factors",
    "href": "W03.html#factors",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Factors",
    "text": "Factors\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values\n\nx <- factor(c(\"BS\", \"MS\", \"PhD\", \"MS\", \"BS\", \"BS\"))\nx\n\n\n\n[1] BS  MS  PhD MS  BS  BS \nLevels: BS MS PhD\n\n\n\nTechnically, a factor is vector of integers with a levels attribute which specifies the categories for the integers.\n\ntypeof(x)\n\n[1] \"integer\"\n\nas.integer(x)\n\n[1] 1 2 3 2 1 1\n\nattributes(x)\n\n$levels\n[1] \"BS\"  \"MS\"  \"PhD\"\n\n$class\n[1] \"factor\"\n\n\n\n\nThe class factor makes R print the level of each element of the vector instead of the underlying integer."
  },
  {
    "objectID": "W03.html#factors-for-data-visualization",
    "href": "W03.html#factors-for-data-visualization",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Factors for data visualization",
    "text": "Factors for data visualization\nWe manipulate factors with functions from the forcats package of the tidyverse core.\n\nPlotReverseOrder by frequencyRegroup\n\n\n\nmpg |> ggplot(aes(y = manufacturer)) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_rev(manufacturer))) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_rev(fct_infreq(manufacturer)))) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_other(manufacturer, keep = c(\"dodge\", \"toyota\", \"volkswagen\")))) + geom_bar()"
  },
  {
    "objectID": "W03.html#dates",
    "href": "W03.html#dates",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Dates",
    "text": "Dates\n\n\n\nISO 8601 standard for dates: YYYY-MM-DD. Today: 2023-10-02.\nDates in R are numeric vectors that represent the number of days since 1 January 1970.\n\n\ny <- as.Date(\"2020-01-01\"); y\n\n[1] \"2020-01-01\"\n\ntypeof(y)\n\n[1] \"double\"\n\nattributes(y)\n\n$class\n[1] \"Date\"\n\nas.double(y)\n\n[1] 18262\n\nas.double(as.Date(\"1970-01-01\"))\n\n[1] 0\n\nas.double(as.Date(\"1969-01-01\"))\n\n[1] -365\n\n\n\n\nhttps://social.coop/@mattl/110509557203534941"
  },
  {
    "objectID": "W03.html#how-many-days-are-you-old",
    "href": "W03.html#how-many-days-are-you-old",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "How many days are you old?",
    "text": "How many days are you old?\n\n\nSys.Date() - as.Date(\"1976-01-16\") \n\nTime difference of 17426 days\n\n# Sys.Date() gives as the current day your computer is set to"
  },
  {
    "objectID": "W03.html#date-times",
    "href": "W03.html#date-times",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Date-times",
    "text": "Date-times\nFor date-time manipulation use lubridate form the tidyverse.\n\nx <- lubridate::ymd_hm(\"1970-01-01 01:00\")\n# Note: Instead of loading package `pack` to use its function `func` you can also write `pack::func`\n# This works when the package is installed even when not loaded.\nx\n\n[1] \"1970-01-01 01:00:00 UTC\"\n\nattributes(x)\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"UTC\"\n\nas.double(x)\n\n[1] 3600\n\n\nUTC: Coordinated Universal Time. We are in the UTC+1 timezone.\nPOSIXct: Portable Operating System Interface, calendar time. Stores date and time in seconds with the number of seconds beginning at 1 January 1970."
  },
  {
    "objectID": "W03.html#how-many-seconds-are-you-old",
    "href": "W03.html#how-many-seconds-are-you-old",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "How many seconds are you old?",
    "text": "How many seconds are you old?\n\nas.double(lubridate::now()) - \n as.double(lubridate::ymd_hm(\"1976-01-16_12:04\"))\n\n[1] 1505563199"
  },
  {
    "objectID": "W03.html#summary-on-factors-and-dates",
    "href": "W03.html#summary-on-factors-and-dates",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Summary on Factors and Dates",
    "text": "Summary on Factors and Dates\n\nFactors\n\nCan be used to create categorical variables specified by the levels-attribute\nOften used to specify the order of categories. Particularly useful for graphics!\nCan be manipulated with functions from the forcats package\nOften it is sufficient to work with character vectors.\n\nDates and times\n\nDo not shy away from learning to work with dates and times properly!\nTedious to get right when the date format from the data is messy, but it is worth it!\nUse the lubridate package. Usually you just need one command to convert a character vector to a date or date-time vector, but you have to customize correctly.\n\n\nRead the chapter of factors and dates in R for Data Science"
  },
  {
    "objectID": "W03.html#string-modification",
    "href": "W03.html#string-modification",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "String modification",
    "text": "String modification\nWe modify strings with the stringr package from the tidyverse core. All functions from stringr start with str_.\nVery few examples:\n\nc(\"x\",\"y\")\n\n[1] \"x\" \"y\"\n\nstr_c(\"x\",\"y\")\n\n[1] \"xy\"\n\nstr_c(\"x\",\"y\",\"z\", sep=\",\")\n\n[1] \"x,y,z\"\n\nlength(c(\"x\",\"y\",\"z\"))\n\n[1] 3\n\nstr_length(c(\"x\",\"y\",\"z\"))\n\n[1] 1 1 1\n\nstr_length(c(\"This is a string.\",\"z\"))\n\n[1] 17  1"
  },
  {
    "objectID": "W03.html#string-wrangling-with-variable-names",
    "href": "W03.html#string-wrangling-with-variable-names",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "String wrangling with variable names",
    "text": "String wrangling with variable names\n\ndata <- tibble(Name = c(\"A\",\"B\",\"C\"), Age_2020 = c(20,30,40), Age_2021 = c(21,31,41), Age_2022 = c(22,32,42))\ndata\n\n# A tibble: 3 × 4\n  Name  Age_2020 Age_2021 Age_2022\n  <chr>    <dbl>    <dbl>    <dbl>\n1 A           20       21       22\n2 B           30       31       32\n3 C           40       41       42\n\n\nWe tidy that data set by creating a year variable.\n\n\ndata |> pivot_longer(c(\"Age_2020\", \"Age_2021\", \"Age_2022\"), names_to = \"Year\", values_to=\"Age\")\n\n\n\n# A tibble: 9 × 3\n  Name  Year       Age\n  <chr> <chr>    <dbl>\n1 A     Age_2020    20\n2 A     Age_2021    21\n3 A     Age_2022    22\n4 B     Age_2020    30\n5 B     Age_2021    31\n6 B     Age_2022    32\n7 C     Age_2020    40\n8 C     Age_2021    41\n9 C     Age_2022    42\n\n\n\n\nOK, but the year variable is a string but we want numbers."
  },
  {
    "objectID": "W03.html#use-word",
    "href": "W03.html#use-word",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Use word",
    "text": "Use word\nword extracts words from a sentence. However, the separator need not be \" \" but can be any character.\n\nword(\"This is a string.\", start=2, end=-2) \n\n[1] \"is a\"\n\n#Selects from the second to the second last word.\nword(\"Age_2022\", start=2, sep = \"_\")\n\n[1] \"2022\"\n\n\n\nIt also works vectorized.\n\ndata |> pivot_longer(c(\"Age_2020\", \"Age_2021\", \"Age_2022\"), names_to = \"Year\", values_to=\"Age\") |> \n  mutate(Year = word(Year, start = 2, sep = \"_\") |> as.numeric())\n\n\n\n# A tibble: 9 × 3\n  Name   Year   Age\n  <chr> <dbl> <dbl>\n1 A      2020    20\n2 A      2021    21\n3 A      2022    22\n4 B      2020    30\n5 B      2021    31\n6 B      2022    32\n7 C      2020    40\n8 C      2021    41\n9 C      2022    42"
  },
  {
    "objectID": "W03.html#string-detection-and-regular-expressions",
    "href": "W03.html#string-detection-and-regular-expressions",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "String detection and regular expressions",
    "text": "String detection and regular expressions\n\nfruits <- c(\"apple\", \"pineapple\", \"Pear\", \"orange\", \"peach\", \"banana\")\nstr_detect(fruits,\"apple\")\n\n[1]  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nstr_extract(fruits,\"apple\")\n\n[1] \"apple\" \"apple\" NA      NA      NA      NA     \n\n\nRegular expressions are useful because strings usually contain unstructured or semi-structured data, and regexps are a concise language for describing patterns in strings. When you first look at a regexp, you’ll think a cat walked across your keyboard, but as your understanding improves they will start to make sense.\n\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n\"^[\\w-\\.]+@([\\w-]+\\.)+[\\w-]{2,4}$\"\n\"\"^[[:alnum:].-_]+@[[:alnum:].-]+$\"\"\n\nThese are all regular expressions for email addresses."
  },
  {
    "objectID": "W03.html#special-values",
    "href": "W03.html#special-values",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Special values",
    "text": "Special values\n\nNA: Not available\nNaN: Not a number\nInf: Positive infinity\n-Inf: Negative infinity\n\n\n\n1/0\n\n\n[1] Inf\n\n\n\n\n\n-1/0\n\n\n[1] -Inf\n\n\n\n\n\n0/0\n\n\n[1] NaN\n\n\n\n\n\n1/0 + 1/0\n\n\n[1] Inf\n\n\n\n\n\n1/0 - 1/0\n\n\n[1] NaN"
  },
  {
    "objectID": "W03.html#nas",
    "href": "W03.html#nas",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "NAs",
    "text": "NAs\nInstead of NaN, NA stands for genuinely unknown values.\nIt can also be in a character of logical vector.\n\nx = c(1, 2, 3, 4, NA)\nmean(x)\n\n[1] NA\n\nmean(x, na.rm = TRUE)\n\n[1] 2.5\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    1.75    2.50    2.50    3.25    4.00       1 \n\n\nThe type of NA is logical.\n\ntypeof(NA)\n\n[1] \"logical\"\n\ntypeof(NaN)\n\n[1] \"double\"\n\n\nDoes it make sense?"
  },
  {
    "objectID": "W03.html#nas-in-logical-operations",
    "href": "W03.html#nas-in-logical-operations",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "NAs in logical operations",
    "text": "NAs in logical operations\nNA can be TRUE or FALSE.\nUsually operations including NA results again in NA, but some not!\n\n\nNA & TRUE\n\n\n[1] NA\n\n\n\n\n\nNA | TRUE\n\n\n[1] TRUE\n\n\n\n\n\nNA & FALSE\n\n\n[1] FALSE\n\n\n\n\n\nNA | FALSE\n\n\n[1] NA\n\n\n\nUnderstanding logical operations is important!"
  },
  {
    "objectID": "W03.html#null-is-the-null-object",
    "href": "W03.html#null-is-the-null-object",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "NULL is the null object",
    "text": "NULL is the null object\n\nused to represent lists with zero length\n\n\nx <- 1:10\nattributes(x)\n\nNULL\n\n\n\nused as a placeholder for missing values in lists and data frames\n\n\nL <- list(a = 1)\nL[[3]] <- 5\nL\n\n$a\n[1] 1\n\n[[2]]\nNULL\n\n[[3]]\n[1] 5"
  },
  {
    "objectID": "W03.html#working-with-more-data-frames",
    "href": "W03.html#working-with-more-data-frames",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Working with more data frames",
    "text": "Working with more data frames\n\nData can be distributed in several data frames which have relations which each other.\nFor example, they share variables as the five data frames in nycflights13.\n\n\n\n\nOften variables in different data frame have the same name, but that need not be the case! See the variable faa in airports matches origin and dest in flights."
  },
  {
    "objectID": "W03.html#data-women-in-science",
    "href": "W03.html#data-women-in-science",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Data: Women in science",
    "text": "Data: Women in science\n10 women in science who changed the world: Ada Lovelace, Marie Curie, Janaki Ammal, Chien-Shiung Wu, Katherine Johnson, Rosalind Franklin, Vera Rubin, Gladys West, Flossie Wong-Staal, Jennifer Doudna\n\n\n\n\nProfessionsDatesWorks\n\n\n\nprofessions <- read_csv(\"data/scientists/professions.csv\")\nprofessions\n\n# A tibble: 10 × 2\n   name               profession                        \n   <chr>              <chr>                             \n 1 Ada Lovelace       Mathematician                     \n 2 Marie Curie        Physicist and Chemist             \n 3 Janaki Ammal       Botanist                          \n 4 Chien-Shiung Wu    Physicist                         \n 5 Katherine Johnson  Mathematician                     \n 6 Rosalind Franklin  Chemist                           \n 7 Vera Rubin         Astronomer                        \n 8 Gladys West        Mathematician                     \n 9 Flossie Wong-Staal Virologist and Molecular Biologist\n10 Jennifer Doudna    Biochemist                        \n\n\n\n\n\ndates <- read_csv(\"data/scientists/dates.csv\")\ndates\n\n# A tibble: 8 × 3\n  name               birth_year death_year\n  <chr>                   <dbl>      <dbl>\n1 Janaki Ammal             1897       1984\n2 Chien-Shiung Wu          1912       1997\n3 Katherine Johnson        1918       2020\n4 Rosalind Franklin        1920       1958\n5 Vera Rubin               1928       2016\n6 Gladys West              1930         NA\n7 Flossie Wong-Staal       1947         NA\n8 Jennifer Doudna          1964         NA\n\n\n\n\n\nworks <- read_csv(\"data/scientists/works.csv\")\nworks\n\n# A tibble: 9 × 2\n  name               known_for                                                  \n  <chr>              <chr>                                                      \n1 Ada Lovelace       first computer algorithm                                   \n2 Marie Curie        theory of radioactivity,  discovery of elements polonium a…\n3 Janaki Ammal       hybrid species, biodiversity protection                    \n4 Chien-Shiung Wu    confim and refine theory of radioactive beta decy, Wu expe…\n5 Katherine Johnson  calculations of orbital mechanics critical to sending the …\n6 Vera Rubin         existence of dark matter                                   \n7 Gladys West        mathematical modeling of the shape of the Earth which serv…\n8 Flossie Wong-Staal first scientist to clone HIV and create a map of its genes…\n9 Jennifer Doudna    one of the primary developers of CRISPR, a ground-breaking…\n\n\n\n\n\n\n\nSource: Discover Magazine\nThe data can be downloaded: professions.csv, dates.csv, works.csv"
  },
  {
    "objectID": "W03.html#we-want-this-data-frame",
    "href": "W03.html#we-want-this-data-frame",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "We want this data frame",
    "text": "We want this data frame\n\n\n# A tibble: 10 × 5\n   name               profession                 birth_year death_year known_for\n   <chr>              <chr>                           <dbl>      <dbl> <chr>    \n 1 Ada Lovelace       Mathematician                      NA         NA first co…\n 2 Marie Curie        Physicist and Chemist              NA         NA theory o…\n 3 Janaki Ammal       Botanist                         1897       1984 hybrid s…\n 4 Chien-Shiung Wu    Physicist                        1912       1997 confim a…\n 5 Katherine Johnson  Mathematician                    1918       2020 calculat…\n 6 Rosalind Franklin  Chemist                          1920       1958 <NA>     \n 7 Vera Rubin         Astronomer                       1928       2016 existenc…\n 8 Gladys West        Mathematician                    1930         NA mathemat…\n 9 Flossie Wong-Staal Virologist and Molecular …       1947         NA first sc…\n10 Jennifer Doudna    Biochemist                       1964         NA one of t…"
  },
  {
    "objectID": "W03.html#joining-data-frames",
    "href": "W03.html#joining-data-frames",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Joining data frames",
    "text": "Joining data frames\nsomething_join(x, y)1 for data frames x and y which have a relation\n\nleft_join(): all rows from x\nright_join(): all rows from y\nfull_join(): all rows from both x and y\ninner_join(): all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches\n…\n\nThe notion join comes from SQL database. In other data manipulation frameworks joining is called merging."
  },
  {
    "objectID": "W03.html#simple-setup-for-x-and-y",
    "href": "W03.html#simple-setup-for-x-and-y",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Simple setup for x and y",
    "text": "Simple setup for x and y\n\nx <- tibble(\n  id = c(1, 2, 3),\n  value_x = c(\"x1\", \"x2\", \"x3\")\n  )\ny <- tibble(\n  id = c(1, 2, 4),\n  value_y = c(\"y1\", \"y2\", \"y4\")\n  )\nx\n\n# A tibble: 3 × 2\n     id value_x\n  <dbl> <chr>  \n1     1 x1     \n2     2 x2     \n3     3 x3     \n\ny\n\n# A tibble: 3 × 2\n     id value_y\n  <dbl> <chr>  \n1     1 y1     \n2     2 y2     \n3     4 y4"
  },
  {
    "objectID": "W03.html#left_join",
    "href": "W03.html#left_join",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "left_join()",
    "text": "left_join()\n\n\n\n\n\nleft_join(x, y)\n\n# A tibble: 3 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     3 x3      <NA>"
  },
  {
    "objectID": "W03.html#right_join",
    "href": "W03.html#right_join",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "right_join()",
    "text": "right_join()\n\n\n\n\n\nright_join(x, y)\n\n# A tibble: 3 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     4 <NA>    y4"
  },
  {
    "objectID": "W03.html#full_join",
    "href": "W03.html#full_join",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "full_join()",
    "text": "full_join()\n\n\n\n\n\nfull_join(x, y)\n\n# A tibble: 4 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     3 x3      <NA>   \n4     4 <NA>    y4"
  },
  {
    "objectID": "W03.html#inner_join",
    "href": "W03.html#inner_join",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "inner_join()",
    "text": "inner_join()\n\n\n\n\n\ninner_join(x, y)\n\n# A tibble: 2 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2"
  },
  {
    "objectID": "W03.html#women-in-science",
    "href": "W03.html#women-in-science",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Women in science",
    "text": "Women in science\n\nleft_joinright_joinfull_joininner_joinFinal\n\n\n\nprofessions |> left_join(works)\n\n# A tibble: 10 × 3\n   name               profession                         known_for              \n   <chr>              <chr>                              <chr>                  \n 1 Ada Lovelace       Mathematician                      first computer algorit…\n 2 Marie Curie        Physicist and Chemist              theory of radioactivit…\n 3 Janaki Ammal       Botanist                           hybrid species, biodiv…\n 4 Chien-Shiung Wu    Physicist                          confim and refine theo…\n 5 Katherine Johnson  Mathematician                      calculations of orbita…\n 6 Rosalind Franklin  Chemist                            <NA>                   \n 7 Vera Rubin         Astronomer                         existence of dark matt…\n 8 Gladys West        Mathematician                      mathematical modeling …\n 9 Flossie Wong-Staal Virologist and Molecular Biologist first scientist to clo…\n10 Jennifer Doudna    Biochemist                         one of the primary dev…\n\n\n\n\n\nprofessions |> right_join(works)\n\n# A tibble: 9 × 3\n  name               profession                         known_for               \n  <chr>              <chr>                              <chr>                   \n1 Ada Lovelace       Mathematician                      first computer algorithm\n2 Marie Curie        Physicist and Chemist              theory of radioactivity…\n3 Janaki Ammal       Botanist                           hybrid species, biodive…\n4 Chien-Shiung Wu    Physicist                          confim and refine theor…\n5 Katherine Johnson  Mathematician                      calculations of orbital…\n6 Vera Rubin         Astronomer                         existence of dark matter\n7 Gladys West        Mathematician                      mathematical modeling o…\n8 Flossie Wong-Staal Virologist and Molecular Biologist first scientist to clon…\n9 Jennifer Doudna    Biochemist                         one of the primary deve…\n\n\n\n\n\ndates |> full_join(works)\n\n# A tibble: 10 × 4\n   name               birth_year death_year known_for                           \n   <chr>                   <dbl>      <dbl> <chr>                               \n 1 Janaki Ammal             1897       1984 hybrid species, biodiversity protec…\n 2 Chien-Shiung Wu          1912       1997 confim and refine theory of radioac…\n 3 Katherine Johnson        1918       2020 calculations of orbital mechanics c…\n 4 Rosalind Franklin        1920       1958 <NA>                                \n 5 Vera Rubin               1928       2016 existence of dark matter            \n 6 Gladys West              1930         NA mathematical modeling of the shape …\n 7 Flossie Wong-Staal       1947         NA first scientist to clone HIV and cr…\n 8 Jennifer Doudna          1964         NA one of the primary developers of CR…\n 9 Ada Lovelace               NA         NA first computer algorithm            \n10 Marie Curie                NA         NA theory of radioactivity,  discovery…\n\n\n\n\n\ndates |> inner_join(works)\n\n# A tibble: 7 × 4\n  name               birth_year death_year known_for                            \n  <chr>                   <dbl>      <dbl> <chr>                                \n1 Janaki Ammal             1897       1984 hybrid species, biodiversity protect…\n2 Chien-Shiung Wu          1912       1997 confim and refine theory of radioact…\n3 Katherine Johnson        1918       2020 calculations of orbital mechanics cr…\n4 Vera Rubin               1928       2016 existence of dark matter             \n5 Gladys West              1930         NA mathematical modeling of the shape o…\n6 Flossie Wong-Staal       1947         NA first scientist to clone HIV and cre…\n7 Jennifer Doudna          1964         NA one of the primary developers of CRI…\n\n\n\n\n\nprofessions |> left_join(dates) |> left_join(works)\n\n# A tibble: 10 × 5\n   name               profession                 birth_year death_year known_for\n   <chr>              <chr>                           <dbl>      <dbl> <chr>    \n 1 Ada Lovelace       Mathematician                      NA         NA first co…\n 2 Marie Curie        Physicist and Chemist              NA         NA theory o…\n 3 Janaki Ammal       Botanist                         1897       1984 hybrid s…\n 4 Chien-Shiung Wu    Physicist                        1912       1997 confim a…\n 5 Katherine Johnson  Mathematician                    1918       2020 calculat…\n 6 Rosalind Franklin  Chemist                          1920       1958 <NA>     \n 7 Vera Rubin         Astronomer                       1928       2016 existenc…\n 8 Gladys West        Mathematician                    1930         NA mathemat…\n 9 Flossie Wong-Staal Virologist and Molecular …       1947         NA first sc…\n10 Jennifer Doudna    Biochemist                       1964         NA one of t…"
  },
  {
    "objectID": "W03.html#keys",
    "href": "W03.html#keys",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Keys",
    "text": "Keys\n\nA key is a variable or a set of variables which uniquely identifies observations\nWhat was the key in the data frame of women in science?\n\n\n\nSwitching back to nycflights13 as example\nIn simple cases, a single variable is sufficient to identify an observation, e.g. each plane in planes is identified by tailnum.\nSometimes, multiple variables are needed; e.g. to identify an observation in weather you need five variables: year, month, day, hour, and origin"
  },
  {
    "objectID": "W03.html#how-can-we-check",
    "href": "W03.html#how-can-we-check",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "How can we check?",
    "text": "How can we check?\nCounting observation and filter those more than one\n\nlibrary(nycflights13)\nplanes |> count(tailnum) |> filter(n > 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tailnum <chr>, n <int>\n\nweather |> count(year, month, day, hour, origin) |> filter(n > 1) \n\n# A tibble: 3 × 6\n   year month   day  hour origin     n\n  <int> <int> <int> <int> <chr>  <int>\n1  2013    11     3     1 EWR        2\n2  2013    11     3     1 JFK        2\n3  2013    11     3     1 LGA        2\n\n# OK, here 3 observations are twice. Probably a data error.\n# Example: Without hour it is not a key\nweather |> count(year, month, day, origin) |> filter(n > 1) \n\n# A tibble: 1,092 × 5\n    year month   day origin     n\n   <int> <int> <int> <chr>  <int>\n 1  2013     1     1 EWR       22\n 2  2013     1     1 JFK       22\n 3  2013     1     1 LGA       23\n 4  2013     1     2 EWR       24\n 5  2013     1     2 JFK       24\n 6  2013     1     2 LGA       24\n 7  2013     1     3 EWR       24\n 8  2013     1     3 JFK       24\n 9  2013     1     3 LGA       24\n10  2013     1     4 EWR       24\n# ℹ 1,082 more rows"
  },
  {
    "objectID": "W03.html#terminology-primary-and-foreign-keys",
    "href": "W03.html#terminology-primary-and-foreign-keys",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Terminology: Primary and foreign keys",
    "text": "Terminology: Primary and foreign keys\n\nA primary key uniquely identifies an observation in its own table. E.g, planes$tailnum in planes.\nA foreign key uniquely identifies an observation in another data frame E.g. flights$tailnum is a foreign key in flights because it matches each flight to a unique plane in planes.\nData frames need not have a key and the joins will still do their work.\nA primary key and a foreign key form a relation.\nRelations are typically 1-to-many. Each plane has many flights\nRelations can also be many-to-many. Airlines can fly to many airports; airport can host many airplanes."
  },
  {
    "objectID": "W03.html#joining-when-key-names-differ",
    "href": "W03.html#joining-when-key-names-differ",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "Joining when key names differ?",
    "text": "Joining when key names differ?\nWe have to specify the key relation with a named vector in the by argument.\n\ndim(flights)\n\n[1] 336776     19\n\nflights |> left_join(airports, by = c(\"dest\" = \"faa\"))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 18 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>, name <chr>, lat <dbl>,\n#   lon <dbl>, alt <dbl>, tz <dbl>, dst <chr>, tzone <chr>\n\n# New version\nflights |> left_join(airports, join_by(\"dest\" == \"faa\"))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 18 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>, name <chr>, lat <dbl>,\n#   lon <dbl>, alt <dbl>, tz <dbl>, dst <chr>, tzone <chr>\n\n\nWhy does the number of rows stays the same after joining?\n\nfaa is a primary key in airports."
  },
  {
    "objectID": "W03.html#left_join-essentially-right_join-with-switched-data-frames",
    "href": "W03.html#left_join-essentially-right_join-with-switched-data-frames",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "left_join essentially right_join with switched data frames",
    "text": "left_join essentially right_join with switched data frames\n\nairports_right_flights <- airports |> right_join(flights, by = c(\"faa\" = \"dest\"))\nairports_right_flights \n\n# A tibble: 336,776 × 26\n   faa   name       lat   lon   alt    tz dst   tzone  year month   day dep_time\n   <chr> <chr>    <dbl> <dbl> <dbl> <dbl> <chr> <chr> <int> <int> <int>    <int>\n 1 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     1     1955\n 2 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     2     2010\n 3 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     3     1955\n 4 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     4     2017\n 5 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     5     1959\n 6 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     6     1959\n 7 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     7     2002\n 8 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     8     1957\n 9 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10     9     1957\n10 ABQ   Albuque…  35.0 -107.  5355    -7 A     Amer…  2013    10    10     2011\n# ℹ 336,766 more rows\n# ℹ 14 more variables: sched_dep_time <int>, dep_delay <dbl>, arr_time <int>,\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\nDifferences\n\nIn a join where keys have different column names the name of the first data frame survives (unless you use keep = TRUE). Here, faa instead of dest\nThe columns from the first data frame come first\nThe order of rows is taken from the first data frame, while duplication and dropping of variables is determined by the second data frame (because it is a right_join)\n\nUsing the fact that flights seem to be ordered by year, month, day, dep_time we can re-arrange:\n\nairports_right_flights |> \n  rename(dest = faa) |> \n  select(names(flights)) |> # Use order of flights\n  arrange(year, month, day, dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      924            917\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n\nNote of caution: A deeper analysis shows that the order is still not exactly the same."
  },
  {
    "objectID": "W03.html#left_join-with-reversed-data-frames",
    "href": "W03.html#left_join-with-reversed-data-frames",
    "title": "W#03 Data Import, Data Wrangling, Relational Data, Exploratory Data Analysis",
    "section": "left_join with reversed data frames",
    "text": "left_join with reversed data frames\n\ndim(airports)\n\n[1] 1458    8\n\ndim(flights)\n\n[1] 336776     19\n\nairports |> \n  left_join(flights, by = c(\"faa\" = \"dest\"))\n\n# A tibble: 330,531 × 26\n   faa   name      lat    lon   alt    tz dst   tzone  year month   day dep_time\n   <chr> <chr>   <dbl>  <dbl> <dbl> <dbl> <chr> <chr> <int> <int> <int>    <int>\n 1 04G   Lansdo…  41.1  -80.6  1044    -5 A     Amer…    NA    NA    NA       NA\n 2 06A   Moton …  32.5  -85.7   264    -6 A     Amer…    NA    NA    NA       NA\n 3 06C   Schaum…  42.0  -88.1   801    -6 A     Amer…    NA    NA    NA       NA\n 4 06N   Randal…  41.4  -74.4   523    -5 A     Amer…    NA    NA    NA       NA\n 5 09J   Jekyll…  31.1  -81.4    11    -5 A     Amer…    NA    NA    NA       NA\n 6 0A9   Elizab…  36.4  -82.2  1593    -5 A     Amer…    NA    NA    NA       NA\n 7 0G6   Willia…  41.5  -84.5   730    -5 A     Amer…    NA    NA    NA       NA\n 8 0G7   Finger…  42.9  -76.8   492    -5 A     Amer…    NA    NA    NA       NA\n 9 0P2   Shoest…  39.8  -76.6  1000    -5 U     Amer…    NA    NA    NA       NA\n10 0S9   Jeffer…  48.1 -123.    108    -8 A     Amer…    NA    NA    NA       NA\n# ℹ 330,521 more rows\n# ℹ 14 more variables: sched_dep_time <int>, dep_delay <dbl>, arr_time <int>,\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\nWhy does the number of rows changes after joining?\ndest is not a primary key in flights. There are more flights with the same destination so rows of airports get duplicated.\nWhy is the number of rows then less than the number of rows in flights?\nLet us do some checks:\n\nlength(unique(airports$faa)) # Unique turns out to be redundant because faa is a primary key\n\n[1] 1458\n\nlength(unique(flights$dest))\n\n[1] 105\n\n# There are much more airports then destinations in flights!\n# ... but the rows of airports prevail when it is the first in a left_join.\n# So, the data frame should even increase because \n# we get several rows of airports without flights\n# Let us dig deeper.\n\nsetdiff( unique(airports$faa), unique(flights$dest)) |> length()\n\n[1] 1357\n\n# 1,357 airports have no flights. But also:\nsetdiff( unique(flights$dest), unique(airports$faa)) |> length()\n\n[1] 4\n\n# There are four destinations in flights, which are not in the airports list!\n\n# How many flights are to these?\nflights |> \n  filter(dest %in% setdiff( unique(flights$dest), unique(airports$faa))) |> \n  nrow()\n\n[1] 7602\n\n# 7,602 flights go to destinations not listed as airport\n\n# Check\nnrow(airports |> left_join(flights, by = c(\"faa\" = \"dest\"))) == nrow(flights) - 7602 + 1357\n\n[1] TRUE\n\n# OK, now we have a clear picture\n# airport with left_joined flights duplicates the rows an airports for each flight flying to it\n# So the total number of rows is the number of flights plus the number of airport which do not \n# appear as a destination minus the flights which go to destinations which are not listed in airports\n\nLearning: The new number of observation after a join can be a complex combination of duplication and dropping. It is your responsibility to understand what is happening."
  },
  {
    "objectID": "W02.html#git-and-github",
    "href": "W02.html#git-and-github",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Git and GitHub",
    "text": "Git and GitHub\n\n\n\n\nGit the version control system\n\nsomehow like Track Changes in Microsoft Word\nsomehow like “Save as …” for multiple files in a folder (with old versions saved)\n\nDeveloped 2005 by Linus Torvalds to maintain the Linux kernel\n\n\n\n\nGitHub our home for Git-based projects on the internet – a bit like DropBox but for code files\nThe platform for web hosting, collaboration, and as our course management system in this course"
  },
  {
    "objectID": "W02.html#our-git-github-dance",
    "href": "W02.html#our-git-github-dance",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Our git-GitHub dance",
    "text": "Our git-GitHub dance\nHappens with thw GitHub-Organization CU-F23-MDSSB-MET-03-VisCommDataStory\n\n\n\nFrom http://datasciencebox.org/"
  },
  {
    "objectID": "W02.html#our-git-github-dance-1",
    "href": "W02.html#our-git-github-dance-1",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Our git-GitHub dance",
    "text": "Our git-GitHub dance\nWe only use only a few of the various git commands.1\n\n\n\nFrom http://datasciencebox.org/\nThat is why the resource http://happygitwithr.com is more helpful to us compared to the git-documentation."
  },
  {
    "objectID": "W02.html#our-git-github-dance-2",
    "href": "W02.html#our-git-github-dance-2",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Our git-GitHub dance",
    "text": "Our git-GitHub dance\nYou git clone your private repositories to your computer.\n\n\n\nFrom http://datasciencebox.org/"
  },
  {
    "objectID": "W02.html#our-git-github-dance-3",
    "href": "W02.html#our-git-github-dance-3",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Our git-GitHub dance",
    "text": "Our git-GitHub dance\ngit add, git commit, git push your work, git pull other’s work\n\n\n\nFrom http://datasciencebox.org/"
  },
  {
    "objectID": "W02.html#programming-languages",
    "href": "W02.html#programming-languages",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Programming languages",
    "text": "Programming languages\nDefinition: Systems of rules which can process instructions to be executed by the computer.\n\nOur two programming languages are:\n   \n\n\n\nIn R:\ndo_this(to_this)\ndo_that(to_this, with_those)\nto_this |> do_this() |> do_that(with_those) \nstore <- do_that(to_this)\n\nIn python:\nto_this.do_this()\nto_this.do_this(with_those)\nto_this.do_this().do_that(with_those)\nstore = do_that(to_this)\n\n\n\nThe role of brackets, dots, spaces, special words, and so on is the syntax of a language. Wrong syntax is a common cause of error. Learning syntax slows you down, but only initially!\nEssential part of the game: Installing and using libraries/packages. In R:\ninstall.packages(\"tidyverse\") (called once to install) and\nlibrary(tidyverse) (in the code before using its commands)"
  },
  {
    "objectID": "W02.html#integrated-development-environment",
    "href": "W02.html#integrated-development-environment",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Integrated development environment",
    "text": "Integrated development environment\nIDEs provide terminals, a source code editor, an object browser (the environment), output and help view, tools for rendering and version control, and more to help in the workflow. Our IDEs are:\n    VS Code\n\nEditors delight us with\n\nsyntax highlighting Then we see if code looks good\n\nc(1O, Text, true, 10,\"Text\",TRUE)\n\ncode completion Start writing, and press Tab to see options\nautomatic indentation, brace matching, keyboard shortcuts, …\n\nLearn to customize and use IDEs ever better and better!"
  },
  {
    "objectID": "W02.html#publishing-system",
    "href": "W02.html#publishing-system",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "Publishing system",
    "text": "Publishing system\nWeaves together text and code to produces good-looking formatted scientific or technical output.\n   \n\nA YAML header and Markdown text with code chunks is rendered to a document in several formats.\n\n\n\nNotebooks are a similar concept: text and executable code mixed together in a browser tab. Notebooks (e.g. .ipynb-files) Can be rendered by quarto. Popular in the python world.\nMain difference between notebook and quarto-output: Code in the notebook can be executed. The page is not static."
  },
  {
    "objectID": "W02.html#quarto-documents",
    "href": "W02.html#quarto-documents",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "quarto Documents",
    "text": "quarto Documents\nHeader: YAML with title, author, document-wide specifications\nBody:\n\n\nParts of plain text with formatting syntax like **bold** for bold, and other for headers, lists, and so on. Good idea: Occassionally spend a few minutes to look up and learn some formatting rules for markdown.\nCode chunks where the programming happens!\n\nBeginning with ```{r}\nThen optional chunk specifications. For example\n\n#| label: NAME to cross-reference to it in the text\n#| echo: false to not show (or show if true) code in output\n#| warning: false to not show (or show if true) the the warnings you would see in the Console\n\nThen comes the code itself. By default, every command which creates some print output or shows a graphic will make this appear in your output-document.\nEnding with ```"
  },
  {
    "objectID": "W02.html#what-is-an-enviroment-and-why-you-should-know-it",
    "href": "W02.html#what-is-an-enviroment-and-why-you-should-know-it",
    "title": "W#02 Data Visualization, Data Formats",
    "section": "What is an Enviroment, and why you should know it",
    "text": "What is an Enviroment, and why you should know it\n\nEnvironment can be understood as the surrounding conditions in which code is executed.\n\nIn RStudio: There is an Environment-Tab.\nMost important: Shows the variables currently accessible from the Console.\n\nThe environment of your quarto document is different for that of the Console! Remember this! It will be a source of confusion.\n\n\n\n\nCU-F23-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "socialmedia.html",
    "href": "socialmedia.html",
    "title": "Data Science on the Internet and in Social Media",
    "section": "",
    "text": "Many people talk data science and related things on social media. Some people even make a business publishing short snippets because they love communicating about data science but also trying to attract you to their newsletters or courses. Also many developers of R or python packages are on social media occasionally posting new releases, ideas, or reacting."
  },
  {
    "objectID": "socialmedia.html#should-you-follow-data-science-on-social-media",
    "href": "socialmedia.html#should-you-follow-data-science-on-social-media",
    "title": "Data Science on the Internet and in Social Media",
    "section": "2 Should you follow data science on social media?",
    "text": "2 Should you follow data science on social media?\nYes, you should! Because\n\nyou can get a feeling of how the field develops.\nyou can find nice snippets to learn things, when lucky they are exactly on your level.\nmany people learn data science things on social media, you react with a questions and may be lucky to get a response.\nthere may sometimes be release for your confused mind through data science memes, cartoons, or jokes.\n\nNo, you should not! Because\n\nall social media is disctraction from focused learning and concentrated working.\nyou may develop imposter syndrome symptoms. There is so much stuff out and you may think everybody else knows so much more and you will never catch up. (Hint, the reality is more like this: No one knows everything, you already have your own perspective and own unique set of skills, every good data scientist is constantly learning and is constantly able to rethink and also sometimes has similar feelings, what gets pushed on social media tends to be more opinionated and overstated.)\nyou may fall into hype topics where people overstate the importance, risks, dangers, or opportunities a lot.\n\nFinal answer: You decide. If you do, learn to stop when it feels bad, learn to detect hype, and do more other data science learning: real exercises, tutorials or your projects. Just assume you are in the blue region on the graph in this cartoon 😉."
  },
  {
    "objectID": "socialmedia.html#which-social-media-and-how-to-follow-data-science",
    "href": "socialmedia.html#which-social-media-and-how-to-follow-data-science",
    "title": "Data Science on the Internet and in Social Media",
    "section": "3 Which social media and how to follow data science?",
    "text": "3 Which social media and how to follow data science?\nThe following is not intended to be complete or balanced in anyway. If you find something interesting let me know and I may add it to the lists.\nFor following Data Science in social media you often need platform accounts, otherwise most platform do not even let you see things. Anyway, accounts help you by selecting who to follow. You do not need to post or like things, you can start just looking around and following.\nThe general ways to access content is by following interesting people, searching for words or hashtags, or use the explore options the platforms offer.\nFor sure data science is a common topic on 𝕏 (which I still call Twitter which is still the URL), Reddit , and Mastodon .\nOn Mastodon: It is not a propriatary service, it is offering a joint open source protocol which different independent servers can use to run something like a decentralized twitter. Some people call it Twitter with an open protocol like email. I think, this is a very good development! When you want to have a mastodon account, you need to decide for a server where to live. In the end it is not so important because you can follow people on all servers and you can also move with your account later. I am in the community https://datasci.social but you could take anythings.\nAfter Elon Musk’s twitter take over microblogging people are a but puzzled where to go and many maintain more than one plattform like twitter/X, bluesky, threads, or mastodon (which I support most). So, you may find the same people at different places."
  },
  {
    "objectID": "socialmedia.html#some-entry-points",
    "href": "socialmedia.html#some-entry-points",
    "title": "Data Science on the Internet and in Social Media",
    "section": "4 Some “entry points”",
    "text": "4 Some “entry points”\n\n4.1 People\nA cool statistician: https://twitter.com/kareem_carr\nhttps://fosstodon.org/@juliasilge\nAllison Horst Chelsea\nChristoph Molnar https://twitter.com/ChristophMolnar Albert Rapp https://twitter.com/rappa753 Daniël Lakens https://twitter.com/lakens\nHadley wickham\n\n\n4.2 Other things\nSubreddits and the like\nYoutube, and podcasts\nA reddit thread https://www.reddit.com/r/datascience/comments/16dk5b6/r_vs_python_detailed_examples_from_proficient/"
  },
  {
    "objectID": "benford .html",
    "href": "benford .html",
    "title": "Data Science Concepts / Data Science Tools",
    "section": "",
    "text": "This is an excersise combining knowledge about logarithms and probability distributions and a statistical test.\nExercise: Analyzing Data with Benford’s Law in R\nBackground: Benford’s Law, also known as the First-Digit Law, is a phenomenon where in many naturally occurring datasets, the leading digit (i.e., the first digit) is more likely to be small, such as 1, 2, or 3, than it is to be large, such as 8 or 9. This law has applications in fraud detection, financial auditing, and data integrity verification.\nTask: You are given a dataset in a CSV file named data.csv. Your task is to perform the following steps using R and data analytics libraries to determine if the dataset follows Benford’s Law:\nData Loading: Read the data from data.csv into a data frame using read.csv().\n\nData Exploration: Explore the dataset to understand its structure and contents. Display summary statistics, and visualize the distribution of the first digits in the dataset using a histogram.\n\nCalculate Expected Benford's Law Frequencies: Calculate the expected frequency of each first digit (1 through 9) according to Benford's Law. You can use the formula:\n\nP(d)=log⁡10(d+1)−log⁡10(d)P(d)=log10​(d+1)−log10​(d)\n\nWhere P(d)P(d) is the expected proportion of numbers starting with digit dd.\n\nCompare Observed vs. Expected Frequencies: Calculate the observed frequency of each first digit in the dataset. Compare the observed frequencies with the expected frequencies from Benford's Law. Create a bar chart or similar visualization to illustrate the comparison.\n\nHypothesis Testing: Perform a statistical hypothesis test to determine if the dataset significantly deviates from Benford's Law. You can use a chi-squared goodness-of-fit test to assess the deviation.\n\nConclusion: Based on the results of your analysis, draw conclusions about whether the dataset follows or deviates from Benford's Law. Explain your findings and their implications.\nNote:\nYou may assume that the dataset contains numerical values.\nYou can use R packages like readr for data reading, and ggplot2 for data visualization.\nInclude comments in your code to explain your approach and calculations.\nProvide clear visualizations and explanations in your final report.\nBonus (Optional): If you’d like to extend this exercise, you can consider applying Benford’s Law to more complex datasets or exploring other variations of the law, such as the Second-Digit Law or the Benford’s Law for Two-Digit Numbers using R."
  }
]